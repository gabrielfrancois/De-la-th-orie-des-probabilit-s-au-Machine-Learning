%Si erreur, regarder \usepackage{mathrsfs} qui peut avoir chamboulé les comandes..

\documentclass[12pt]{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float} %permet de mieux gérer les emplacements lors de l'insertion d'images notamment avec  [H] permettant de contrôler parfaitement l'emplacement de l'image
\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage[T1]{fontenc} %pour encoder la police
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings} %utile pour taper du code
\usepackage{setspace}%Pour gérer les interlignes

\usepackage{titlesec} % pour personnaliser les titres
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{stmaryrd} % pour les double-crochets pour les entiers
\usepackage{tikz-cd}
\usepackage{geometry} %pour gérer les marges
\usepackage{vmargin}
\setpapersize{A4}
%\usepackage{fourier}
\usepackage{bbold}%pour que les indicatrices fonctionnent
\usepackage{comment}
\usepackage{pgfplots}
\usepackage{mathabx}%Pour faire des crochets d'entiers (\ldbrack x \rdbraxk)
%hypref était la avant
\usepackage{mathrsfs}
\usepackage{diagbox} % Pour la barre diagonale
\usepackage{changepage} %pour écrire sur le côté
\usepackage{lmodern}%nouvelle police lateX
\usepackage{hyperref} %Pour que les la table des matières fonctionne correctement





%\usepackage{tcolorbox}
%
%
%\newtcolorbox{définition}[2][]{
%    colback=white,
%    colframe=black,
%    title=#2,
%    #1
%}


%%%%%%%%%%%%%%%%%%%%%%%%Commandes %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







% Correction

\newif\ifcorrection	
\newcommand{\corr}[1]{\ifcorrection{\color{lightblue}#1\color{black}}\fi}

\correctiontrue

%couleurs

\definecolor{green}{RGB}{0,150,0}
\definecolor{violet}{RGB}{140,50,250}
\definecolor{darkblue}{RGB}{0, 0, 150}
\definecolor{lightblue}{RGB}{0, 0, 250}
\definecolor{bleugenie}{RGB}{0,0,120}

%géométrie

\geometry{left=4cm, right=-1cm,top=4cm, bottom=0cm }


%%%%%%%%%%%%%%%%%%%%Nouvelles Commandes%%%%%%%%%%%%%%%%%%%%


%utilitaire générale non mathématique

\newcommand{\espace}{\vspace{1.5em}}
\newcommand{\petitespace}{\vspace{0.5cm}}
\newcommand{\shift}{\hspace{2em}}
\newcommand{\cercler}[1]{\tikz[baseline=(char.base)]{\node[shape=circle,draw,inner sep=1pt](char){#1};} }


%raccourcis/utilitaire mathématique
 
\newcommand{\bb}[1]{\mathbb{#1}} %Indicatrîce

\newcommand{\R}{\bb{R}} %R

\renewcommand{\bf}[1]{\mathbf{#1}}

\newcommand{\N}{\bb{N}}%N

\newcommand{\Q}{\bb{Q}}%Rationels

\newcommand{\C}{\bb{C}}%Complexes

\renewcommand{\P}{\bb{P}}%Probabilité

\renewcommand{\d}{\partial} %dérivée

\newcommand{\union}[2]{\overset{#2}{\underset{#1}{\bigcup}}} %Union

\newcommand{\inter}[2]{\overset{#2}{\underset{#1}{\bigcap}}} %Union

\newcommand{\transpo}{\intercal}

\newcommand{\ortho}{\bot}

\newcommand{\rg}{\textit{rang}}

\newcommand{\ie}{\textit{i.e.}}

\newcounter{theorem}[subsection] % Crée un nouveau compteur pour les théorèmes
\renewcommand{\thetheorem}{\thesection|\thesubsection|\arabic{theorem}} % Définit le format de numérotation
\newcommand{\thm}{\refstepcounter{theorem}\textbf{Théorème \thetheorem} \\} % Utilise le compteur dans la commande


\newcounter{definition}[subsection] % Crée un nouveau compteur pour les définitions
\renewcommand{\thedefinition}{\thesection|\thesubsection|\arabic{definition}} % Définit le format de numérotation
\newcommand{\Def}{\refstepcounter{definition}\textbf{Définition \thedefinition} \\} % Utilise le compteur dans la commande


\newcommand{\Rq}{\underline{Remarque :} \\}

\newcounter{proposition}[subsection] % Crée un nouveau compteur pour les propositions
\renewcommand{\theproposition}{\thesection|\thesubsection|\arabic{proposition}} % Définit le format de numérotation
\newcommand{\Prop}{\refstepcounter{proposition}\textbf{Proposition \theproposition} \\} % Utilise le compteur dans la commande


\newcommand{\somme}[2]{\sum\limits_{#1}^{#2}}

\newcommand{\produit}[2]{\mathop{\Pi}\limits_{#1}^{#2}}

\renewcommand{\cal}{\mathcal}

\newcommand{\Sup}[1]{\sup\limits_{#1}}%Sup

\newcommand{\Max}[1]{\max\limits_{#1}}

\newcommand{\Min}[1]{\min\limits_{#1}}

\newcommand{\Inf}[1]{\inf\limits_{#1}}

\renewcommand{\bf}[1]{\mathbf{#1}}

\newcommand{\Lim}[1]{\lim\limits_{#1}}
  
\newcommand{\norme}[1]{\lVert #1 \rVert}

%variables aléatoires

\newcommand{\bin}[2]{\mathcal{B}(#1,#2)} %Bernouilli

\newcommand{\ber}[1]{\mathcal{B}(#1)} %Bernouilli

\newcommand{\pois}[1]{\mathcal{P}(#1)} %Poisson

\newcommand{\Expo}[1]{\mathcal E(#1)}%Exponentielle

\newcommand{\gam}[2]{\Gamma(#1,#2)}%Loi gamma

\newcommand{\gaminv}[2]{\Gamma^{-1}(#1,#2)}%Loi gamma

\newcommand{\chideux}[1]{\chi^2(#1)} %Chi deux

\newcommand{\normale}[2]{\mathcal{N}(#1,#2)} % loi normale 

\newcommand{\geom}[2]{\mathcal{G}(#1,#2)} %Loi géométrique

%ensembles pratiques

\newcommand{\Xunan}{X_1,\ldots,X_n} % va X1, ..., Xn

\newcommand{\Xunad}{X_1,\ldots,X_d} % va X1, ..., Xd

\newcommand{\Xnsuite}{(X_n)_{n\geq1}} % suite de va X1, ...

\newcommand{\Ynsuite}{(Y_n)_{n\geq1}} % suite de va Y1, ...

\newcommand{\esp}[1]{\bb{ E} \mathopen{}\left[#1\right]} % espérance

\newcommand{\Var}[1]{\bb{ V} \mathopen{}\left(#1\right)} % variance

\newcommand{\cov}{\text{Cov}}

\newcommand{\espcond}[2]{\mathbb{E}\mathopen{}\left[#1\middle|#2\right]}%espérance conditionelle

\newcommand{\BR}{\mathcal B(\bb{R})} % boréliens de R

\newcommand{\BRd}{\mathcal B(\bb{R}^d)} % boréliens de R^d

\newcommand{\EE}{(E, \mathcal E)} % espace mesurable (E, E)

\newcommand{\FF}{(F, \mathcal F)} % idem (F, F)

\newcommand{\OmegaAP}{(\Omega, \mathcal A, P)} % espace probabilisé (Omega, A, P)

\newcommand{\indep}{\perp \!\!\! \perp} % symbole indépendant (pour des événements)

\newcommand{\EEunan}{(E_1, \mathcal E_1),(E_2, \mathcal E_2),\ldots,(E_n, \mathcal E_n)} % espaces mesurables (E1, E1), ..., (En, En)

\newcommand{\EEnsuite}{(E_n, \mathcal E_n)_{n\geq1}} % suites d'espaces mesurables (E1, E1), ...

\newcommand{\modelstat}{\{ \P_\theta, \theta \in \Theta\}}

\newcommand{\EMV}{\widehat{\theta}_n^{MV}}

\newcommand{\estB}{\overline\theta_n^B}

\newcommand{\esptheta}[1]{\bb{E}_\theta[#1]}

\newcommand{\simiid}{\overset{\text{iid}}{\sim}}

%fonctions

\newcommand{\fonct}[4]{\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  #1 :& #2 &\mapsto& #3\\
  & x & \mapsto & #4 \end{array}}
  
\newcommand{\ffonct}[5]{\begin{array}{rrrl}%fonction à deux variables dans E : {f}{E}{F}{R}{2x+3y}
  #1 :& #2 \times #3 &\mapsto& #4\\
  & (x;y)& \mapsto & #5 \end{array}}
  
\newcommand{\fffonct}[6]{\begin{array}{rrrl}%fonction à trois variables dans E : {f}{E}{F}{G}{R}{2x+3y+2z}
  #1 :& #2 \times #3 \times #4 &\mapsto& #5\\
  & (x;y;z)& \mapsto & #6 \end{array}}
  
\newcommand{\fgam}[1]{\int_0^\infty x^{k-1} e^{-x} \, dx}%fonction gamma
  
\newcommand{\1}{\bb{1}} % fonction indicatrice

%fonctions densités

\newcommand{\densexpo}[2]{#1e^{-#1#2}\1_{[0;+\infty)}(#2)}%fonction densité de la loi expo de (lambda,x)

\newcommand{\densnorm}[3]{\frac{1}{\sqrt{2\pi#2^2}}e^{\frac{-(#3-#1)^2}{#2^2}}} %fonction densité de la loi normale (mu,sigma,x)

\newcommand{\densgam}[3]{\frac{1}{#2^#1\Gamma(#1)}#3^{#1-1}e^{-\frac{1}{#2}#3}\1_{[0;+\infty)}(#3)} %fonction densité de la loi gamma(n,lambda,x)

\newcommand{\densgaminv}[3]{\frac{#2^#1}{\Gamma(#1)}#3^{#1-1}e{-#2#3}\1_{[0;+\infty)}(#3)} %fonction densité de la loi gamma(n,lambda,x)

\newcommand{\denskhideux}[2]{\densgam{\frac{#1}{2}}{2}{#2}}%densité de la loi du khi deux (n,x)$

\newcommand{\Xbarre}{\overline{X_n}}

%Convergence

\newcommand{\cvn}{\underset{n\rightarrow+\infty}{\longrightarrow}} %normale

\newcommand{\cvu}{\overset{\text{$\cal U$}}{\cvn}} % converge uniforme

\newcommand{\cvps}{\overset{\text{p.s.}}{\cvn}} % converge p.s. vers

\newcommand{\cvP}{\overset{P}{\cvn}} % converge en proba vers

\newcommand{\cvLp}[1]{\overset{L^#1}{\cvn}} % converge dans L^p vers

\newcommand{\cvl}{\overset{\mathcal (d)}{\cvn}} % converge en loi vers

%utilitaires mathématiques complexes

%inscrire \[ juste avant
\newcommand{\accogauche}{
    \left\{
    \begin{aligned}
}

% Définir une commande \finaccogauche
\newcommand{\finaccogauche}{
    \end{aligned}
    \right.
}
%puis \] après

\newcommand{\accodroite}{$$\left. % \text{les 4 solutions sont ainsi :}\left\{{ puis }\right. si je voulais l'accolade à gauche
\begin{array}{rrr}
}

\newcommand{\finaccodroite}[1]{\end{array}
\right \} 
#1$$
}

%utilitaire générale

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

%Mes choix particuliers




\begin{document}         

\onehalfspacing %interligne de 1.5

\begin{titlepage}

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE }\\[2cm] % Name of your university/college
\vspace{-1em}

\includegraphics[width=0.9\textwidth]{/Users/gabriel/Desktop/LateX/images/ensae.png} 
 % Include a department/university logo - this will require the graphicx package
 \vspace{6em}
 
\textsc{\Large Mathématiques}\\[0.5cm] % Major heading such as course name
\textsc{\large }\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.7cm]
{ \huge \bfseries Théorie des Probabilités et Statistiques }\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large \vspace{5em}
\centering
\emph{Auteur:}\\
Gabriel \textsc{François}\\ % Your name
\end{flushleft}

\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\tableofcontents
\newpage


%\color{lightblue}

\section{Introduction}

\petitespace

L'objectif de ce polycopié est de synthétiser les connaissances essentielles en théorie des probabilités et en statistiques pour un niveau de Master 2. Il est structuré de manière à ce que chaque section puisse servir d'aide-mémoire pour les connaisseurs, ce qui explique que certains éléments des premiers chapitres sur la théorie des probabilités soient utilisés avant d'être formellement introduits. Les lecteurs souhaitant utiliser ce cours comme base initiale devront se référer aux sections introductives des concepts en question. Il s'adresse à toute personne ayant des bases solides en théorie de la mesure et en analyse. Les preuves sont mises en évidence en bleu pour les distinguer des propositions et théorèmes. Cependant, les martingales et la théorie des processus stochastiques ne sont pas abordées ici ; l'accent est mis sur la théorie des probabilités essentielle et les statistiques, en vue de leur application dans l'apprentissage automatique ou la recherche en général.

Cet enseignement s'adresse principalement aux étudiants de niveau Master 2 en mathématiques, statistiques, ou dans des domaines connexes nécessitant une solide compréhension des probabilités et des statistiques. Il est aussi conçu pour les chercheurs et professionnels souhaitant disposer d'un référentiel pratique pour se remémorer rapidement les concepts clés, ainsi que les preuves associées. Les divers chapitres couvrent un large éventail de sujets, allant des lois de probabilités fondamentales aux notions avancées de convergence et aux vecteurs aléatoires, en passant par les espérances conditionnelles et les tests statistiques. Mon objectif est de fournir une ressource complète qui rassemble tout ce qu'il est essentiel de savoir en théorie des probabilités et en statistiques, en incluant un maximum de preuves détaillées. Ces preuves, mises en évidence, visent à approfondir la compréhension des concepts et à servir d'aide-mémoire et de gain de temps pour les lecteurs, qu'ils soient novices ou experts dans le domaine. Ainsi, ce polycopié se veut un outil pratique et accessible, permettant à chacun de se familiariser avec ces sujets complexes et de les appliquer dans leurs travaux de recherche ou leurs projets professionnels.

Ce polycopié est structuré pour offrir une présentation rigoureuse et concise des concepts essentiels en théorie des probabilités et en statistiques. Chaque chapitre est conçu pour être à la fois autonome et complémentaire, permettant une consultation efficace et ciblée. L'objectif est de fournir un outil exhaustif où chaque notion est expliquée avec précision et accompagnée de preuves détaillées. Cette approche vise à aller droit au but, en privilégiant la rigueur mathématique pour servir d'aide-mémoire fiable aux étudiants et professionnels. Les chapitres sont organisés de manière logique, facilitant ainsi une compréhension progressive et approfondie des sujets abordés.



\newpage

\section{Lois de Probabilités à Connaître}

\subsection{Lois Discrètes}

$\boxed{X \sim \pois{\lambda}, \lambda>0} \shift \mathbb{P}(X=k)= \frac{e^{-\lambda}\lambda^k}{k!}$ ; $\mathbb{E}(X)=\lambda$ ;  $\mathbb{V}(X)=\lambda$ ; $\varphi_X(t)=e^{\lambda(e^{it}-1)}$\vspace{2em}


$\boxed{X \sim \mathcal{U}(\{1;...;n\})}$ \shift $ \mathbb{P}(X=k)= \frac{1}{n}$ ; $ \mathbb{E}(X)= \frac{n+1}{2}$ ; $\mathbb{V}(X)=\frac{n^2-1}{12}$ ; $\varphi_X(t)=\frac{e^{it}}{n}\frac{e^{itn}-1}{e^{it}-1}$\vspace{2em}


$\boxed{X \sim \bin{n}{p}}$ $ \mathbb{P}(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$ ; $ \mathbb{E}(X)=np$ ; $\mathbb{V}(X)=np(1-p)$ ; $\varphi_X(t)=(1+p(e^{it}-1))^n$\vspace{2em}


$\boxed{X \sim \mathcal{G}_\mathbb{N^*}(p)}$ \shift $\mathbb{P}(X=k)=(1-p)^{k-1}p$ ; $ \mathbb{E}(X)=\frac{1}{p}$ ; $\mathbb{V}(X)=\frac{1-p}{p^2}$ ;  $\varphi_X(t)= \frac{pe^{it}}{1-(1-p)e^{it}}$


\subsection{Lois Continues}

$\boxed{X \sim \cal{E}(\lambda),\lambda >0}$  $f_X(x)=\lambda e^{-\lambda x}$ ; $ \mathbb{E}(X)=\frac{1}{\lambda}$ ; $ \mathbb{V}(X)= \frac{1}{\lambda^2} $ ; $\varphi_X(t)=\frac{\lambda}{\lambda-it}$ ; Soit $X_1,....,X_n \sim  \mathcal{E}(\lambda) $ \petitespace

Alors $\sum\limits_{i=1}^nX_i \sim \gam{n}{\frac{1}{\lambda}}$ et $\sum\limits_{i=1}^nX_i \sim \Gamma^{-1}(n,\lambda)$ ; Soit $a\in \R, aX \sim \Expo{\frac{\lambda}{a}}$ ; $\forall k \in \N, \esp{X^k} = \frac{k!}{\lambda^k}$\\\\




$\boxed{X \sim \gam{n}{\lambda}, n, \lambda>0}$ ; $f_X(x)=\frac{1}{\lambda^n\Gamma(n)}x^{n-1}e^{-\frac{x}{\lambda}}$ ; $ \mathbb{E}(X)=n\lambda$ ; $ \mathbb{V}(X)=n\lambda^2$ ; si $X \sim \gam{a}{\lambda}$ et \petitespace

$Y\sim\gam{b}{\lambda}, X\indep Y$ alors $X+Y \sim \gam{a+b}{\lambda}$ ; $\varphi_X(t)=(\frac{1}{1-\lambda it})^n$ ;  si $X \sim \gam{n}{\lambda}$ alors $\forall c \in \bb{R}$ \petitespace

$cX \sim \gam{n}{\lambda c}$ ;
$\forall k \in \N, \esp{X^k}=\lambda^k\produit{i=1}{k}(n+i-1)$\vspace{2em}



$\boxed{X \sim \Gamma^{-1}(n,\lambda), n, \lambda>0}$ ; $f_X(x) = \frac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x}\1_{[0,+\infty]}(x)$ ; 
$\esp{X} = \frac n\lambda$ ; $\Var{X} = \frac{n}{\lambda^2}$ ; $c\in\R,$ \petitespace

$cX \sim \gamma^{-1}(n,\frac \lambda c)$ ; $X \sim \gaminv{a}{\lambda}$ et $Y\sim\gaminv{b}{\lambda}, X\indep Y$ alors $X+Y \sim \gaminv{a+b}{\lambda}$ \petitespace

 $\varphi_X(t) = (\frac{\lambda}{\lambda-it})^n$, $\gaminv{n}{\lambda}=\gam{n}{\frac 1\lambda}$



\vspace{2em}



$\boxed{X \sim \mathcal{U}([a,b])}$ ; $f_X(x)=\frac{1}{b-a}\mathbb{1}_{(b-a)}(x)$ ; $F(x) = \frac{x-a}{b-a}\1_{[a,b]}(x)+\1_{(a, +\infty)}(x) $ ; $\esp{X}=\frac{a+b}2$ ; $\Var{X}=\frac{(b-a)^2}{12}$ \petitespace

 $\varphi_X(t)= \frac{e^{itb}-e^{ita}}{it(b-a)}$   \vspace{2em}




$ \boxed{X\sim \normale{\mu}{\sigma^2}}$ ; $f_X(x)=\densnorm{\mu}{\sigma}{x}$ ; $\Xunan \sim \normale{0}{1 } \implies \sum\limits_{i=1}^nX_i^2 \sim \chideux{n}$  \petitespace

$\varphi_X(t) = e^{i\mu t - \frac{1}{2} \sigma^2 t^2}$ ; Si $(X_i)_{1\le i \le n} \overset{\indep}{\sim} \normale{\mu_i}{\sigma_i^2}, t_1 \ldots t_n \in \R, \somme{i=1}{n}t_iX_i \sim \normale{\somme{i=1}{n}t_i\mu_i}{\somme{i=1}{n}t_i^2\sigma_i^2}$\vspace{2em}



$ \boxed{X\sim \cal N_d(\mu,\Sigma), \mu \in \R^d, \Sigma \in \cal M_{d\times d}(\R)}$ ; $f_X(x) = \frac{1}{(2\pi)^{d/2}\sqrt{ det(\Sigma)}}e^{-\frac12 (x-\mu)^T\Sigma^{-1}(x-\mu)}$ ; $\esp{X}=\mu$ \petitespace

$\Var{X}=\Sigma$ ; $\varphi_X(t) = e^{it^T\mu -\frac 12 t^T\Sigma t}$ (Voir chapitre vecteur gaussiens pour plus de détails) \vspace{2em}



$\boxed{X\sim \chideux{n}}$ ; $X\sim \chideux{n} \iff X \sim \gam{\frac{n}{2}}{2} = \gaminv{\frac n2}{\frac 12}$ ; $f_X(x)=\densgam{\frac{n}{2}}{2}{x}$\petitespace

  $\esp{\overline{X}_n}=n$ ; $\bb{V}(\overline{X}_n)=2n$\vspace{2em}


$\boxed{X\sim \text{Cauchy}(\lambda, a), \lambda \in \R, a>0}$ $f_X(x) = \frac{a}{\pi ((x-\lambda)^2+a^2)}$ ; d'où, si $\lambda =0, x=1$ on a $f_X(x) = \frac{1}{\pi (x^2+1)}$ \petitespace

 $\esp{X} = +\infty$ ; $\varphi_X(t) = e^{i\lambda t-a\lvert t\rvert}$ ; $F_X(t) = \frac 12 + \frac 1\pi \text{arctan}(\frac{x-\lambda}{a}) $ \vspace{2em}

$\boxed{X\sim \text{Beta}(\alpha, \beta)}$  $\alpha, \beta>0$ $f_X(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}\1_{(0,1)}(x)$ avec $B(\alpha, \beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$ $\esp{X}=\frac{\alpha}{\alpha+\beta}$\petitespace

 $\Var{X}=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ Si $Y \sim \cal U([0,1]), \forall r >0, Y^r \sim \text{Beta}(\frac 1r, 1)$ \petitespace
 
 Si $Z \sim\text{Beta}(\alpha, 1)$ Alors $\log(Z)\sim \Expo{\alpha}$


\newpage

\section{Glossaire des Notations}

$\bullet$ Les scalaires ou variables aléatoires notés en \textbf{gras} correspondent à des \textbf{vecteurs} $\bf x, \bf X, \bf u$ \textit{etc}.

$\bullet$ Les matrices sont notées en majuscule.

$\bullet$ Soit $f : \R^p \mapsto \R^d$ une fonction dérivable en un point $\bf x =\begin{pmatrix} x_1 \\ \vdots \\ x_p  \end{pmatrix}   \in \R^p$. On note alors \petitespace

$$\nabla f(\bf x) = \begin{pmatrix} &\frac{\d f_1}{\d x_1}(\bf x) & \ldots & \frac{\d f_d}{\d x_1}(\bf x) \\ &\vdots &\ddots &\vdots \\ & \frac{\d f_1}{\d x_p}(\bf x) & \ldots & \frac{\d f_d}{\d x_p}(\bf x) \end{pmatrix} \in \cal M_{p \times d}(\R)$$

Avec 


$$\forall i \in \ldbrack 1, p \rdbrack,\quad f_i(\bf x) : \R^p \mapsto \R\mid \quad f(\bf x) = \begin{pmatrix} f_1(\bf x) \\ \vdots \\ f_d( \bf x)    \end{pmatrix} $$

On notera aussi $J_f(\bf x) = (\nabla f(\bf x))^T \in \cal M_{d \times p}(\R)$

$\bullet$ Soit $f : \R^p \mapsto \R$ une fonction dérivable en un point $\bf x =\begin{pmatrix} x_1 \\ \vdots \\ x_p  \end{pmatrix}   \in \R^p$. On note alors \petitespace

$$\nabla^2 f(\bf x) = \begin{pmatrix} &\frac{\d^2 f}{\d x_1^2}(\bf x) & \ldots & \frac{\d^2 f}{\d x_1\d x_p}(\bf x) \\ &\vdots &\ddots &\vdots \\ & \frac{\d^2 f}{\d x_p\d x_1}(\bf x) & \ldots & \frac{\d^2 f}{\d x_p^2}(\bf x) \end{pmatrix} \in \cal M_{p \times p}(\R)$$\petitespace


$\bullet$ "$X \sim Q$" signifie que la loi de $X$ notée $P_X$ est $Q$.

$\bullet$ $\lambda$ définira ici la mesure de Lebesgue

$\bullet$ $\log$ représente sans autres précisions le logarithme naturel \ie : $ \log(e^x) = x, \forall x \in \R$

$\bullet$  $\Xbarre = \frac 1n \somme{i=1}{n} X_i$


$\bullet$  Pour un modèle statistique $\modelstat$ donné, on a $l(x,\theta) = \log(f(x, \theta)))$ avec $f(x,\theta) = \frac{d\P_\theta}{d \mu}(x)$ et on écrira $l'(x, \theta) = \frac{\d l(x,\theta)}{\d \theta}$.

$\bullet$  On notera le cardinal d'un ensemble fini $E$ : $\#E$ ou bien $|E|$.

$\bullet$ L'ensemble \(E^N\) représente l'ensemble de tous les \(N\)-uplets \((x_1, x_2, \ldots, x_N)\) où chaque \(x_i \in E\) pour \(i = 1, 2, \ldots, N\), où $N$ peut être un ensemble indénombrable.


\newpage


\section{Dénombrement}

Avant de définir les variables aléatoires et les espaces probabilisés, nous allons rappeler les bases du dénombrement. Le dénombrement est une branche des mathématiques qui se concentre sur le comptage des objets. Il s'agit de déterminer le nombre d'éléments dans un ensemble fini ou de calculer le nombre de configurations possibles dans un certain cadre. C'est un domaine très utile pour répondre à des questions telles que  "combien de façons différentes peut-on arranger $n$ objets distincts ?" ou "combien de sous-ensembles peut-on former à partir d'un ensemble de $n$ éléments ?". Son utilisation se retrouve partout, et notamment en probabilités et en statistiques. C'est pourquoi nous aborderons ces notions, qui sont pour certains élémentaires mais fondamentales. On s'intéressera ici au Dénombrement dans des ensembles finis. 

Cette partie peut être allègrement sautée par ceux ayant déjà les bases dans ce domaine, cependant, un peu de rafraîchissement ne peut jamais faire de mal.

\subsection{Principes de Base du Dénombrement}
\petitespace

\subsubsection{Définitions et Propriétés Élémentaires} \espace



\Def
Soit $E$ un ensemble. On définit l'ensemble $N_n = \big\{ x \in \N \mid x <n \big\}$.  On dit que $E$ est finit de \textbf{cardinal} $n \in \N$ \ie : $ |E| = n$ (ou bien $\# E = n$) si $E$ est en bijection avec $N_n$. C'est-à-dire qu'il existe une application bijective $f$ de $E$ dans $N_n$. \petitespace

\thm 
 Soit $A$ une partie d'un ensemble fini $E$. Alors $A$ est elle-même finie et $|A| \le |E|$. 
 
 Si en outre, $|A| = |E|$ alors $A = E$ (La preuve est immédiate). 
 \petitespace
 
 
\Prop 
 
Soit $E$ un ensemble fini, et $F$ un ensemble et $f : E \mapsto F$ une application. On a :\petitespace
 
 \shift (i) $|f(E)| \le |E|$ \petitespace
 
 \shift (ii) $f$ est injective  $\iff |f(E)| = |E|$\petitespace
 
\corr{\textbf{Preuve :}

\cercler 1 On précise que comme $f$ est une application, elle est définie sur tout l'espace $E$. On a que $f : E \mapsto f(E)$ avec $f(E) = \{ f(x) : x \in A \} $ est une surjection.  Ainsi, 


$$|f(E)|\le |E|$$
 
 \cercler 2 Si $f$ est injective, alors tout les éléments de $f(E)$ ont un antécédent unique. Ainsi, l'application $f : E \mapsto f(E)$ est bijective, et donc $|f(E)|= |E|$. 

Réciproquement, si $|f(E)| = |E|$, alors $f : E \mapsto f(E)$ est injective (par construction) et par conséquent bijective, puisque $|f(E)| = |E|$. Donc, sachant que $E \subset F$, on a $f : E \mapsto F$ est injective. 
\petitespace}
 
 \subsubsection{Principe d'Addition}
 
 \petitespace
 
 Le principe d'addition stipule que si un événement peut se produire de $n$ manières différentes et un autre événement, qui ne peut pas se produire en même temps que le premier, peut se produire de $m$ manières différentes, alors le nombre total de façons dont l'un ou l'autre événement peut se produire est donné par $n+m$. En d'autres termes, si deux événements sont mutuellement exclusifs, le nombre total de résultats possibles est la somme des résultats possibles pour chaque événement.
 
 \petitespace
 
 \Prop
 Soit $E$ et $F$ deux ensembles finis disjoints avec $|E| = k \in \N$ et $|F| = n \in \N$. Alors on a 
 
 $$|E \cup F| = |E| + |F| = k + n$$
 
 \petitespace
 
\corr{ \textbf{Preuve :}

Soit $f$ une bijection de $E$ dans $\ldbrack 1, k \rdbrack$ ($\ldbrack 1, k \rdbrack= \{ 1, 2, \ldots , k \}$) et $g$ une bijection de $F$ dans $\ldbrack 1+k, n+k  \rdbrack$. On peut alors construire $h$ une application de $E \cup F$ dans $\ldbrack 1, n+k \rdbrack$ telle que sa restriction à $E$ est $f$ et sa restriction $F$ est $g$.  Comme $h$ est une bijection, c'est une injection, et donc $|h(E \cup F)| = |\ldbrack 1, k+n \rdbrack| = n+k$ par la proposition précédente.
\petitespace}

\textbf{corollaire}

Par récurrence on a bien que pour $(E_i)_{1 \le i \le n}$ une suite d'ensemble finis disjoints. Alors 

$$\big|\union{i=1}{n} E_i \big| = \somme{i=1}{n} |E_i| $$ 

 \Prop
 Soit $(E_n)_{n \in \N}$ une famille dénombrable d'ensemble disjoints. Alors 
 
 $$\big|\union{i=1}{+\infty} E_i\big| = \somme{i=1}{+\infty}|E_i| $$
 
 \corr{ \textbf{Preuve :}
 
 Chaque $E_i$ est finit, donc on peut dire que pour tout $i \in \N$, il existe $n_i \in \N$ tel que $|E_i| = |\{0, \ldots, n_i \}| = n_i$. Pour chaque $i \in \N, E_i = \{ e_{i,1}, \ldots, e_{i, n_i} \}$. Comme les $E_i$ sont disjoints, on peut étiqueter chaque élément dans $\union{i=1}{+\infty} E_i$ par le couple $(i,j)$ avec $j \in \{1, \ldots, n_i \}$. Pour $A = \{ (i,j) \in \N \times\N \mid 1 \le j \le n_i \}$, on définit l'application par :
 
 $$f : \union{i=1}{+\infty} E_i \mapsto A $$
 
avec pour tout $e_{i,j} \in \union{i=1}{+\infty} E_i$, $f(e_{i,j}) = (i,j)$. Cette fonction est clairement injective car les $(E_i)_{i in \N}$ sont disjoints et surjective car tout élément de $A$ correspond à un élément de $(E_i)$.

On termine puisque, par définition, $|A| = \somme{i=1}{+\infty} n_i = \somme{i=1}{+\infty}  |E_i|$ donc il existe une bijection entre $\union{i=1}{+\infty} E_i$ et $A$ et $|A| = \somme{i=1}{+\infty}  |E_i|$. Donc finalement on a 

$$ \big|\union{i=1}{+\infty} E_i\big| = \somme{i=1}{+\infty}|E_i| $$
 \petitespace}



\subsubsection{Principe de Multiplication}

En combinatoire, le principe de multiplication stipule que si une tâche peut être accomplie de $n$ manières différentes et qu'une autre tâche peut être accomplie de $m$ manières différentes, alors les deux tâches peuvent être accomplies ensemble de $n\times m$ manières différentes.\petitespace

\Prop
Soit $E$ et $F$ deux ensembles finis de cardinaux respectifs $n$ et $k$. Alors $E \times F$ est fini de cardinal 

$$|E\times F| = nk $$

\petitespace

\corr{\textbf{Preuve :}

On a $E\times F = \{ (e,f) \mid e \in E, f \in F \}$ avec $E = \{e_1, \ldots ,e_n \}$ et  $F = \{f_1, \ldots ,f_n \}$ or donc 

$$ E\times F =  \{(e_i,f_j) \mid 1 \le i \le n, 1 \le j \le k\} $$

Chaque paires $(i,j)$ donne une paire unique $(e_e, f_j)$, et comme il y a $n$ choix pour $i$ et $k$ pour $j$, alors 

$$|E\times F| =  |\{(e_i,f_j) \mid 1 \le i \le n, 1 \le j \le k\}| =nk $$
\petitespace}

\textbf{corollaire}

Soit $n \in \N$ et $(E_i)_{1 \le i \le n}$ une famille d'ensembles finis. Alors :

$$|E\times \ldots \times E_n| = \produit{i=1}{n}|E_i| $$

\Rq

Contrairement au principe d'addition, ce dernier ne se généralise pas pour un produit cartésien infini. 

\petitespace

\subsubsection{Partitions d'un Ensemble}\espace

\Def
Soit $S$ un ensemble. Une \textbf{partition} d'un ensemble $S$ est une collection (pas nécessairement finie ni même dénombrable) de sous-ensembles non vides de $S$ tels que : \petitespace

\shift 1) \textbf{Recouvrement} : L'union de tous les sous-ensembles de la partition est égale à $S$. \petitespace

\shift 2) \textbf{Disjoints} : Les sous-ensembles sont deux à deux disjoints, c'est-à-dire que l'intersection de deux sous-ensembles distincts est vide. \petitespace

\Rq

Il est à noter que lorsque l'on introduira une partition, elle sera, sans informations supplémentaire au plus dénombrable, si elle ne l'est pas, ça sera précisé. 

\espace

\Prop
Soit $E$ un ensemble fini tel que $|E|=k$. Alors si on définis $\cal P(E)$ comme l'ensemble des parties de $E$, on a 

$$|\cal P(E)| = 2^k $$


\corr{\textbf{Preuve :}

Soit une application $f : E \mapsto \{0,1\} $. On définit l'ensemble $A_f = \{ x \in E \mid f(x) = 1 \}$ et on obtient une bijection entre l'ensemble $\cal F = \big\{ f \mid f : E \mapsto \{0,1\} \big\}$ et $\cal P(E)$. L'application en question associe à chaque $f \in \cal F$ l'ensemble $A_f \in \cal P(E)$, nous allons démontrer qu'elle est bijective.

En effet, on a l'injection car si $f \ne g$ alors $A_f \ne A_g$. De plus, pour tout $A \in \cal P(E)$ on définit l'application 

$$f_A(x) = \begin{cases} 1 \quad &\text{si $x \in A$} \\ 0 \quad &\text{sinon} \end{cases} $$ 

Donc pour tout $x \in \cal P(E)$, on peut lui associer une fonction $f \in \cal F$ par l'application définie plus haut. Ainsi on a la bijection et donc $|\cal P(E)| = |\cal F|$.Maintenant, regardons $\cal F$ : chaque application $f: E \mapsto \{0,1\} $ associe à un $x \in E$ 0 ou 1. Il y a donc 2 choix pour chaque $x$. Comme $|E| = k$ on a donc $2^k$ $f$ possible. Ainsi : 

$$|\cal F| = |\cal P(E)| = 2^k $$
}

\subsection{Permutations}\petitespace

Une situation fréquente en combinatoire consiste à ordonner tous les éléments d’un ensemble donné. Une \textbf{permutation} désigne une réorganisation (ou réarrangement) complète des éléments d’un ensemble fini, c’est-à-dire une bijection de cet ensemble sur lui-même.
\petitespace

\Def
Soit $E$ un ensemble. On appelle \textbf{permutations de $E$} l'ensemble des bijections de $E$ dans lui-même.


\espace

\subsubsection{Permutations sans répétitions}\espace

\Prop
Soit $E$ un ensemble fini à $n$ élément. Alors 

$$|E| = n! $$
\petitespace

\corr{\textbf{Preuve :}
Soit $\sigma : E \mapsto E$ une bijection (donc une permutation). Notons $E = \{ x_1, \ldots, x_n \}$ alors pour construire une permutation $f$ de $E$ dans $E$ : \petitespace

\shift $\bullet$  pour $x_1$ on a $n$ choix d'image pour $\sigma$. \petitespace

\shift $\bullet$ pour $x_2$, on a plus que $n-1$ choix (pour éviter l'image de $x_1$. 

$$\vdots $$

\shift $\bullet$ Pour le dernier choix, il ne reste plus qu'une possibilité.\petitespace


Ainsi, on a bien $n(n-1) \ldots 1 = n!$ nombre de permutations possibles.
\petitespace}

\underline{Exemple} : \petitespace

Si on considère un ensemble $E = \{1,2,3\}$, alors les différentes manières de réorganiser tous ses éléments correspondent aux permutations de $E$. Ces permutations sont : $(1,2,3)$, $(1,3,2)$, $(2,1,3)$, $(2,3,1)$, $(3,1,2)$, et $(3,2,1)$ soit $6$ au total.

\espace

\subsubsection{Permutations avec Répétition}\petitespace

Dans certaines situations, on souhaite permuter tous les éléments d’un ensemble, mais certains éléments sont identiques. Cela donne lieu à ce qu’on appelle des \textbf{permutations avec répétition}.

Contrairement aux permutations classiques (où tous les éléments sont distincts), ici, certaines configurations deviennent indiscernables du fait de la répétition d’éléments.
 \espace

\Def
Soit $E = \{ x_1, \ldots, x_n \}$ un ensemble fini de taille $n$. Soit aussi $k_1, \ldots , k_n$ des entiers naturels tel que $k_1 + \ldots + k_n= k \in \N$. \textbf{Une permutation} de $k$ éléments de $E$ avec $k_1, \ldots, k_n$ \textbf{répétitions} est un n-uplet (une collection ordonnée et finie) d'éléments de $E$ dans lequel chacun des éléments $x_1, \ldots, x_n$ apparaissent respectivement $k_1, \ldots, k_n$ fois. \petitespace

\underline{Exemple} :\petitespace

Soit $E = \{ x_1, \ldots, x_n \}$ un ensemble fini de taille $n$. Et soit $k_1, \ldots , k_n$ des entiers naturels tel que $k_1 + \ldots + k_n= k \in \N$. Le n-uplet $$(\underbrace{x_1, \ldots, x_1}_{k_1-\text{fois}}, \ldots, \underbrace{x_n, \ldots, x_n}_{k_n-\text{fois}} )$$

est une permutation avec répétition particulière. \petitespace


\Prop 
Le nombre de permutations de n éléments avec $k_1, \ldots, k_n$ répétitions est égal à 

$$\frac{k!}{k_1! k_2!\ldots k_n!} $$

Ce nombre est connu sous le nom de \textbf{coefficient multinomial} et se note habituellement 

$$\begin{pmatrix} k \\ k_1, k_2, \ldots, k_n \end{pmatrix}=\frac{k!}{k_1! k_2!\ldots k_n!} $$ 
\petitespace

\corr{\textbf{Preuve :}

Considérons qu'il y a $k$ positions distinctes à remplir dans le  $k$-uplet. Chaque position contiendra un élément de $E$, et l'élément $x_i$ doit apparaître exactement $k_i$ fois.

Pour le premier élément $x_1$, on doit choisir $k_1$ positions parmi les $k$ restantes. Il s'agit donc d'une combinaison car l'ordre ne compte pas, de $k_1$ parmi $k$ (voir combinaison en $\boldsymbol{4.4}$). Ainsi, on a :

$$C_k^{k_1} = \begin{pmatrix} k \\k_1 \end{pmatrix}=\frac{k!}{k_1!(k-k_1)!} $$

Après cette étape, pour placer $x_2$, on doit choisir $k_2$ parmi $k-k_1$ positions donc $C_{k-k_1}^{k_2}$. On généralise donc à $k_i, i \in \ldbrack 1, n\rdbrack$, et on obtient que pour placer $x_i$ on a donc 

$$C_{k-\somme{j=1}{i-1} k_j}^{k_i} = \begin{pmatrix} k-\somme{j=1}{i-1} k_j \\k_i \end{pmatrix}= \frac{(k-\somme{j=1}{i-1} k_j)!}{k_i!(k-\somme{j=1}{i} k_j)!} \quad \text{possibilités}$$

Pour le dernier éléments, toutes les places sont prises donc il n'y a qu'une seule façon de les arranger.

Ainsi, on a un nombre total de possibilités est le produit des choix pour chaque étape, car les étapes sont séquentielles et indépendantes (le choix des positions pour un élément n'affecte pas les choix ultérieurs, si l'ordre des éléments est fixé). On a donc un nombre de possibilités égales à :

$$\frac{k!}{k_1!(k-k_1)!} \times \frac{(k-k_1)!}{k_2!(k-k_1)!} \times \ldots \times \frac{(k-k_1- \ldots-k_{n-1})!}{k_n!(0)!}  = \frac{k!}{k_1! k_2!\ldots k_n!} $$
\petitespace}

\underline{Exemple} :\petitespace

Considérons le mot \texttt{ABA}. Il contient trois lettres, dont certaines sont identiques. On veut déterminer combien de mots distincts on peut former en permutant les lettres de ce mot.

\begin{itemize}
    \item Il y a au total $n = 3$ lettres.
    \item La lettre \texttt{A} apparaît $2$ fois.
    \item La lettre \texttt{B} apparaît $1$ fois.
\end{itemize}

Si toutes les lettres étaient distinctes, le nombre de permutations serait $3! = 6$. Mais ici, échanger les deux lettres \texttt{A} ne produit pas un nouveau mot distinct. Il faut donc corriger ce comptage en divisant par le nombre de permutations internes possibles des lettres répétées.

Le nombre de permutations distinctes est donné par le \textbf{coefficient multinomial} :
\[
\frac{3!}{2! \cdot 1!} = \frac{6}{2} = 3
\]

On peut les lister pour vérifier :

\begin{center}
\begin{tabular}{lll}
1. & \texttt{AAB} \\
2. & \texttt{ABA} \\
3. & \texttt{BAA}
\end{tabular}
\end{center}

On retrouve bien $3$ arrangements distincts.

\medskip

\noindent\textbf{Formule générale.} Si un mot de $n$ lettres contient :
\[
n_1 \text{ lettres de type } a_1,\quad n_2 \text{ lettres de type } a_2,\quad \dots,\quad n_k \text{ lettres de type } a_k
\]
avec \( n_1 + n_2 + \cdots + n_k = n \), alors le nombre de permutations distinctes est donné par :
\[
\binom{n}{n_1, n_2, \dots, n_k} = \frac{n!}{n_1! \cdot n_2! \cdots n_k!}
\]



\espace

\subsection{Arrangements}\petitespace

Dans de nombreux problèmes de dénombrement, on s'intéresse à la manière de sélectionner et d’ordonner un certain nombre d’éléments à partir d’un ensemble donné. Lorsque \textbf{l’ordre des éléments choisis a de l’importance}, on parle alors d’\textbf{arrangements}.

Autrement dit, un arrangement de $k$ éléments parmi $n$ correspond à un choix ordonné de $k$ éléments distincts (ou non, selon le contexte) pris dans un ensemble de $n$ éléments.

Par exemple, dans une course de $10$ coureurs, combien de podiums différents (1\textsuperscript{er}, 2\textsuperscript{e}, 3\textsuperscript{e}) peut-on former ? Ici, l’ordre dans lequel les coureurs montent sur le podium est crucial, et le problème consiste donc à compter des arrangements. Les arrangements permettent de modéliser des situations où la position, la hiérarchie ou l’ordre chronologique jouent un rôle déterminant.

Dans ce qui suit, nous étudierons deux types d’arrangements :
\begin{itemize}
    \item les \textbf{arrangements sans répétition}, où chaque élément de l’ensemble ne peut apparaître qu’une seule fois dans l’arrangement ;
    \item les \textbf{arrangements avec répétition}, où un même élément peut être utilisé plusieurs fois.
\end{itemize}

\petitespace

\subsubsection{Arrangements sans Répétitions}\espace

\Prop
Soit $E= \{ x_1, \ldots, x_n \}$ un ensemble fini à $n$ éléments et $k \in \N$ tel que $k \le n$. Le nombre de manières de choisir $k$ objets parmi les $n$ objets de $E$, chaque objet ne pouvant être choisi qu'une fois et l’ordre du choix étant pris en compte, est donné par 

$$A_n^k = \frac{n!}{(n-k)!} $$

\corr{\textbf{Preuve :}

Analysons les choix possibles à chaque étape. La construction d'un tel $k$-uplet se fait en choisissant successivement les éléments pour chaque position, de la première à la $k$-ième position, sans remise. Analysons les choix possibles à chaque étape :


Pour le premier élément $x_1$, on a $n$ choix possible pour le placer. 

Pour le second élément $x_2$ on a $n-1$ possibilités 

On déduit donc que pour le $i$-ième élément, $i \in \ldbrack 1, k \rdbrack$, on a $n-k+1$ possibilités.  

Ainsi, on en déduit qu'on a donc

$$n \times (n-1) \times \ldots \times (n-k+1)! =  \frac{n!}{(n-k)!} =A_n^k  \quad \text{Possibilités}$$

Ainsi, le nombre d'arrangement possibles sans répétitions de $k$ éléments parmi $n$ est de $ \frac{n!}{(n-k)!}$
\petitespace}

\Rq

\cercler 1 L'arrangement de $n$ éléments parmi $n$, noté $A_n^n$ correspond donc à l'ensemble des permutations sans répétitions d'un ensemble $E$ à $n$ éléments. \petitespace 
 
\cercler 2 On en déduit que le nombre d'injections d'un ensemble à $k$ éléments dans un ensemble à $n$ éléments est égale à $A_n^k$ si $k \le n$ et 0 sinon. 

\petitespace

\subsubsection{Arrangements avec Répétitions}\espace

\Prop
Soit $E$ un ensemble fini de taille $n$ et $k \le n$. Le nombre de façons de choisir $k$ élément de $E$ parmi ses $n$ éléments, chaque élément pouvant être choisi plusieurs fois et l’ordre du choix étant pris en compte est noté $B_n^k$ et est égal à $n^k$. 

\petitespace

\corr{\textbf{Preuve :}

Posons $E= \{ x_1, \ldots, x_n \}$. La preuve est en réalité élémentaire.

Un choix ordonné de $k$ éléments de $E$ avec répétitions autorisées correspond à une séquence ordonnée de longueur $k$ (un $k$-uplet) où chaque composante est un élément de $E$. Ainsi, Pour construire une telle séquence, nous avons $k$ positions à remplir (position 1, position 2, ..., position $k$).  À chaque position, nous sélectionnons un élément de $E$. Comme les répétitions sont  autorisées, le choix à chaque position est indépendant des choix précédents et il n'y a aucune restriction sur le nombre de fois qu'un élément peut apparaître. Ainsi, à chaque fois on a $n$ possibilités pour chaque positions $i \in \ldbrack 1, k\rdbrack$ ce qui donne un total de 


$$\overbrace{n \times ... \times n}^{n \text{ fois}} = n^k  \quad \text{possibilités}$$
\petitespace}
\underline{Exemple} : \petitespace

Considérons l’ensemble $E = \{a, b\}$ de $n = 2$ éléments, et supposons que nous voulons former des mots de longueur $k = 3$ en utilisant les éléments de $E$, avec \textbf{répétition autorisée}.

Chaque position dans le mot peut être remplie par n’importe quel élément de $E$. Il y a donc :
\[
\text{nombre d’arrangements avec répétition} = n^k = 2^3 = 8
\]

On peut vérifier cela en listant toutes les possibilités :

\begin{center}
\begin{tabular}{cccc}
1. & \texttt{aaa} & 5. & \texttt{bab} \\
2. & \texttt{aab} & 6. & \texttt{bba} \\
3. & \texttt{aba} & 7. & \texttt{bbb} \\
4. & \texttt{abb} & 8. & \texttt{baa} \\
\end{tabular}
\end{center}

On obtient bien $8$ arrangements, ce qui confirme le résultat donné par la formule générale :
\[
n^k \quad \text{où } n = \text{nombre de symboles possibles, et } k = \text{nombre de positions.}
\]

\petitespace

\subsection{Combinaisons}\espace

Dans un grand nombre de problèmes de dénombrement, il est essentiel de savoir compter combien de sous-ensembles de taille donnée peuvent être formés à partir d’un ensemble initial. 

Lorsque l’ordre des éléments choisis "n’a pas d’importance", on parle alors de \textbf{combinaisons}. Autrement dit, choisir une combinaison de $k$ éléments parmi $n$ revient à sélectionner un sous-ensemble de $k$ éléments dans un ensemble de $n$ éléments, sans se soucier de l’ordre dans lequel ces éléments sont choisis.

Les combinaisons jouent un rôle fondamental en probabilité, en statistiques, et dans l’analyse de situations où seule la composition d’un groupe importe, et non la façon dont ses éléments sont arrangés. Dans ce qui suit, nous distinguerons deux cas importants :
\begin{itemize}
    \item les \textbf{combinaisons sans répétition}, où chaque élément de l’ensemble de départ ne peut être choisi qu’une seule fois ;
    \item les \textbf{combinaisons avec répétition}, où chaque élément peut être choisi plusieurs fois.
\end{itemize}

\petitespace

\subsubsection{Combinaisons sans Répétition}\petitespace

Les combinaisons sans répétitions concernent le nombre de façons de choisir $k$ éléments d'un ensemble de $n$ éléments, où les répétitions ne sont pas autorisées et l'ordre n'est pas important. Cela signifie que chaque élément ne peut être choisi qu'une seule fois. Par exemple, si vous avez un ensemble de fruits $\{$Pomme, Banane, Cerise$\}$ et vous voulez choisir 2 fruits, les combinaisons possibles sont $\{$Pomme, Banane$\}$, $\{$Pomme, Cerise$\}$, et $\{$Cerise, Banane$\}$.  Les combinaisons sans répétitions sont couramment utilisées dans les problèmes de sélection où chaque élément ne peut être choisi qu'une seule fois, comme dans les tirages sans remise ou les problèmes de sous-ensembles. \petitespace

\Prop
Soit $E$ un ensemble fini à $n$ éléments et $k \in \N$ tel que $k \le n$. Le nombre de combinaisons (sans répétition) de $k$ éléments de $E$ noté $C_n^k$ correspond au nombre au nombre de possibilité de sélectionner de manière non ordonnée $k$ éléments de $E$ sans répétitions. Il est égal à 

$$C_n^k= \begin{pmatrix} n \\k \end{pmatrix}= \frac{n!}{k!(n-k)!} $$

\petitespace


\corr{\textbf{Preuve :}

Une combinaison d'un ensemble $E$ est un sous-ensemble de $k$ éléments distincts de $E$, où l'ordre n'a pas d'importance. Ainsi, chaque combinaison de $k$ éléments correspond à $k!$ arrangements différents. 

En effet, puisque que l'ensemble des permutations possible dans un ensemble à $k$ élément est égal à $k!$, il y a $k!$ possibilités d'ordonner les $k$ éléments. Donc un arrangement de $k$ éléments parmi $n$ équivaut à $k!$ combinaisons de $k$ parmi $n$. Ainsi, on a 

$$C_n^k = \frac{A_n^k}{k!} = \frac{n!}{k!(n-k)!}  $$


\petitespace}

\underline{Exemple} : \petitespace

\cercler 1 Si vous avez un ensemble de 3 éléments distincts $\{A,B,C\}$ et vous voulez choisir 2 éléments sans répétition, le nombre de combinaisons possibles est donné par :

$$C_3^2 =  \begin{pmatrix} 3 \\2 \end{pmatrix} =\frac{3!}{2!(3-2)!} =3 $$\petitespace

Les combinaisons possibles sont en effet au nombre de 3 : $\{A,B\},\{A,C\},\{B,C\}$. 

\petitespace

\cercler 2 Calcul du nombre de paires$(i,j)$ tels que $1 \le i <j \le n, n \in \N$.

Il s'agit d'un problème d'ordre combinatoire. En effet, le 1 calcul du nombre de paires$(i,j)$ tels que $1 \le i <j \le n, n \in \N$ revient à calculer la somme 

$$ \somme{1 \le i <j}{n} 1$$

Ce problème revient à compter le nombre de façons de choisir 2 indices distincts parmi $n$, où l'ordre n'a pas d'importance. L'ordre n' a pas d'importance puisqu'on a pour tout $(i,j), i<j$ l'ensemble $(j,i)$ qui est identique à $(i,j)$ donc on ne compte qu'une seule fois les paires $(i,j), i<j$ . Ainsi, chaque paire $(i,j), i <j$ correspond à un sous-ensemble non ordonné de 2 éléments choisis parmi $n$ indices. on a donc 

$$\begin{pmatrix} n \\2 \end{pmatrix}  =  \frac{n!}{2(n-2)!}$$

\espace

\subsubsection{Combinaisons avec répétition}\espace

Les combinaisons avec répétitions permettent de déterminer le nombre de façons de choisir $k$ éléments d'un ensemble de $n$ éléments, où les répétitions sont autorisées et l'ordre n'est pas important. Cela signifie que vous pouvez choisir le même élément plus d'une fois. Par exemple, si vous avez un ensemble de fruits $\{$Pomme,Banane, Cerise$\}$, et vous voulez choisir 2 fruits, vous pouvez avoir des combinaisons comme $\{$Pomme, Pomme$\}$, $\{$Pomme, Banane$\}$, $\{$Banane, Banane$\}$ \textit{etc}. Les combinaisons avec répétitions sont utiles dans des situations où les éléments peuvent être sélectionnés plus d'une fois, comme dans les problèmes de distribution ou de sélection avec remise. \petitespace

\Prop
Soit $E$ un ensemble fini de taille $n$ et $k \le n$. Le nombre de possibilités différentes de sélectionner de manière ordonnée $k$ éléments de $E$ sans répétitions est égal à 

$$ \begin{pmatrix} n+k-1 \\k \end{pmatrix}=\frac{(n+k-1)!}{k!(n-1)!}$$
\petitespace

\corr{\textbf{Preuve :}

On pose $E = \{ x_1, \ldots, x_n \}$,  Nous voulons choisir $k$ éléments de $E$ avec répétitions autorisées, sans tenir compte de l'ordre. Cela revient à déterminer le nombre de d'ensemble différents de taille $k$ formés à partir des éléments de $E$, ou de manière équivalente, le nombre de solutions en entiers naturels de l'équation :

$$\somme{î=1}{n} X_i = k, \quad \text{$X_i \ge 0$ est le nombre de fois où $x_i$ est choisi} $$

Considérons $k*$ représentant les $k$ éléments choisis. Pour séparer ces étoiles en $n$ groupes (un groupe par élément de $E$), plaçons $n-1$ barres "|" entre les étoiles. Chaque groupe correspond au nombre d'occurrences d'un élément de $E$.

Par exemple, pour $n=3$ et $k=2$, la solution $X_1 = 1,X_2= 1,X_3 = 0$ est représentée *|*|. 

Chaque séquence de $k*$ et de $n-1$ "|" correspond à une unique solution de l'équation $\somme{î=1}{n} X_i = k$. Dans une telle séquence, il y a $n-1+k$ symbols. Le nombre de manières différentes de combiner ces symbols est le nombre de façons de choisir les positions des $k*$ parmi $n-1$ . Il s'agit d'une combinaison car les $k*$ et les |"" sont indistinguables, et donc il y a :

$$ \frac{(n+k-1)!}{k!(n-1)!} \quad \text{possibilités} $$


\petitespace}

\underline{Exemple} :\petitespace

Si vous avez un ensemble de 3 éléments distincts $\{A,B,C\}$ et vous voulez choisir 2 éléments avec répétition autorisée, le nombre de combinaisons possibles est :

$$ \begin{pmatrix} 3+2-1 \\2 \end{pmatrix}=\begin{pmatrix} 4 \\2 \end{pmatrix}=\frac{(4)!}{2!(4-2)!} = 6$$\petitespace

En effet, Les combinaisons possibles sont : $\{A,A\},\{A,B\},\{A,C\},\{B,B\},\{B,C\},\{C,C\}$

\newpage

\section{Espace probabilisé et mesure de probabilité}\espace

\subsection{Définitions et Exemples}\espace

L'étude des espaces probabilisés se fonde sur la théorie de la mesure. On considèrera donc ce dernier champs comme acquis. Les notations seront cependant légèrement différentes, pour respecter les conventions établies en théorie des probabilités.

\petitespace

\Def

Soit $(\Omega, \cal A)$ un espace mesurable. Une \textbf{probabilité} sur $(\Omega, \cal A)$ est une fonction $P : \cal A \mapsto \R_+$ telle que : \petitespace

\shift (i) $P(\emptyset) = 0$ \petitespace

\shift (ii) $P(\Omega)=1$  \petitespace

\shift (iii) Pour toute suite $(A_n)_{n\ge 1}$ d'éléments 2 à 2 disjoints de $(\cal A)$ : 

$$ P\left(\bigcup_{n\geq 1}A_n\right)=\sum_{n=1}^\infty P(A_n) $$


$\OmegaAP$ est alors appelé "espace de probabilité". Dans ce contexte, les éléments de la tribu \(\mathcal A\) sont appelés des "événements".

\petitespace

\Rq 

Pour tout \(A\in\mathcal A\), \(P(A^c)=1-P(A)\). Il suffit de considérer \(A_1=A\), \(A_2=A^c\) et \(A_n=\emptyset\) pour tout \(n\geq 3\).

\petitespace

\underline{Exemples} : \petitespace

1) $\Omega = \{ 1, \ldots 6 \}$ et $\cal A = \cal P(\Omega)$

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  P :& \cal P(\Omega) &\mapsto&\R \\
  & A & \mapsto &  \frac{\# A }{6} \end{array}$$

Alors $P$ est une probabilité sur $(\Omega, \cal A)$. $P(\emptyset) = \frac 06 = 0 \quad P(\Omega) = \frac{6}{6} = 1$ et si $(A_n)_{n \ge 1}$ suite d'événements deux à deux disjoints de $\cal A$, on a $\# (\union{n \ge 1}{} A_n ) = \somme{n \ge 1}{} \# A_n$, donc 

$$P(\union{n \ge 1}{} A_n ) = \somme{n=1}{\infty} P(A_n) $$

\petitespace

2) Généralisation $\Omega = \{a_1, \ldots, a_k \}, \quad k \ge 1$. Soient $p_1, p_2, \ldots, p_k \ge 0$ tels que $p_1+ \ldots+p_k = 1$. Soit 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  P :& \cal P(\Omega) &\mapsto&\R \\
  & A & \mapsto & \somme{i\in \ldbrack 1,k \rdbrack,  a_i \in A}{} p_i = \somme{i=1}{k} p_i \1_{A}(a_i) \end{array}$$


Alors $P$ est une probabilité sur $(\Omega, \cal A)$.\petitespace

En effet, $P(\emptyset) = 0 \quad P(\Omega) = \somme{i=1}{k}p_i = 1$ et si $(A_n)_{n \ge 1}$ suite d'événements deux à deux disjoints de $\cal A$, on a $\1_{\big\{ \union{n \ge 1}{} A_n\big\} } = \somme{n=1}{\infty} \1_{A_n} $ et donc $P(\union{n \ge 1}{} A_n) = \somme{n=1}{\infty} P(A_n)$


\petitespace

3) Soit $f : \R \mapsto \R$ une fonction mesurable positive telle que $\int_\R f(x) dx = 1$ et soit 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  P :& \cal B(\R)  &\mapsto&\R \\
  & A & \mapsto &  \int_A f(x)dx \end{array}$$

Alors $P$ est une probabilité sur $(\R, \cal B(\R))$.

En effet, pour  $(A_n)_{n \ge 1}$ suite d'événements deux à deux disjoints de $\cal B(\R)$ on a 

$$\1_{\big\{ \union{n \ge 1}{} A_n \big\} } = \somme{n \ge 1}{}\1_{A_n} $$

\begin{align*}
	\int_{\union{n \ge 1}{} A_n}f(x) dx &= \int_\R f(x) \1_{\big\{ \union{n \ge 1}{} A_n \big\} }(x) dx \\
	&= \int_\R f(x)\somme{n \ge 1}{}\1_{A_n}(x)dx \\
	& = \somme{n \ge 1}{} \int_\R f(x) \1_{A_n}(x)dx\\
	&= \somme{n \ge 1}{} P(A_n)
\end{align*}

\espace

\Prop

Soit \((\Omega,\mathcal A,P)\) un espace probabilisé. Soient \(A,B\in\mathcal A\).

        \begin{enumerate}
            \item Si \(A\cap B=\emptyset\) alors \(P(A\cup B)=P(A)+P(B)\)
            \item \(P(A^{\setminus B})=P(A)-P(A\cap B)\)
            \item \(P(A^c)=1-P(A)\)
            \item \(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)
            \item "Croissance de \(P\)". Si \(A\subset B\) alors \(P(A)\leq P(B)\)
            \item "Borne d'union".
            \begin{enumerate}
                \item \(P(A\cup B)\leq P(A)+P(B)\)
                \item Si \((A_n)_{n\geq 1}\) est une suite d'événements alors \(P\left(\bigcup\limits_{n\geq 1}A_n\right)\leq \sum_{n=1}^\infty P(A_n)\), où cette somme peut être éventuellement infinie.
            \end{enumerate}
            \item Si \((A_n)_{n\geq 1}\) est une suite croissante d'événements alors \(P\left(\bigcup\limits_{n\geq 1}A_n\right)=\Lim{n\to\infty}P(A_n)\).
            \item Si \((A_n)_{n\geq 1}\) est une suite décroissante d'événements alors \(P\left(\bigcap\limits_{n\geq 1}A_n\right)=\Lim{n\to\infty}P(A_n)\).
        \end{enumerate}

\petitespace

\corr{\textbf{Preuve :}

\cercler 1 On considère la suite $(A_n)_{n \ge 1}$ telle que $A_1 = A, A_2 = B, A_k = \emptyset,\quad \forall k \ge 3$ et on a 

$$P(A\cup B) = \P(\union{i=1}{n} A_i) = P(A) + P(B) $$

\petitespace

\cercler 2 On a $A^{\setminus B} \cap (A\cap B) = \emptyset$, et $A^{\setminus B} \cup (A\cap B) = A$ donc 

$$P(A) = P(A^{\setminus B} \cup (A\cap B)) = P(A^{\setminus B}) + P(A\cap B )  $$

\cercler 3 On a $\Omega = A \cup A^c$ avec bien évidemment $A\cap A^c = \emptyset$ donc 

$$1=P(\Omega) = P(A \cup A^c) = P(A) + P(A^c) $$

\cercler 4 On a $A^{\setminus B} \cap (A\cap B) = \emptyset$ donc 

$$P(A^{\setminus B} \cup (A\cap B)) = P(A^{\setminus B}) + P(B) = P(A) + P(B)-P(A \cap B)$$

\cercler 5 

$$P(B) = P(B^{\setminus A}\cup A) = P(B^{\setminus A})-P(A) \ge P(A)$$

\cercler 6 On a pour le premier point

$$P(A\cup B) = P(A) + P(B)-P(A\cap B) \le P(A)+P(B) $$

Pour le second point, on a 

$$P(\bigcup\limits_{n\geq 1}A_n) = P(A_1) + P(\bigcup\limits_{n\geq 2}A_n^{\setminus A_1 \cap \ldots A_{n-1}}) =P(A_1)+ \somme{n\ge 2}{} P(A_n^{\setminus A_1 \cap \ldots A_{n-1}}) \le \somme{n \ge 1}{} P(A_n)$$

car pour tout $n \ge 1, A_n^{\setminus A_1 \cap \ldots A_{n-1}} \subset A_n$. \petitespace

\cercler 7 On a 

\begin{align*}
	P\left(\bigcup\limits_{n\geq 1}A_n\right)& =  P(A_1) + P(\bigcup\limits_{n\geq 2}A_n^{\setminus A_{n-1}})\\
	& =P(A_1)+ \somme{n \ge 2}{} P(A_n^{\setminus A_{n-1}})\\
	&= \Lim{n \to \infty} P(A_n)
\end{align*}

\cercler 8 On a de même que si $A_1 \supset A_2 \ldots \supset A_n$, alors $A_1^c \subset \ldots \subset A_n^c$ donc $(A_n^c)_{n \ge 1}$ est une suite croissante d'événements. Ainsi 

$$P(\union{n \ge 1}{} A_n^c) = \Lim{n \to \infty}P(A_n^c) \overbrace{\iff}^{\text{passaga au complémentaire}} P(\inter{n \ge 1}{} A_n) = \Lim{n \to \infty}P(A_n)$$


\petitespace}





\subsection{Expériences Aléatoires et Modélisation}



\petitespace


 Une expérience aléatoire est une expérience dans laquelle on modélise le résultat de manière aléatoire.
    
    Une manière naturelle consiste à définir \(\Omega\) comme l'ensemble des résultats possibles de l'expérience et \(\mathcal A\) comme la tribu engendrée par les événements observables de l'expérience, puis en définissant une probabilité \(P\) sur \((\Omega,\mathcal A)\) permettant de quantifier l'aléa.

 Ici, un "événement observable" est une partie \(A\) de \(\Omega\) telle qu'on peut déterminer, à l'issue de l'expérience, si le résultat est bien dans \(A\).
 \petitespace

\textbf{Exemples.} \petitespace

\begin{enumerate}
        \item Lancé d'un dé équilibré à 6 faces. Ici on connaît le résultat du lancer.
        
        On prend \(\Omega=\{1,2,3,4,5,6\}\) et \(\mathcal A=\mathcal P(\Omega)\) et
        \[\begin{array}{rrcl}
            P: & \mathcal P(\Omega) & \longrightarrow & \mathbb R \\
            & A & \longmapsto & \frac{\#A}{6}
        \end{array}\]
        \item Lancé d'un dé équilibré mais on n'observe que la parité du résultat. Ici on a uniquement l'information sur la parité du résultat.
    
        On prend \(\Omega=\{1,2,3,4,5,6\}\) et \(\mathcal A=\{\emptyset,\{1,3,5\},\{2,4,6\},\Omega\}\) et la même proba qu'avant.
    
        \item Une urne contient 6 boules : 3 jaunes, 1 rouge et 2 vertes. On en tire une au hasard. Deux boules de même couleur sont indiscernables.
    
        On prend \(\Omega=\{J_1,J_2,J_3,R,V_1,V_2\}\) et :
        \begin{center}
            \(\mathcal A=\Big\{\emptyset,\{J_1,J_2,J_3\},\{R\},\{V_1,V_2\},\) \(\{J_1,J_2,J_3,R\},\{J_1,J_2,J_3,V_1,V_2\},\{R,V_1,V_2\},\Omega\Big\}\)
        \end{center}
    
        \item Lancé d'une craie dans un bac à sable de longueur \(L\in\mathbb R_+^*\).
    
        On prend \(\Omega=[0,L]\) et \(\mathcal A=\mathcal B([0,L])\), ainsi que
        \[\begin{array}{rrcl}
            P: & \mathcal B([0,L]) & \longrightarrow & \mathbb R \\
            & A & \longmapsto & \frac{\lambda(A)}L, \text{\shift $\lambda$ la mesure de Lebesgues dans $\R$}
        \end{array}\]
    \end{enumerate}



\subsection{Probabilités Conditionnelles} 

\petitespace

Soit \((\Omega,\mathcal A,P)\) un espace de probabilité.

\petitespace

\Def

Soit \(A\in\mathcal A\) tel que \(P(A)\ne 0\). Soit \(P_A:\begin{array}[t]{rcl}
        \mathcal A&\to&\mathbb R \\
        B&\mapsto &\frac{P(A\cap B)}{P(A)}
    \end{array}\).\petitespace

On appelle cette application \textbf{probabilité conditionnelle} sachant \(A\) et on la note généralement pour mesurer un événement $B \in \cal A$ : $P(B|A)$. $P_A$ est une probabilité sur $(\Omega, \cal A)$ (c'est trivial à montrer). 

\petitespace

\Prop

Soient \(A,B\in\mathcal A\) tels que \(P(A)\ne 0\) et \(P(B)\ne 0\).

\petitespace
    \begin{itemize}
        \item \(P(A\cap B)=P(A)P(B\mid A)=P(B)P(A\mid B)\)
        \item \textbf{Formule de Bayes.} \(P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B)}\).
        \item Généralisation. Soient \(n\geq 2\) et \(A_1,\dots,A_n\in\mathcal A\) tels que \(P(A_1\cap \cdots \cap A_{n-1})\ne 0\).

        Pour des raisons d'inclusion cela implique que pour tout \(k\in\{1,\dots,n-1\}\), \(P(A_1\cap\cdots\cap A_k)\ne 0\).
        
        Alors \(P(A_1\cap\cdots\cap A_n)=P(A_1)P(A_2\mid A_1)\cdots P(A_n\mid A_1\cap \cdots \cap A_{n-1})\).
        \item \textbf{Formule des probabilités totales.} Soient \(A_1,\dots,A_n\in\mathcal A\) formant une partition de \(\Omega\) tels que pour tout \(i\in\{1,\dots,n\},P(A_i)\ne 0\) (partition voulant dire  $\union{i=1}{n} A_i = \Omega$ et $\forall i \ne j, A_i \cap A_j = \emptyset $). Alors
        \[\forall B\in\mathcal A,\quad P(B)=\sum_{i=1}^nP(B\mid A_i)P(A_i)\]
    \end{itemize}





\newpage





\section{Variables Aléatoires}\espace


\Def 


Une \textbf{variable aléatoire}  est une application allant d'un espace probabilisé $\OmegaAP$ dans $\EE$ \\ mesurable.

\subsection{Indépendances}

\petitespace

On considère dans cette partie un espace probabilisé $\OmegaAP$. 

Ce peut être par exemple $(\Omega, \cal A) = (\R, \cal B(\R), P)$, avec $\cal B (\R)$ le borélien de $\R$ et $P$ une mesure sur $(\R, \cal B(\R))$.

On peut aussi avoir par exemple $(\Omega, \cal A, P) = (\{ 1,2, \ldots, 6 \}, \cal P(\{ 1,2, \ldots, 6 \}), P )$, et $P$ mesure de probabilité sur $(\{ 1,2, \ldots, 6 \}, \cal P(\{ 1,2, \ldots, 6 \}))$

\petitespace

\Def \textbf{indépendance mutuelle} \petitespace

1) Soit $n \ge 2$ et $A_1\ldots A_n\in \cal A$. On dit que $A_1\ldots A_n$ sont mutuellement indépendants si et seulement si 

$$\forall I \subset \{1, \ldots n \}, P(\inter{i\in I}{}A_i) = \produit{i\in I}{}P(A_i)\shift (1)$$

Avec les conventions $\inter{i\in \emptyset}{}A_i=\Omega	$ et $\produit{i\in \emptyset}{}u_i =1$ \vspace{2em}


2) Soit $(A_n)_{n \ge 1}$  une suite d’événements. 

On dit que les $A_n, n \ge 1$, sont mutuellement indépendants si et seulement si pour toute partie \textbf{finie} $I$ de $\N^*$, les $A_i$, $i \in I$ sont indépendants au sens de (1) \petitespace

\corr{\textbf{Preuve :}

\cercler 1 Démontrons la première implication $(\implies)$. Si pour tout $I \subset \N^*$ fini, les $A_i, i \in I$ sont indépendants, soit $n \in \N^*$, en prenant $I = \{ 1, \ldots n \}$, les $A_1, \ldots, A_n$ sont indépendants.

\petitespace

\cercler 2 $(\Longleftarrow)$ Supposons que pour tout $n \in \N^*$, $A_1, \ldots, A_n$ sont indépendants. Et soit $I\subset \N^*$ fini, si $I = \emptyset, $ alors $ P(\union{i \in \emptyset}{}A_i) = \produit{i\in \emptyset}{} P(A_i) = 1$. Si $I \ne \emptyset,$ on pose $n= \max(I)$. Par hypothèse, les $A_1, \ldots, A_n$ sont indépendants, donc 

$$\forall J \subset \{1, \ldots, n\}, \quad P(\inter{j \in J}{} A_j) = \produit{j \in J}{} P(A_j) $$

Et comme $I \subset \{ 1, \ldots n\}$, c'est encore vrai pour tout $J \subset I$. Donc les $(A_i)_{i \in I}$ sont indépendants.

\petitespace}

\Def
\textbf{Indépendances de tribus :} \petitespace

1) Soient $\mathcal{A}_1, \ldots \mathcal{A}_n, n \ge 2$ des sous tribus de $\cal A$. On dit que $\mathcal{A}_1, \ldots \mathcal{A}_n$ sont mutuellement indépendantes si et seulement si 	pour tout $A_1 \ldots A_n\in \mathcal{A}_1\times \ldots \times\mathcal{A}_n$ sont indépendants.\vspace{2em}


2) Soit $(\cal A)_{n \ge 1}$ une suite de sous-tribus de $\cal A$. On dit que les $\cal A_n, n \ge 1, $ sont indépendantes si et seulement si pour tout $I\subset \N^*$ \textbf{fini}, les $\cal A_i$, $i \in I$ sont indépendantes au sens de la première partie de la définition. \petitespace


\Prop 

Soient $(F_1, \cal F_1)\ldots (F_n, \cal F_n)$ des espaces mesurables.

Et soient $f_1 : (E_1, \cal E_1) \to (F_1, \cal F_1) \ldots f_n : (E_n, \cal E_n) \to  (F_n, \cal F_n)$ des applications mesurables.

Si $\Xunan$ sont indépendantes, alors $f_1(X_1)\ldots f_n(X_n)$ le sont aussi.\petitespace

\corr{\textbf{Preuve :}

On sait que les tribus $\sigma(X_i), \forall i \in \ldbrack 1, n \rdbrack$ sont indépendantes, or $\forall B_i \in \cal E_i$ on a 


$$(f_i \circ X_i(B_i))^{-1}=X_i^{-1}\circ f_i^{-1}(B_i)\subset \sigma(X_i)$$

Car $f_i^{-1}(B_i) \in \cal E_i$ puisque $f_i$ est $\cal E_i $-mesurable.

Donc on a bien pour tout $i\in \ldbrack 1,n \rdbrack, \quad \sigma(f(X_i))\subset \sigma(X_i)$. Donc $f_1(X_1)\ldots f_n(X_n)$ sont indépendantes.

\petitespace}

\Def
\textbf{Indépendances de lois de probabilités : }\petitespace

$\Xunan$, des variables aléatoires à valeur dans $(E_1, \cal E_1)\ldots (E_n, \cal E_n)$ sont indépendantes si et seulement si les tribus $\sigma(X_1) \ldots \sigma(X_n)$ sont indépendantes ($\sigma(X) = \sigma(\{ X^{-1}(B):B \in \cal E\}))$, ce qui est le cas si et seulement si pour tout $B_1 \in \cal E_1, \ldots B_n \in \cal E_n$, on a 


$$P(X_1 \in B_1, \ldots X_n \in B_n) = \produit{i=1}{n} P(X_i\in B_i)$$

De manière équivalente, $\Xunan$ sont indépendantes si et seulement si leur loi jointe est le produit de leurs lois marginales, \ie

$$P_{(\Xunan)} = P_{X_1}\otimes \ldots \otimes \P_{X_n}$$

\petitespace

\corr{\textbf{Preuve :} 

"$\Longleftarrow$" : Soient $B_1 \in \cal E_1, \ldots, B_n \in \cal E_n$ on a alors 

$$P_{(\Xunan)}(B_1 \times \ldots \times B_n) = (P_{X_1} \otimes \ldots \otimes P_{X_n})(B_1, \ldots, B_n) $$

C'est-à-dire 

$$P((\Xunan) \in (B_1 \times \ldots \times B_n)) = \produit{i=1}{n} P_{X_i}(B_i)$$

Donc $\Xunan$ sont indépendants. \petitespace

"$\implies$" : Soient $B_1 \in \cal E_1, \ldots, B_n \in \cal E_n$, alors :

$$P_{(\Xunan)}(B_1 \times \ldots \times B_n) = (P_{X_1} \otimes \ldots \otimes P_{X_n})(B_1, \ldots, B_n)  $$

donc les probabilités $P_{(\Xunan)}$ et $ P_{X_1} \otimes \ldots \otimes P_{X_n}$ coïncident sur le $\pi-$système

$$\cal C = \big\{  B_1 \times \ldots \times B_n \mid B_1 \in \cal E_1, \ldots, B_n \in \cal E_n \big\} $$

Donc coïncident sur $\sigma(\cal C) = \cal E_1 \otimes \ldots \otimes \cal E_n$ tout entier.



\petitespace}


\Prop

 Soient \(X_1,\dots,X_n\) des v.a \textbf{indépendantes} à valeurs dans des espaces mesurables \((E_1,\mathcal E_1),\dots,(E_n,\mathcal E_n)\) respectivement. Supposons que pour tout \(i=1,\dots,n\), \(X_i\) admet une densité \(f_i\) par rapport à une mesure de référence \(\nu_i\) (voir plus loin dans le chapitre pour la définition des densités). Alors \((X_1,\dots,X_n)\) admet une densité par rapport à la mesure \(\nu_1\otimes\cdots\otimes\nu_n\), donnée par
    
$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  f :& E_1 \times \ldots \times E_n &\mapsto&\R \\
  & (x_1, \ldots, x_n) & \mapsto &  \produit{i=1}{n}f_i(x_i) \end{array}$$
  
\petitespace  

\corr{\textbf{Preuve :}

Soient $B_1 \in \cal E_1, \ldots, B_n \in \cal E_n$ on a alors 

\begin{align*}
	P(X_1 \in B_1, \ldots, X_n \in B_n) &= \int_{B_1 \times \ldots \times B_n}f(x_1, \ldots, x_n) d \nu_1 \otimes \ldots \otimes \nu_n(x_1, \ldots, x_n) \\
	&= \int_{B_1 \times \ldots \times B_n} \produit{i=1}{n}f_i(x_i) d \nu_1(x_1) \ldots d\nu_n(x_n) \\
	\text{(Fubini-Tonelli)} \quad &= \produit{i=1}{n} \int_{B_i}f_i(x_i) d\nu_i(x_i) \\
	&= \produit{i=1}{n} P(X_i \in B_i)
\end{align*}

Donc $\Xunan$ sont indépendantes. 
\petitespace}

\Prop 

Soient $(E_1, \cal E_1)\ldots (E_n, \cal E_n)$ des espaces mesurables ($n \ge 2$). Soient $\Xunan$ des variables aléatoires dans $E_1\ldots E_n$ respectivement.

Les assertions suivantes sont équivalentes : \petitespace

\shift (i) $\Xunan$ sont indépendantes. \petitespace

\shift (ii) Pour toutes fonctions mesurables positives $g_1 : E_1 \to \R_+, \ldots g_n : E_n \to \R_+$ on a $$\esp{g_1(X_1)\ldots g_n(X_n)}=\esp{g_1(X_1)}\ldots\esp{g_n(X_n)}$$



\shift (iii) Pour toutes fonctions $g_1 : E_1 \to \R, \ldots g_n : E_n \to \R$ mesurables, telles que $g_i\in L^1(P_{X_i}),$

\shift\shift  $\forall i \in \ldbrack 1, n \rdbrack$ on a : \petitespace

\hspace{4em}  $\bullet$  $g_1\otimes\ldots \otimes g_n\in L^1(P_{(\Xunan)})$

\hspace{4em} $\bullet$ $\esp{g_1(X_1)\ldots g_n(X_n)}=\esp{g_1(X_1)}\ldots\esp{g_n(X_n)}$

(voir plus loin dans le chapitre pour la définition des espérances)
\espace

\corr{\textbf{Preuve :}

Procédons en 3 étapes. 

\cercler 1 "$(i) \implies (ii)$" : Supposons que $\Xunan$ soient indépendantes dans $(E_i, \cal E_i), i \in \ldbrack 1, n \rdbrack$. Posons pour tout $i \in \ldbrack 1, n \rdbrack, g_i : E_i \mapsto \R_+$ des fonctions mesurables positives. Alors :

\begin{align*}
	\esp{g_1(X_1)  \ldots  g_n(X_n)} &= 	\int_{E_1 \times \ldots \times E_n} g_1(x_1) \ldots g_n(x_n) dP_{\Xunan}(x_1, \ldots, x_n) \\
	&= \int_{E_1 \times \ldots \times E_n} g_1(x_1) \ldots g_n(x_n) dP_{X_1} \otimes \ldots \otimes P_{X_n}(x_1, \ldots, x_n) \text{ car $\indep$}\\
	&= \produit{i=1}{n} \int_{E_i} g_i(x_i) d P_{X_i}(x_i) \text{ (Par Fubini-Tonnelli)} \\
	&= \produit{i=1}{n} \esp{g_i(X_i)}
\end{align*} 

\cercler 2 "$(ii) \implies (iii)$" Soient pour tout $i \in \ldbrack 1, n \rdbrack, g_i : E_i \mapsto \R$ des fonctions mesurables telles que $g_i \in L^1(P_{X_i})$. Pour commencer, comme on a que $|g_i|$ est positives (mesurables positives) on a :

\begin{align*}
	 \esp{\produit{i=1}{n} |g_i(X_i)|}&= \produit{i=1}{n} \esp{ |g_i(X_i)|} < \infty
\end{align*}

Ainsi, $|g_1 \otimes \ldots \otimes g_n| $ est intégrable, donc, par Fubini, on a $g_1 \otimes \ldots \otimes g_n\in L^1(P_{\Xunan})$

Montrons maintenant l'égalité des espérances. Pour chaque $i\in \ldbrack 1, n \rdbrack$, décomposons les $g_i = g_i^+-g_i^-$ (comme en théorie de la mesure), avec $g_i^+ = \max(0, g_i), g_i^- = \max(0,-g_i)$ mesurables positives. Alors :

$$g_1(X_1) \ldots g_n(X_n) = \produit{i=1}{n} (g_i^+(X_i)-g_i^-(X_i)) = \somme{\epsilon \in \{0,1\}^n}{}(-1)^{\sigma(\epsilon)}\produit{i=1}{n}g_i^{\epsilon_i}(X_i)$$

où $\epsilon = (\epsilon_1, \ldots, \epsilon_n), \epsilon_i \in \{+,-\}$, et $\sigma(\epsilon)$ est le nombre de $\epsilon_i$ égaux à $-$ dans $\epsilon$. Par linéarité de l'espérance, on a 
 
 \begin{align*}
	\esp{g_1(X_1) \ldots g_n(X_n)} &=  \somme{\epsilon \in \{0,1\}^n}{}(-1)^{\sigma(\epsilon)} \esp{\produit{i=1}{n}g_i^{\epsilon_i}(X_i)} \\
	&=  \somme{\epsilon \in \{0,1\}^n}{} (-1)^{\sigma(\epsilon)}\produit{i=1}{n}\esp{g_i^{\epsilon_i}(X_i)} \quad \text{(Fubini-Tonelli)}\\
\end{align*}

Or, on remarque que 

$$\produit{i=1}{n}\esp{g_i(X_i)} =  \produit{i=1}{n}\big(\esp{g_i^+(X_i)}-\esp{g_i^-(X_i)}\big) =  \somme{\epsilon \in \{0,1\}^n}{} (-1)^{\sigma(\epsilon)}\produit{i=1}{n}\esp{g_i^{\epsilon_i}(X_i)} $$

Ainsi, $\esp{g_1(X_1) \ldots g_n(X_n)} $ et $\produit{i=1}{n}\esp{g_i(X_i)}$ coïncident ce qui est le résultat recherché.

\cercler 3 "$(iii) \implies (i)$" Posons, pour tout $i \in \ldbrack 1, n \rdbrack$, $A_i \in \cal E_i$ et considérons les fonctions $g_i = \1_{A_i}$ des fonctions mesurables. Ces dernières sont trivialement dans $L^1(P_{X_i})$, puisque $g_i \le 1$, donc 

$$\esp{g(X_i)} = \int_{E_i} g_i(x) dP_{X_i}(x) \le 1\times \int_{E_i} dP_{X_i}(x) = P_{X_i}(E_i) = 1$$

également, on a 

$$\esp{g_1(X_1) \ldots g_n(_n)} =\esp{\1_{A_1}(X_1) \ldots \1_{A_n}(X_n)} = P(X_1 \in A_1, \ldots, A_n \in A_n) $$

et

$$\esp{g_1(X_1)} \ldots \esp{g_n(_n)} = \esp{\1_{A_1}(X_1)} \ldots \esp{\1_{A_n}(X_n)} = P(X_1 \in A_1) \ldots P(A_n \in A_n)$$

Ce qui démontre que $\Xunan$ sont indépendants. 
}

\subsection{Fonctions de Répartitions et Densités}\petitespace






\Def
Soit $X$ une variable aléatoire réelle, on définit sa \textbf{fonction de répartition} comme 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  F :& \R &\mapsto&[0,1]\\
  & t & \mapsto & P_X((-\infty,t]) = P(X\le t) \end{array}$$
  
  \Prop

  La fonction de répartition de $X$ suffit à déterminer sa loi. \petitespace

\Prop 

Soit $F$ la fonction de répartition d'une variable aléatoire réelle $X$, alors :\petitespace

\shift $\bullet$ $F$ est croissante \petitespace

\shift $\bullet$ $F$ est continue à droite et limitée à gauche \ie

$$\forall t \in \R \Lim{s \to t, s >t}F(s)=F(t) \text{\shift et \shift} \Lim{s \to t, s <t}F(s) \text{ existe, et vaut } P(X<t)$$ \petitespace

\shift $\bullet$ $F(t) \underset{t \to +\infty}{\longrightarrow} 1 \text{\shift et \shift} F(t) \underset{t \to -\infty}{\longrightarrow} 0$ \petitespace

\corr{\textbf{Preuve :}

\cercler 1 Montrons que $F$ est croissante. Pour $s, t \in \R, s \le t$, alors 

$$\{\omega \in \Omega \mid X(\omega) \le s\} \subset  \{\omega \in \Omega \mid X(\omega) \le t\}\implies F(s) \le F(t)$$

\cercler 2 Montrons que $F(n) \cvn 1$. Puisque $F$ est croissante, il suffit de montrer que $\Lim{t \to + \infty}F(t) = 1$. 

Pour $n \ge 1$ notons $A_n = \{\omega \in \Omega \mid X(\omega)\le n\}$ on a $\Lim{n \to +\infty}P(A_n) = P(\union{n \in \N^*}{}A_n)$ par croissance des $A_n$. 

Pour tout $\omega \in \Omega,$ on peut trouver $n \in \N^*\mid \omega \in A_n$ (en prenant par exemple $n = \lfloor X(\omega) \rfloor + 1$) donc $\Omega \subset \union{n \in \N^*}{}A_n$ Donc $\Omega = \union{n \in \N^*}{}A_n$ et donc $F(n) \cvn 1$\petitespace


\cercler 3 Soit $t \in \R$, montrons que $F$ est continue à droite en $t$, \ie : $F(x) \underset{x \to t, x>t}{\longrightarrow}F(t)$

Par croissance de $F$, il est suffisant de montrer que $F(t+\frac 1n) \cvn F(t)$, pour $n \ge 1$. Posons

 $B_n =\{\omega \in \Omega \mid X(\omega)\le t+\frac 1n\}$, on a donc $F(t+\frac 1n)=P(B_n), \forall n \ge 1$. Comme $(B_n)_{n \in \N^*}$ est une suite décroissante d'événements, on a :
 
 $$\Lim{x \to t,  x>t}F(x) = \Lim{n \to \infty}F(t+\frac 1n) = \Lim{n \to \infty}P(B_n) = P(\inter{n =1}{+\infty} B_n) $$ 

Or, $\inter{n \in \N^*}{} B_n = \{ \omega \in \Omega : \forall n \ge 1, X(\omega) \le t+\frac 1n\} = \{ \omega \in \Omega : X(\omega) \le t\}$ Donc $F(x) \underset{x \to t, x>t}{\longrightarrow}F(t)$\petitespace


\cercler 4 Soit $t \in \R$, montrons que $F$ est limitée à gauche \ie : $F(x) \underset{x \to t, x<t}{\longrightarrow} P(X <t)$. Il suffit de déterminer $\Lim{n \to \infty}F(t-\frac1n)$\petitespace

Pour $n \ge 1$, $C_n = \{ \omega \in \Omega : X(\omega)\le t-\frac 1n\}$, $(C_n)_{n \in \N^*}$ est une suite croissante d'événements, donc $\Lim{n \to + \infty}P(C_n) = P(\union{n \in \N^*}{}C_n)$, or $\union{n \in \N^*}{}C_n = \{ \omega \in \Omega \mid \exists n \ge 1 : X(\omega) \le t-\frac 1n\} = \{ \omega \in \Omega \mid X(\omega) < t\}$\petitespace

Donc $\Lim{n \to \infty}F(t-\frac1n) = P(X <t)$

\espace}



\textbf{Corollaire}

Les points de continuité de $F$ sont le $t \in \R$ qui ne sont pas des atomes de $X$ de sorte que 

$$\Lim{x \to t, x<t}F(x) = P(X<t)=P(X \le t)=F(t) $$

\petitespace

\Rq 


Soit $X$ un vecteur aléatoire dans $\R^d, d \ge 1$, et $x \in \R^d$, on dit que $x$ est un \textbf{atome} de $X$ si 

$$P(X=x) >0$$

\petitespace

\Def

Soit $X$ une variable aléatoire sur un espace probabilisé $\OmegaAP$ dans $\EE$, On dit que $X$ \textbf{admet une densité} par rapport à $\eta$ si, et seulement si il existe $f : E \to \R_+$ mesurable tel que $\forall B \in \cal E$,

$$P(X \in B) = \int_Bf(x)d\eta(x)$$

$f$ est alors appelée \textbf{densité} de $X$ par rapport à $\eta$\petitespace

\Prop

Si $X$ admet deux densités $f$ et $g$ par rapport à une mesure $\eta$, alors $f=g, \eta$ p.p. \ie :  $$\eta(\{x \in E:f(x)\ne g(x)\})=0$$

\corr{\textbf{Preuve :}

Soit \( X \) une variable aléatoire admettant deux densités \( f \) et \( g \) par rapport à une mesure \(\eta\). Nous voulons montrer que \( f = g \) \(\eta\)-presque partout, c'est-à-dire que :

\[
\eta(\{x \in E : f(x) \ne g(x)\}) = 0.
\]

Avec \(\eta\) est une mesure sur l'espace mesurable \( (E, \mathcal{E}) \).





    \cercler 1 \textbf{Ensemble de Désaccord :}
    \begin{itemize}
        \item Considérons l'ensemble \( D = \{x \in E : f(x) \ne g(x)\} \). Nous devons montrer que \( \eta(D) = 0 \).
    \end{itemize}

    \cercler 2 \textbf{Décomposition de \( D \) :}
    \begin{itemize}
        \item Décomposons \( D \) en deux ensembles disjoints $D_1, D_2$ tels que $D_1 \cup D_2 = D$
        \[
        D_1 = \{x \in E : f(x) > g(x)\} \quad \text{et} \quad D_2 = \{x \in E : f(x) < g(x)\}.
        \]
        \end{itemize}

    \cercler 3 \textbf{Intégrales sur \( D_1 \) et \( D_2 \) :}
    \begin{itemize}
        \item Considérons l'intégrale de \( f \) et \( g \) sur \( D_1 \) :
        \[
        \int_{D_1} f \, d\eta \quad \text{et} \quad \int_{D_1} g \, d\eta.
        \]
        \item Puisque \( f(x) > g(x) \) pour tout \( x \in D_1 \), nous avons :
        \[
        \int_{D_1} f \, d\eta > \int_{D_1} g \, d\eta.
        \]
        \item Cependant, comme \( f \) et \( g \) sont des densités de \( X \) par rapport à \( \eta \), nous avons :
        \[
        \int_{D_1} f \, d\eta = P(X \in D_1) = \int_{D_1} g \, d\eta.
        \]
        \item Cela implique que :
        \[
        \int_{D_1} (f - g) \, d\eta = 0.
        \]
        \item Puisque \( f - g > 0 \) sur \( D_1 \), cela implique que \( \eta(D_1) = 0 \).
        \item Avec le même raisonnement pour $D_2$, on obtient que $\eta(D_2) =0$.
    \end{itemize}

On conclut :


    \begin{itemize}
        \item Puisque \( \eta(D_1) = 0 \) et \( \eta(D_2) = 0 \), nous avons :
        \[
        \eta(D) = \eta(D_1 \cup D_2) = \eta(D_1) + \eta(D_2) = 0.
        \]
        \item Par conséquent, \( f = g \) presque partout par rapport à \( \eta \).
    \end{itemize}


Et donc :
\[
\eta(\{x \in E : f(x) \ne g(x)\}) = 0.
\]
}

\Def

Soit $\EE$ un espace mesurable tel que $\cal E$ contient tout les singletons. Une variable aléatoire $X$ dans $E$ est dite \textbf{discrète} si, t seulement si $P(X \in A)=1$, où $A$ est l'ensemble des atomes de $X$.\petitespace

\Rq

Cette dernière définition est équivalente à dire que $X$ une variable aléatoire est dite \textbf{discrète} si elle admet une densité par rapport à la mesure de comptage sur $A$ (l'ensemble des atomes de $X$) donnée par 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  f :& E &\mapsto&\R \\
  & x & \mapsto & P(X=x) \end{array}$$
  
  On rappelle que la mesure de comptage sur un sous-ensemble au plus dénombrable $F$ de $E$ est la mesure $\d$ sur $\EE$ donnée par 
  
  $$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}
  \d :& \EE &\mapsto&\R_+\cup \{\infty\} \\
  & B & \mapsto & \#(B \cap F) \end{array}$$
  
  Soit $f : E \to \R_+$  mesurable, alors
  
  $$\int_Ef(x)d\d(x) = \somme{x \in F}{}f(x)$$\petitespace
  
  \Def
  
  Une variable aléatoire dans $\R^d, d \ge 1$ est dite "continue" si, et seulement si elle admet une densité par rapport à la mesure de Lebesgue. \petitespace


\Prop

Une variable aléatoire dans $\R^d, d \ge 1$  continue n'admet pas d'atome. (La réciproque n'est pas vraie)\petitespace

\thm

\textbf{Formule du changement de variable}

Soit $X$ une variable aléatoire dans $\R^d$ admettant une densité $f$ par rapport à la mesure de Lebesgue.

Soit $U$ un ouvert de $\R^d$ tel que $X \in U$ p.s.. Soit $\varphi$ un $\cal C^1$-difféomorphisme de $U$ dans un ouvert $V$ de $\R^d$.

Alors $\varphi(X)$ admet une densité par rapport à la mesure de Lebesgue, donnée par 

$$
\begin{array}{rrrl}
g : & \mathbb{R}^d & \mapsto & \mathbb{R} \\
    & y         & \mapsto & \left\{
        \begin{aligned}
            & f\big(\varphi^{-1}(y)\big)  \left| \det J_{\varphi^{-1}}(y) \right| \text{Si $ y \in V$}\\
            & 0 \text{ Sinon}
        \end{aligned}
    \right.
\end{array}
$$
\petitespace

\corr{\textbf{preuve : }

$\forall A \in \cal B(\R^d)$, on a 

\begin{align*}
P(\varphi(X) \in A)&=P(\varphi(X) \in A, X \in U) + P(\varphi(X) \in A, X \notin U)\\
	&=P(\varphi(X) \in A, X \in U)\\
	&=P(X\in \varphi^{-1}(A)\cap U)\\
	&=\int_{\varphi^{-1}(A)\cap U}f(x)dx\\
	&=\int_U f(x)\1_{x \in \varphi^{-1}(A)}dx\\
\end{align*}

On effectue le changement de variable $y=\varphi(x), y \in V$ $ x=\varphi^{-1}(y)$ et donc $dx=|det J_{\varphi^{-1}(y)}|$
et on obtient 

\begin{align*}
	P(\varphi(X) \in A)&=\int_U f(x)\1_{x \in \varphi^{-1}(A)}dx\\
	&=\int_V f\big(\varphi^{-1}(y)\big)|det J_{\varphi^{-1}(y)}|\1_{y \in A}dy\\
	&=\int_A f\big(\varphi^{-1}(y)\big)|det J_{\varphi^{-1}(y)}|\1_{y \in V}dy \\
	&=\int_A g(y)dy
\end{align*}
}

\Rq 

On dit que $\varphi$ est un $\cal C^1$-difféomorphisme dans $V$ si : \petitespace

\shift $\bullet$ $\varphi$ est bijective de $U$ dans $V$ \petitespace

\shift $\bullet$ $\varphi$ est $\cal C^1(U)$  \petitespace

\shift $\bullet$ $\varphi^{-1} : V \mapsto U$ est $\cal C^1(V)$

\petitespace


\subsection{Espérance}\petitespace

\Def

Soit $X$ une variable aléatoire sur $\OmegaAP$ dans $\EE$. On définit l'espérance $\esp{X}$ tel que 

\[
\esp{X} = \accogauche
&\int_\Omega X(\omega)d P(\omega) \text{ si $X \in L^1( P)$}\\
&+\infty \text{ sinon}
\finaccogauche
\]

\Rq

Soit $X$ variable aléatoire dans $\OmegaAP$ dans un espace mesurable $\EE$ on a alors :

$$\esp{X} =\int_E x dP_X(x) $$

(C'est direct en remarquant que $P_X$ est la mesure image de $P$ par $X$).

\petitespace

Si $X$ admet une densité $f$ par rapport à une mesure $\eta$, alors on a 

$$\esp{X} =\int_E xf(x) d\eta(x)$$

\Prop

Soit $(X_n)_{n \in \N^*}$ une variable aléatoire réelle positive, alors 

$$\esp{X} = \int_0^\infty P(X \ge t)dt$$\petitespace

\corr{\textbf{preuve :}

\begin{align*}
	\esp{X} &= \int_\R xdP_X(x)dx \text{ (théorème de transfert)}\\
	&=\int_{\R^*_-}xdP_X(x) +\int_{\R_+}xdP_X(x)\\
	&= \int_{\R_+}xdP_X(x)\text{ car $P_X(\R^*_-) = \P(X<0)=0$}\\
	&=\int_{\R_+}(\int_0^xdt)dP_X(x)\\
	&=\int_0^\infty(\int_0^\infty \1_{t\le x}dt)dP_X(x)\\
	&=\int_0^\infty(\int_0^\infty  \1_{t\le x}dP_X(x))dt \text{ (Fubini Toneli)}\\
	&=\int_0^\infty P(X \ge t)dt
\end{align*}
}


\Prop

Soit $X$ une variable aléatoire discrète , alors 

$$\esp{X} = \somme{k=1}{\infty}P(X \ge k)$$

\corr{\textbf{preuve :}

\begin{align*}
	\esp{X} &= \somme{k=1}{\infty}k P(X=k)\\
	&= \somme{k=1}{\infty}\somme{j=1}{k}P(X = k) \text{ car $k = \somme{j=1}{k}1$}\\
	&= \somme{j=1}{\infty}\somme{k=j}{\infty}P(X=k)\text{ car $\forall n \in \overline{\N^*}, \somme{k=1}{n}\somme{j=1}{k}u_k =\somme{j=1}{n}\somme{k=j}{n}u_k$}\\
	&=\somme{j=1}{\infty} P(X \ge j)
\end{align*}
}

\Prop
\textbf{Fonction test}\petitespace

Soit $X$ une variable aléatoire à valeurs dans un espace mesurable $\EE$ et soit $\d$ une mesure de référence sur $\EE$.

Alors $X$ admet une densité $f$ par rapport à $\d$ si, et seulement si pour toute fonction $g : E \mapsto \R$ mesurable et tel que $g \in L^1(P_X)$, 

$$\esp{g(X)} = \int_E g(x)f(x)d\d(x) $$ \petitespace







\subsection{Moments, Variance, Covariance}\petitespace

\Def 

Soit $X$ une variable aléatoire réelle sur $\OmegaAP$ et $n \ge 1$, alors si $X \in  L^n( P),$ on appelle  $n^{\text{ième}}$ moment de $X$ le réel $\esp{X^n}$.
\petitespace

\Def

Soit $p \ge 1$ et $X$ un vecteur aléatoire, on dit que $X\in  L^p(P)$ si $\esp{\norme{X}^p}<+\infty$

\petitespace

\Def

Soit $X$ un vecteur aléatoire sur un espace probablisé $\OmegaAP$ dans $(\R^d, \mathcal B(\R^d) )$, on dit que $X \in L^{\infty}(P)$ si 

$$\exists M>0, \quad ||X|| \overset{p.s.}{\le} M \iff P(\omega \in \Omega, |X(\omega)|>M) = 0 $$

Par l'équivalence des norme en dimension finie, ça ne dépend pas de la norme.

\petitespace


\Prop \textbf{Cauchy-Schwarz pour les espérances}\petitespace

Soient $X$ et $Y$ deux variables aléatoires réelles admettant un moment d'ordre deux. Alors :\petitespace

\shift  i) $XY \in L^1( P)$ \petitespace 
 
\shift ii ) $\esp{\lvert XY\rvert} \le \sqrt{\esp{X^2}\esp{Y^2}}$\petitespace

\Def \textbf{Variance}\petitespace


Si $X \in L^2( P)$, on appelle variance de $X$ le réel $$\Var{X}=\esp{X^2}-(\esp{X})^2=\esp{(X-\esp{X})^2}$$

Cette variable minimise la fonction $$\fonct{f}{\R}{\R}{\esp{(X-x)^2}}$$


\Def \textbf{Covariance}\petitespace

Soit $X,Y$ deux variables aléatoires réelles dans $L^2(P)$. On définit la covariance de X et Y par $$\cov(X,Y)= \esp{XY}-\esp{X}\esp{Y}$$

On a 
\begin{align*}
	&i)\hspace{2em} \cov(X,Y)=\esp{(X-\esp{X})(Y-\esp{Y})}\\
	&ii)\hspace{2em} \lvert \cov(X,Y)\rvert \le \sqrt{\Var{X}\Var{Y}}\\
	&iii)\hspace{2em} \cov(X,Y)=\cov(Y,X)\\
	&iv)\hspace{2em} \cov(X,X)=\Var{X}
\end{align*}

\petitespace


\Prop


Soit $X$ un vecteur aléatoire dans $L^q(P)$, et $p \le q, \quad (q,p \in \N)$, Alors 

$$X \in L^p(P)$$

\petitespace

\corr{\textbf{preuve :} 

On a $$\norme{X}^p \le  1\times \1_{\norme{X}^p < 1} + \norme{X}^q \1_{\norme{X}^p \ge 1}$$ 

Donc 

\begin{align*}
	\esp{\norme{X^p}} & \le \esp{\1_{\norme{X}^p < 1} + \norme{X}^q \1_{\norme{X}^p \ge 1}}\\
	&\le 1 +  \esp{ \norme{X}^q } < + \infty
\end{align*}

car $X \in L^q(P)$. Donc $X \in L^p(P)$.



\petitespace}


\newpage

\section{Notions de Convergence} %cv ps,en probas, en loi, leurs propriétés... 
\petitespace

\subsection{Tout d'abord, les Égalités}\petitespace


Pour toute cette sous partie, on prendra $X$ et $Y$ des variables aléatoires de $\OmegaAP$ dans $\EE$\\


\Def

On dit que $X$ égal  presque sûrement $Y$ \ie

 $$X\overset{p.s}{=}Y$$

si $$P( \omega \in \Omega\mid X(\omega)=Y(\omega))=1$$

\Def

On dit que $X$ est égal en distribution (en loi) à $Y$ \ie

 $$X\overset{(d)}{=}Y$$ 
 
 
 si $\forall A \in \mathcal{E},$ 
 
 $$P(X^{-1}(A))=P(X\in A)=\P(Y \in A)= P(Y^{-1}(A) )$$\petitespace

\Rq 

Si $X\overset{(d)}{=}Y$, $X$ et $Y$ ne sont pas nécessairement définit sur le même espace probabilisé, on peut avoir $X$ dans $\OmegaAP$ et $Y$ dans $(\Omega', \mathcal A',  P)$. En revanche, ils sont forcément vers les même espace $\EE$\petitespace

On a également que $$X\overset{p.s}{=}Y \implies X\overset{(d)}{=}Y$$. 

\petitespace

\subsection{Convergence de Variable et Vecteur Aléatoire}\petitespace

Soit $(X_n)_{n \ge 1}$ une suite de variables aléatoires et $X$ une variable aléatoire, sans mention contraire, elles seront toutes d'un espace probabilisé $\OmegaAP$ dans $\EE$. Pour rappel, si on écrit $(\bf X_n)_{n \ge 1}$ et $\bf X$ alors ce sont des vecteurs aléatoires de dimension $d \ge 1$, de $\OmegaAP$ dans $\EE$. \petitespace

\subsubsection{Convergence Presque Sure}\petitespace

\Def

On dit que $X_n \cvps X$ \ie "$X_n$ \textbf{converge presque sûrement} vers $X$" si 

$$P(\{ \omega \in \Omega \mid X_n(\omega) \cvn X(\omega) \})=1 $$ \petitespace

\Prop

Soit $ (\bf X_n)_{n \ge 1}$ et $\bf X$ des vecteur aléatoires de dimension $d \ge 1$, alors 

$$\bf X_n \cvps \bf X \iff \forall i \in \ldbrack 1,d \rdbrack,\quad \bf X_n^{(i)} \cvps \bf X^{(i)}  $$

C'est d'ailleurs équivalent à dire que si $X_n \cvps X$ et $Y_n \cvps Y$, alors 

$$(X_n,Y_n) \cvps (X,Y)$$

\corr{\textbf{Preuve :}\petitespace





\petitespace}



\Prop

Soient $X_n$ et $X$ deux variables aléatoires réelles, on a 

$$\Sup{k \ge n}\lvert X_k-X\rvert \cvP 0 \implies X_n \cvps X$$\petitespace

\corr{\textbf{Preuve :}\petitespace


Soit $V_n = \Sup{k \ge n}\lvert X_k-X\rvert$\petitespace

$\forall n \in \N^*, V_n$ est positive (donc minorée) et décroissante, donc $(V_n)_{n \ge 1}$ est une suite décroissante minorée donc converge presque sûrement.\petitespace

Ainsi, comme $V_n \cvP 0$ alors $V_n \cvps 0$ (on verra par la suite que convergence presque sûre implique convergence en probabilité).

Donc $\exists \Omega_0 \mid P(\Omega_0) =1$ et $\forall \omega \in \Omega_0, \quad X_n(\omega)$ converge uniformément vers $X(\omega)$ \ie : $X_n \cvu X$\petitespace

 Or, convergence uniforme implique convergence simple, donc $\forall \omega \in \Omega_0, \quad X_n(\omega) \cvn X(\omega)$, ainsi on a bien $X_n \cvps X$

\petitespace}


\Prop 

Soit $(\bf X_n)_{n \ge 1}$ un vecteur aléatoire réel de taille $d \ge 1$, dans $\R^p$ et $\bf X$ un vecteur aléatoire réel de taille $d \ge 1$, dans $\R^p$\\

Si $\bf X_n \cvps \bf X$, et $$\fonct{h}{\R^d}{\R^p}{h(x)}, p \ge 1$$ une fonction continue, alors 

$$h(\bf X_n) \cvps h(\bf X)$$\petitespace

\corr{\textbf{Preuve :}\petitespace

On a $\bf X_n \cvps \bf X$ \ie : $P(\{\omega \in \Omega \mid \bf X_n(\omega) \cvn\bf X(\omega)\})$. Donc $\exists \Omega_0\subset \Omega, \quad P(\Omega_0)=1$ et $\forall \omega \in \Omega_0, \quad \bf X_n(\omega) \cvn \bf X(\omega)$ Or, comme $h$ est $ \cal C^0, \forall \omega \in \Omega_0,\quad h(\bf X_n(\omega))\cvn h(\bf X(\omega))$ D'où


 $$h(\bf X_n) \cvps h(\bf X)$$

\petitespace}





\petitespace

\subsubsection{Convergence dans $L^p$}\petitespace

\Def

Soit $(\bf X_n)_{n \ge 1}$ une suite de vecteurs aléatoires dans $\R^d, d \ge 1$. Si, pour $p \ge 1$, on a pour tout $n \ge 1,$
$$ \bf X_n \in L^p( P) \iff \esp{\lVert\bf X_n \rVert^p}<+\infty$$
 
 On dit que $\bf X_n \cvLp{p} \bf X$ si

$$\esp{\lVert \bf X_n-\bf X\rVert^p} \cvn 0$$(Ne dépends pas de la norme)\\

\thm 
\textbf{Théorème de convergence dominée, version probabiliste}\\

Soit $(\bf X_n)_{n \ge 1}$ une suite de vecteurs aléatoires réels dans $\R^d$, et $\bf X$ un vecteur aléatoire dans $\R^d$. 

Supposons que :\\

\shift 1) $\bf X_n \cvps \bf X$\\

\shift 2)$\bf X \in  L^p( P),\quad p\ge 1$\\

\shift3) Il existe une variable aléatoire réelle $\bf Y\in  L^p( P)$ tel que $\forall n \ge 1$

$$ \lVert \bf X_n \rVert \le \bf Y \text{  p.s.}$$\\

Alors $$\bf X_n \cvLp{p} \bf X$$\petitespace

\corr{ \textbf{preuve : }\petitespace

Par (3), on a que $\bf X_n \in L^p( P), \quad  \forall n \ge 1$. Pour montrer que $\bf X_n \cvLp{p}\bf  X$, \ie 


$$\esp{\lVert \bf X_n -\bf X\rVert^p} \cvn 0 \iff \int_{\Omega}\lVert\bf X_n(\omega)-\bf X(\omega) \rVert^p dP(\omega)$$

On sait que $\bf X_n(\omega) \cvn \bf X(\omega)$ $P$- presque partout donc 

$$\rVert\bf X_n(\omega)-\bf X(\omega)\rVert^p \cvn 0$$ 

également, on a 

\begin{align*}
	\lVert \bf X_n -\bf X\rVert^p&\le (\lVert \bf X_n\rVert + \lVert \bf X\rVert)^p\\
	&\le 2^p(\rVert \bf X_n\rVert^p+\lVert \bf X\rVert^p)\\
	&\le 2^p(\lVert \bf Y \rVert^p+\lVert \bf X\rVert^p) \in  L^1( P) \text{   car $\bf X,\bf Y \in  L^p( P)$}
\end{align*}

Donc $\lVert \bf X_n-\bf X\rVert$ est dominée par une fonction dans $ L^p( P)$, ainsi on a 

\begin{align*}
	\lim\limits_{n \to +\infty}\esp{\lVert \bf X_n-\bf X\rVert^p}&=\lim\limits_{n \to +\infty}\int_\Omega\lVert \bf X_n(\omega)-\bf X(\omega)\rVert^pd P(\omega)\\
	&=\int_\Omega \lim\limits_{n \to +\infty}\lVert \bf X_n(\omega)-\bf X(\omega)\rVert^pd P(\omega)\text{(TCD)}\\
	&= 0
\end{align*}

Donc $\bf X_n \cvLp{p}\bf X$
\petitespace}



\Prop 

Si $\bf X_n \cvLp{p}\bf X$ et $\bf Y_n \cvLp{p} \bf Y$ (tous des vecteurs aléatoires dans $\R^d, d \ge 1$), alors $$ \bf X_n + \bf Y_n \cvLp{p}\bf{X+Y}$$\petitespace


\Prop

Pour $X$ est une variable aléatoire réelle et pour $p,q >1 \mid \frac{1}{p}+\frac{1}{q}=1$. Si $X_n \cvLp{p} X$ et $Y_n \cvLp{q}Y$ alors :


$$X_nY_n \cvLp{1}XY$$\petitespace


\subsubsection{Convergence en Probabilité}\petitespace


\Def

Soit $(\bf X_n)_{n \ge 1} $ et $\bf X$ des vecteurs aléatoires de dimension $d\ge 1$, on dit que $\bf X_n \cvP X$ ($\bf X_n$ \textbf{converge en probabilité} vers $\bf X$) si 

$$\forall \epsilon >0, \quad P(\lVert \bf X_n-\bf X\rVert>\epsilon) \cvn 0$$

\Prop 


Soit $(\bf X_n)_{n \ge 1} $ et $\bf X$ des vecteurs aléatoires dans $\R^d, d \ge 1$. Si $\bf X_n \cvP \bf X$, et $$\fonct{h}{\R^d}{\R^p}{h(x)}, p \ge 1$$ continue, alors 

$$h(\bf X_n) \cvP h(\bf X)$$\petitespace

\corr{\textbf{preuve :}\\


On a $h \in \cal C^0(\R^d)$ et $\bf X_n\cvP \bf X$. Soit $\alpha>0, M>0$ tel que $\P(\lVert \bf X\rVert >M) \le \alpha$

On a que $P(\lVert\bf X_n-\bf X\rVert > 1)\cvn 0$ car $\bf X_n \cvP \bf X$\petitespace

Soit $K = B(0, M+1)$, d'après le théorème de Heine, comme $h$ continue, $h$ est absolument continue sur le compact $K$ \ie:
$\forall \epsilon >0, \exists \eta>0 \mid \forall x,y\in K, \lVert x-y\rVert \le \eta \implies \lVert h(x)-h(y)\rVert \le \epsilon$


Soit $\epsilon >0,$

\begin{align*}
	P(\lVert h(\bf X_n)-h(\bf X)\rVert >\epsilon) &=P(\{\lVert h(\bf X_n)-h(\bf X)\rVert >\epsilon\}\cap(\{ \lVert \bf X\rVert \le M\}\cap \{ \lVert \bf X_n-\bf X\rVert \le 1\})+\\
	&P(\{\lVert h(\bf X_n)-h(\bf X)\rVert >\epsilon\}\cap(\{ \lVert\bf X\rVert > M\}\cup \{ \lVert\bf X_n-\bf X\rVert > 1\})\\
	&\le P(\{\lVert h(\bf X_n)-h(\bf X)\rVert >\epsilon\}\cap \{\bf X,\bf X_n \in K\}) + P(\{\lVert\bf X\rVert >M\}\cup \{\lVert\bf X_n-\bf X\rVert >1\})\\
	&\le P(\lVert \bf X_n-X\rVert >\eta)+P(\lVert\bf X\rVert >M \cup \lVert \bf X_n-\bf X\rVert > 1)\\
	&\le \alpha + P(\lVert\bf X\rVert >M)+P(\lVert \bf X_n-\bf X\rVert > 1) \text{   (Pour n assez grand)}\\
	&\le 3\alpha
\end{align*}

Donc, $\forall \epsilon >0, P(\lVert h(\bf X_n)-h(\bf X)\rVert > \epsilon)\cvn 0$

Ainsi
$$h(\bf X_n) \cvP h(\bf X)$$

\petitespace}

\Prop 

Soit $(\bf X_n)_{n \ge 1}, \bf X$ et $(\bf Y_n)_{n \ge 1}, \bf Y$ des vecteurs aléatoires dans $\R^d$ et $\R^n$, $d,n \ge 1$. Si $\bf X_n \cvP \bf X$ et $\bf Y_n \cvP \bf Y$, alors 

$$ (\bf X_n,\bf Y_n) \cvP (\bf X,\bf Y)$$

\petitespace

\Prop

Soient $\bf X_n$ et $\bf Y_n$ deux vecteurs aléatoires. Si $\bf X_n \cvP \bf X$ et $\bf Y_n-\bf X_n \cvP 0$ alors :

$$\bf Y_n \cvP\bf  X $$ 

\corr{\textbf{Preuve :}

Soit $\epsilon>0$, alors :

\begin{align*}
	P(\norme{\bf Y_n-\bf X}> \epsilon) &=P(\norme{\bf Y_n-\bf X_n+\bf X_n-\bf X}> \epsilon) \\
	\text{(I.T.)}&\le P(\norme{\bf Y_n-\bf X_n}+\norme{\bf X_n-\bf X}> \epsilon) \\
	&\le P(\norme{\bf Y_n-\bf X_n}> \frac \epsilon 2 \cup \norme{\bf X_n-\bf X}>\frac \epsilon 2)\\
	& \le \underbrace{P(\norme{\bf Y_n-\bf X_n}> \frac \epsilon 2)}_{\cvn 0} + \underbrace{P(\norme{\bf X_n-\bf X}> \frac \epsilon 2)}_{\cvn 0} \cvn 0
\end{align*}





\petitespace}

\Prop

Si $\bf X_n \cvps \bf X$ ou bien $\bf X_n \cvLp{p} \bf X$, $p \ge 1$, Alors $$\bf X_n \cvP \bf X$$\petitespace

\corr{\textbf{Preuve :}\petitespace

Supposons que $\bf X_n \cvps \bf X$, alors il existe $\Omega_0$ tel que $P(\Omega_0)=1$ et pour tout $\omega \in \Omega_0$ on a que pour tout $\epsilon >0$, il existe $N_\omega \in \N, \quad \forall n \ge N_\omega, \quad \norme{\bf X_n(\omega)-\bf X(\omega)} \le \epsilon$

Donc prenons $\epsilon >0$,

\begin{align*}
	\Lim{n \to \infty}P(\norme{\bf X_n-\bf X}\ge \epsilon)&=\Lim{n \to \infty}P(\omega \in \Omega \mid (\norme{\bf X_n(\omega)-\bf X(\omega)}\ge \epsilon)\\
	&= P(\Omega_0) =0
\end{align*}

Donc $\bf X_n \cvP \bf X$

Pour la convergence dans $L^1$, supposons que $\bf X_n \cvLp{1}\bf X$, alors soit $\epsilon>0$

\begin{align*}
	P(\norme{\bf X_n-\bf X}\ge \epsilon)& \overbrace{\le}^{\text{(Markov)}} \frac{\esp{\norme{\bf X_n-\bf X}}}{\epsilon} \cvn 0
\end{align*}

Donc $\bf X_n \cvP\bf X$.


\petitespace}

\subsubsection{Convergence en Loi}\petitespace

\Def

Soit $(\bf X_n)_{n \ge 1}$ une suite de vecteurs aléatoires dans $\EE$ de taille $d \ge 1$ et $\bf X$ vecteur aléatoire dans $\FF$.

On dit que $\bf X_n \cvl \bf X$ ($\bf X_n$ \textbf{converge en loi} vers $\bf X$) si pour toute fonction $f$ \textbf{continue bornée}, $f : \R^d \mapsto \R ,$ on a :\\

$$\esp{f(\bf X_n)} \cvn \esp{f(\bf X)}$$\petitespace

Cette définition est \textbf{équivalente à}, si $X_n$ et $X$ sont dans $\R$ à dire que pour tout $t \in \R^{\setminus A}$ où $A$ représente l'ensemble des atomes de $X$

$$ F_{X_n}(t) \cvn F_X(t)$$\petitespace

\textbf{Également équivalente à} (\textbf{Théorème de Levy}) pour $(\bf X_n)_{n \ge 1}$ une suite de vecteurs aléatoires dans $\EE$ de taille $d \ge 1$ et $\bf X$ vecteur aléatoire dans $\FF$. 

$$\forall t \in \R^d, \quad \varphi_{\bf X_n}(t) \cvn \varphi_{\bf X}(t)$$

Avec 

$$ \fonct{\varphi}{\R^d}{\bb{C}}{\esp{e^{ix^TX}}}$$

(voir section suivante pour plus de précision sur la fonction caractéristique).

\petitespace
\Prop 

Soit $\bf{\Xunan}$ et $\bf X$ des vecteurs aléatoires de dimension $d \ge 1$. Supposons que $\bf X_n\cvl\bf X$. Alors pour tout continue, $$\fonct{h}{\R^d}{\R^p}{h(x)}$$ 

$p\ge 1$ On a alors $$h(\bf X_n) \cvl h(\bf X)$$\petitespace

\Prop 

Si $\bf X_n \cvl \bf X$ et $\bf X_n \cvl \bf Y$ alors $\bf X$ et $\bf Y$ ont la même loi. \petitespace


\Prop

Soit $\bf{\Xunan}$ et $\bf X$ des vecteurs aléatoires de dimension $d \ge 1$. Si $\bf X_n \cvP \bf X$, Alors $\bf X_n \cvl\bf X$\petitespace

\corr{\textbf{preuve :}

Supposons que $\bf X_n \cvP \bf X$, alors $$\forall \epsilon >0, \P(\lVert\bf X_n-\bf X\rVert > \epsilon) \cvn 0$$

Soit $t \in \R$,

\begin{align*}
	\lvert \varphi_{\bf X_n}(t)-\varphi_{\bf X}(t)\rvert&=\lvert \esp{e^{it^T\bf X_n}-e^{it^T\bf X}}\rvert \\
	&=\lvert \esp{e^{it^T\bf X_n}(1-e^{it^T\bf X-it^T\bf X_n})}\rvert\\
	&\le \esp{\lvert e^{it^T\bf X_n}\rvert\lvert 1-e^{it^T\bf X-it^T\bf X_n}\rvert} \text{ (I.T.)} \\
	&\le \esp{\lvert 1-e^{it^T(\bf X-\bf X_n)}\rvert}   \\
	&\le  \esp{\lvert 1-e^{it^T(\bf X-\bf X_n)}\rvert\1_{\lVert\bf X_n -\bf X\rVert \le \epsilon}} + \esp{\lvert 1-e^{it^T(\bf X-\bf X_n)}\rvert\1_{\lVert \bf X_n -\bf X\rVert > \epsilon}}
\end{align*}

Or pour le deuxième terme, on a 

\begin{align*}
	 \esp{\lvert 1-e^{it^T(\bf X-\bf X_n)}\rvert\1_{\lVert \bf X_n -\bf X\rVert > \epsilon}} &\le 2\esp{\1_{\lVert\bf X_n -\bf X\rVert > \epsilon}} \text{  (I.T)}\\
	 &\le 2\P(\lVert \bf X_n -\bf X\rVert > \epsilon)\cvn 0 \\
\end{align*}

Et pour le premier terme, on a 

\begin{align*}
	 \esp{\lvert 1-e^{it^T(\bf X-\bf X_n)}\rvert\1_{\lVert \bf X_n -\bf X\rVert \le \epsilon}}&\le  \esp{\lvert 1-(1+it^T(\bf X-\bf X_n)\rvert\1_{\lVert \bf X_n -\bf X\rVert \le \epsilon}}\\
	 &\le\esp{\lVert t\rVert \epsilon} \\
	 &\le \epsilon \esp{\lVert t \rVert} \cvn 0
\end{align*}

Donc on a bien que 

\begin{align*}
	\lvert \varphi_{\bf X_n}(t)-\varphi_{\bf X}(t)\rvert \cvn 0
\end{align*}

Ainsi,  $$\bf X_n \cvl \bf X$$

\petitespace}

\Prop 

Soit $(\bf X_n)_{n \ge 1}$ une suite de variables vecteurs réelles dans $\R^d, d \ge 1$, et $\bf c\in \R^d$ Alors 

$$\bf X_n \cvP \bf c \iff \bf X_n \cvl \bf c \iff  \bf X_n \cvl \delta_{\bf c}$$

\petitespace

\corr{ \textbf{preuve :}

Prouvons-le en dimension 1. $X_n \cvP c \implies X_n \cvl c$ est toujours vrai. 

Montrons que $X_n \cvl c \implies X_n \cvP c$.

Soit $\epsilon >0$, alors 

\begin{align*}
	\P(\lvert X_n-c\rvert > \epsilon)&= \P(\{X_n-c > \epsilon\} \cup \{ X_n -c < -\epsilon \} )\text{  car ces deux événements sont disjoints}\\
	&= \P(X_n-c > \epsilon) + \P(X_n-c<-\epsilon)\\
	&=1-\P(X_n \le c+\epsilon) + \P(X_n<c-\epsilon)\\
	&\cvn 1-P(c \le c+\epsilon)+ \P(c<c+\epsilon)\\
	&\cvn 1-1+0\\
	&=0
\end{align*}

Donc on a bien que 

$$\forall \epsilon >0, \P(\lvert X_n-c\rvert > \epsilon) \cvn 0 \iff  X_n \cvP c$$

Donc $$X_n \cvl c \iff X_n \cvP c$$
\petitespace}

\thm
\textbf{Théorème de Scheffé : } \\ 

Soit $(\bf X_n)_{n \ge 1}$ une suite de vecteurs aléatoires réels de taille $d \ge 1$ et $\bf X$ un vecteur aléatoire réel de taille $d$.

Soit $\mu$ une mesure sur  $(\R^d; \BRd)$ et supposons que $\bf X$ et tout les $\bf X_n$ admettent une densité par rapport à $\mu$. 

On note $f$ la densité de $\bf X$ par rapport à $\mu$ et $f_n$ celle des $\bf X_n$ par rapport à $\mu$, pour tout $n \ge 1$.\\

Si pour $\mu$ presque tout $x \in \R^d,\quad f_n(x) \cvn f(x)$ alors 

$$\bf X_n \cvl \bf X$$

\corr{\textbf{preuve : }

Soit $g$ une fonction continue bornée, on a $$\esp{g(X_n)}= \int_{\R^d}g(x)f_n(x) d\mu(x)$$ et $$ \esp{g(X)} = \int_{\R^d}g(x)f(x) d\mu(x)$$

On a 

\begin{align*}
	&\lvert \esp{g(X_n)}- \esp{g(X)}\rvert\\
	&=\lvert \int_{\R^d}g(x)f_n(x) d\mu(x)- \int_{\R^d}g(x)f(x) d\mu(x)\rvert \\
	&=\lvert \int_{\R^d}g(x)(f_n(x)-f(x)) d\mu(x)\rvert\\
	&\le  \int_{\R^d}\lvert g(x)\rvert\lvert f_n(x)-f(x)\rvert d\mu(x)\\
	&\le M \int_{\R^d}\lvert f_n(x)-f(x)\rvert d\mu(x) \text{avec $M= \sup\limits_{x \in \R^d}\lvert g(x)\rvert$}\\
	&\le M \int_{\R^d}[2(f(x)-f_n(x))^+-(f(x)-f_n(x))] d\mu(x) \\
	&\text{car $\lvert u \rvert = 2u^+-u$ avec $u^+=\sup(0,u)$ avec ici $u=f(x)-f_n(x)$}\\
	&\le M( \int_{\R^d}2(f(x)-f_n(x))^+d\mu(x) - \int_{\R^d}(f(x)-f_n(x)) d\mu(x))\\
	&\le 2M (\int_{\R^d}(f(x)-f_n(x))^+d\mu(x) -(1-1))\\
	&\le 2M \int_{\R^d}(f(x)-f_n(x))^+d\mu(x)
\end{align*}

or on a :

$\bullet f_n(x) \cvn f(x)$\petitespace

$\bullet (f(x)-f_n(x))^+ \le f(x)$\petitespace

$\bullet f \in L^1(\mu)$\petitespace

donc par Théorème de convergence dominée, on a 

\begin{align*}
		&\lvert \esp{g(X_n)}- \esp{g(X)}\rvert\\
		&\le 2M \int_{\R^d}(f(x)-f_n(x))^+d\mu(x) \\
		&\cvn 0\\
		&\implies \lim\limits_{n \to \infty}\lvert \esp{g(X_n)}- \esp{g(X)}\rvert=0
\end{align*}

Donc $X_n \cvl X$
}

\petitespace

\textbf{corollaire : }\\

Soit $E\subset \R^d, d \ge 1$ au plus dénombrable, $\bf X \in E$ presque surement et $\forall n \ge 1$, $\bf X_n \in E$ presque surement, alors si $\forall \bf x \in E, \quad P(\bf X_n=\bf x) \cvn P(\bf X=\bf x)$ on a donc $$ \bf X_n \cvl \bf X$$

En effet, il suffit juste de remarqué que $f_n(x) = P(\bf X_n =\bf x)$ est une densité de $\bf X_n$ par rapport à la mesure de comptage, et $f(\bf x) = P(\bf X=\bf x)$ celle de $\bf X$ pour la même mesure. On sait que $\bf X$ admet une densité par rapport à cette mesure car $\bf X \overset{p.s.}{\in} E$ dénombrable.

\petitespace

\Prop 

Si $\bf X_n \indep \bf Y_n$, deux vecteurs aléatoires dans $\R^d$ et $\R^p$, $d,p \ge 1$ et $\bf X_n \cvl \bf X$, $\bf Y_n \cvl \bf Y$, alors on a 

$$(\bf X_n,\bf Y_n)\cvl (\bf X,\bf Y)$$

\petitespace

\corr{\textbf{preuve : }\petitespace

Soit $\bf t = \begin{pmatrix} \bf t_1 \\ \bf t_2 \end{pmatrix} \in \R^{d+p}$, $\bf t_1 \in \R^d, \bf t_2 \in \R^p$, on a :


\begin{align*}
	\varphi_{(\bf X_n,\bf Y_n)}(\bf t) &= \esp{e^{i\bf t_1\bf X_n +i\bf t_2\bf Y_n}} \\
	&=\esp{e^{i\bf t_1\bf X_n}}\esp{e^{i\bf t_2\bf Y_n}} \quad \text{ (par indépendance)}\\
	&= \varphi_{\bf X_n}(\bf t_1)  \varphi_{\bf Y_n}(\bf t_2) \cvn \varphi_{\bf X}(\bf t_1)  \varphi_{\bf Y}(\bf t_2)  \quad \text{ (car $|\varphi | \le 1 $ )}\\
	\\& \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad = \varphi_{(\bf X,\bf Y)}(\bf t)
\end{align*}

Même si $\bf X$ et $\bf Y$ ne sont pas indépendants, on pourra toujours construire $\bf X' \overset{(d)}{=} \bf X$ et $\bf Y' \overset{(d)}{=} \bf Y$ tel que $\bf X' \indep\bf Y'$, de sorte que l'on ait une convergence vers $ \varphi_{(\bf X',\bf Y')}(\bf t)= \varphi_{(\bf X,\bf Y)}(\bf t)$.

Pour prouver l'existence de telles variables aléatoires, on va les construire. soient $\bf X', \bf Y'$ deux variables aléatoires sur l'espace probabilisé $(\Omega', \cal A', P') = (\Omega\times\Omega, \cal A \otimes \cal A, P\times P)$. On note aussi pour tout $\omega_1, \omega_2 \in \Omega',$ $\bf X'(\omega_1, \omega_2) = \bf X(\omega_1)$ et $\bf Y'(\omega_1, \omega_2) = \bf Y(\omega_2)$. on a ainsi clairement $\bf X' \overset{(d)}{=} \bf X$ car $P_{\bf X'} = P_{\bf X}$ et $\bf Y' \overset{(d)}{=} \bf Y$. De plus, on a $\bf X' \indep \bf Y'$ car dépendent de coordonnées différentes de l'espace produit $\Omega'$, ce qui conclut notre preuve.

\petitespace}




\Rq

\shift i)  Si $\bf X_n \cvl \bf X$, $\bf X_n$ et $\bf X$ ne sont pas nécessairement définit sur le même espace probabilisé, on peut avoir $\bf X_n$ dans $\OmegaAP$ et $\bf X$ dans $(\Omega', \mathcal A', P)$. En revanche, ils sont forcément vers les même espace $\EE$.

\petitespace

\shift ii) Si $\bf X_n \cvps \bf X$ ou $\bf X_n \cvP \bf X$ ou $\bf X_n \cvLp{p} \bf X$ (n,p $\ge$ 1) alors $\bf X_n$ et $\bf X$ sont forcément définit 
sur le même espace probabilisé $\OmegaAP$ (et sont à valeur dans le même espace). \petitespace

\shift iii) Pour la convergence en probabilité et dans $L^p$, on ne s'intéresse pas à la norme. En effet, par l'équivalence des normes en dimension finie, cela n'a pas d'importance, on peut donc prendre n'importe quelle norme. 

\newpage

\section{Théorèmes de Convergences dans le Cas iid}\petitespace

\thm 


\cercler 1Soit $\Xunan \sim \normale{\mu}{\sigma^2}$ alors 

$$\overline{X}_n \sim \normale{\mu}{\frac{\sigma^2}{n}}$$


\cercler 2 On note $\widehat{\sigma}^2_n=\frac{1}{n}\somme{i=1}{n}(X_i-\overline{X}_n)^2$ la variance empirique, alors :

$$n\frac{\widehat{\sigma}^2_n}{\sigma} \sim \chideux{n-1}$$

et 

$$ \widehat{\sigma}^2_n\indep \overline{X}_n$$\petitespace

\corr{\textbf{Preuve :}

\cercler 1 Soit $t \in \R$, on a :

\begin{align*}
	\varphi_{\overline{X}_n}(t) &=\esp{e^{i\frac tn \somme{i=1}{n}X_i}}\\
	&= (\varphi_{X_1}(\frac tn))^n\\
	&=(e^{i\frac tn \mu-\frac{1}{2n^2}t^2\sigma^2})^n\\
	&=e^{it\mu-\frac{1}{2n}\sigma^2t^2}\\
	&=\varphi_\psi(t) \quad \quad \text{Pour $\psi \sim \normale{\mu}{\frac{\sigma^2}{n}}$}
\end{align*}

\petitespace

\cercler 2 Notons $\bf{X} = \begin{pmatrix}X_1\\\vdots\\X_n\end{pmatrix}$, $\bf{Z} = \begin{pmatrix}Z_1\\\vdots\\Z_n\end{pmatrix}$, avec $Z_i = \frac{X_i-\mu}{\sigma} \sim \normale{0}{1}$ et donc $\bf{Z} \sim \cal N_n(0, \bf I_n)$ avec $\bf Z$ un vecteur \petitespace 

gaussien, puisque les $Z_i$ sont iid

\begin{align*}
	n\frac{\widehat{\sigma}^2_n}{\sigma^2}&=\somme{i=1}{n}(\frac{X_i-\overline{X}_n}{\sigma})^2\\
	&=\somme{i=1}{n}(Z_i-\overline{Z_n})^2\\
	&=\lVert \bf Z-\frac 1n\1_n^T\bf Z\1_n\rVert^2\\
	&=\lVert (\bf I_n-\frac 1n \1_n\1_n^T)\bf Z\rVert^2\text{   Notons $\Pi_1 = \bf I_n-\frac 1n \1_n\1_n^T$ et $\Pi_2 = 1-\Pi_1$}\\ 
	&=\lVert \Pi_1\bf Z\rVert^2\sim \chideux{rg(\Pi_1)} \text{   Par Cochran (voir chapitre sur les vecteurs gaussiens)} \\
\end{align*}


Or $rg(\Pi_1) = n-1$ donc on a bien $ n\frac{\widehat{\sigma}^2_n}{\sigma} \sim \chideux{n-1}$. Pour montrer que $\overline{X_n} \indep \widehat{\sigma_n}^2$, on sait que


$$ \begin{pmatrix} \Pi_1\bf Z \\ (\bf I_n-\Pi_1)\bf Z\end{pmatrix} =  \begin{pmatrix} \Pi_1 \\ (\bf I_n-\Pi_1)\end{pmatrix}\bf Z$$
est un vecteur gaussien, 

\begin{align*}
	Cov(\Pi_1\bf Z,(1-\Pi_1)\bf Z)&=\Pi_1\esp{\bf Z\bf Z^T}(\bf I-\Pi_1)\text{ (car $(\bf I_n-\Pi_1)^T=\bf I_n-\Pi_1$)}\\
	&=\Pi_1\bf I_n(\bf I_n-\Pi_1)\text{ ($\esp{\bf Z \bf Z^T} = \Var{Z}=\bf I_n$)}\\
	&=\bf I_n
\end{align*}

Ainsi , comme c'est un vecteur gaussien, (voir section sur les vecteurs gaussien), on a $\Pi_1\bf Z\indep(1-\Pi_1)\bf Z$. On a :

\begin{align*}
	&\Pi_1\bf Z\indep(1-\Pi_1)\bf Z\\
	\iff&\widehat{\sigma_n}^2\indep \frac1n \1_n\1_n^T\bf Z\\
	\iff&\widehat{\sigma_n}^2\indep \frac1n\1_n^T\bf Z = \overline{X_n}
\end{align*}
\petitespace}

\subsection{Convergences en Loi} \petitespace

\thm
\textbf{Théorème Central limite}\petitespace

Soit $\Xunan$ iid $\in  L^2( P)$ tel que $\esp{X_1}=\mu$ et $\bb{V}(X_1)=\sigma^2$, alors 

$$\sqrt{n}(\overline{X}_n-\mu)\cvl \normale{0}{\sigma^2}$$ 

\corr{\textbf{Preuve : } 

Soit $t \in \R$, on a :

\begin{align*}
	\varphi_{\sqrt n(\overline X_n - \mu)}&=\esp{e^{it\sqrt n(\overline X_n-\mu)}}\\
	&=\esp{e^{i\frac{t}{\sqrt n}\somme{i=1}{n}(X_i-\mu)}}\\
	&=\produit{i=1}{n}\esp{e^\frac{t}{\sqrt n}(X_1-\mu)}\\
	&=(\varphi_{X_1-\mu}(\frac{t}{\sqrt n}))^n\\
	&=(\varphi_{X_1-\mu}(0)+\frac{t}{\sqrt n}\varphi_{X_1-\mu}'(0)+\frac{t^2}{2n}\varphi_{X_1-\mu}''(0) +o(\frac{1}{n^2}))^n\\
\end{align*}



Or on sait que $\varphi_{X_1-\mu}(0)=1$, $\varphi_{X_1-\mu}'(0)=i\esp{X_1-\mu}=0$ et que $\varphi_{X_1-\mu}''(0)=i^2\esp{X_1^2}=-\sigma^2$ \\ (car $\esp{X_1}=0$). Donc :

\begin{align*}
	\varphi_{\sqrt n(\overline X_n - \mu)}&=(1-\frac{t^2\sigma^2}{2n} +o(\frac{1}{n^2}))^n\\
%	&\underset{\infty}{\sim}e^{-\frac{(t\sigma)^2}{2}}\\
%	&\underset{\infty}{\sim} \varphi_\psi(t)  \text{      pour $\psi \sim \normale{0}{\sigma^2}$}
\end{align*}

Pour éviter l'introduction d'un logarithme complexe, que, pour tout nombre complexes $u,z$ de module inférieur ou égal à 1 on peut observer que 

$$\forall n \in \N^*, \quad\quad |z^n-u^n| = |(z-u)\somme{k=0}{n-1} z^ku^{n-1-k}| \le n|z-u| $$ 



On sait que $e^{-\frac{\sigma^2 t^2}{2n}} = 1 - \frac{\sigma^2 t^2}{2n} + o(\frac 1n)$ et $\varphi_{X_1-\mu}(\frac{t}{\sqrt n}) = 1-\frac{t^2\sigma^2}{2n} +o(\frac{1}{n^2}) $ donc 

\begin{align*}
	| \varphi_{\sqrt n(\overline X_n - \mu)} - e^{-\frac{\sigma^2 t^2}{2}}| &= |(\varphi_{X_1-\mu}(\frac{t}{\sqrt n}))^n - (e^{-\frac{\sigma^2 t^2}{2n}})^n| \\
	&\le n|\varphi_{X_1-\mu}(\frac{t}{\sqrt n})- e^{-\frac{\sigma^2 t^2}{2n}} |\\
	& \le n|o(\frac 1n)| = o(1) \cvn 0
\end{align*}

Donc $ \varphi_{\sqrt n(\overline X_n - \mu)} \cvn e^{-\frac{\sigma^2 t^2}{2}} = \varphi_\psi(t)$ pour $\psi \sim \normale{0}{\sigma^2}$


\petitespace

Donc on a par théorème de Levy, que $$\sqrt{n}(\overline{X}_n-\mu)\cvl \normale{0}{\sigma^2}$$
\petitespace}

\thm 
\textbf{Théorème de Slutsky : }\\

Soit $\bf X_n$ et $\bf X$ des vecteurs aléatoires de taille $d \ge 1$ tel que $\bf X_n \cvl \bf X$, et $\bf Y_n$ et $\bf c \in \R^p$ un vecteur fixé, tous dans $\R^p, \quad p \ge 1$ tel que $\bf Y_n \cvl \bf c$. Alors 

\begin{align*} 
\begin{pmatrix} 
\bf X_n\\
\bf Y_n 
\end{pmatrix} \cvl  \begin{pmatrix} 
\bf X\\
\bf c
\end{pmatrix}
\end{align*}\petitespace

\corr{\textbf{Preuve :}

Soit $\bf s \in \R^p, \bf t \in \R^p$, montrons que $\varphi_{\bf X_n, \bf Y_n}(\bf s,\bf t) \cvn \varphi_{\bf X,\bf c}(\bf s,\bf t)$\\

\ie: $\esp{e^{i(\bf s^T\bf X_n+\bf t^T\bf Y_n}}\cvn \esp{e^{i(\bf s^\bf T\bf X+\bf t^T\bf c}}$

\begin{align*}
	|\esp{e^{i(\bf s^T\bf X_n+\bf t^T\bf Y_n}}-\esp{e^{i(\bf s^T\bf X+\bf t^T\bf c}}|&=|\esp{e^{i(\bf s^T\bf X_n+\bf t^T\bf Y_n)}} - \esp{e^{i(\bf s^T\bf X_n+\bf t^T\bf c)}} +\esp{e^{i(\bf s^T\bf X_n+\bf t^T\bf c)}} - \esp{e^{i(\bf s^T\bf X+\bf t^T\bf c)}}|\\
	&=|\esp{e^{i\bf s^T\bf X_n}(e^{i\bf t^T\bf Y_n}-e^{i\bf t^T\bf c})} +\esp{e^{i\bf t^T\bf c}(e^{i\bf s^T\bf X_n}-e^{i\bf s^T\bf X})}|\\
	\text{(IT)}&\le \esp{|e^{i\bf s^T\bf X_n}||e^{i\bf t^T\bf Y_n}-e^{i\bf t^T\bf c}|}+\esp{|e^{i\bf t^T\bf c}||e^{i\bf s^T\bf X_n}-e^{i\bf s^T\bf X}|}\\
	\text{car $|e^{i\bf s^T\bf X_n}|=1$}&\le\esp{|e^{i\bf t^T\bf Y_n}-e^{i\bf t^T\bf c}|}+\esp{|e^{i\bf s^T\bf X_n}-e^{i\bf s^T\bf X}|}\\
\end{align*}

Or, soit $\epsilon>0,$ on a :


\begin{align*}
	\esp{|e^{i\bf t^T\bf Y_n}-e^{i\bf t^T\bf c}|} &= \esp{|e^{i\bf t^T\bf Y_n}-e^{i\bf t^T\bf c}|\1_{\norme{\bf Y_n-\bf c}>\epsilon}}+\esp{|e^{i\bf t^T\bf Y_n}-e^{i\bf t^T\bf c}|\1_{\norme{\bf Y_n-\bf c}\le\epsilon}}\\
	&\le 2P(\norme{\bf Y_n-\bf c}>\epsilon)+\esp{|\bf t^T\bf Y_n-\bf t^T\bf c|\1_{\norme{\bf Y_n-\bf c}\le\epsilon}}\\
	&\le2P(\norme{\bf Y_n-\bf c}>\epsilon)+\esp{|\bf t^T(\bf Y_n-\bf c)|\1_{\norme{\bf Y_n-\bf c}\le \epsilon}}\\
	&\le 2P(\norme{\bf Y_n-\bf c}>\epsilon)+\esp{\norme{\bf t}\norme{\epsilon}} \cvn 0
\end{align*}
Avec le même raisonnement pour le second terme on a bien que $|\esp{e^{i(\bf s^T\bf X_n+\bf t^T\bf Y_n}}-\esp{e^{i(\bf s^T\bf X+\bf t^T\bf c}}|\cvn 0$

Et ainsi que 


\begin{align*} 
\begin{pmatrix} 
\bf X_n\\
\bf Y_n 
\end{pmatrix} \cvl  \begin{pmatrix} 
\bf X\\
\bf c
\end{pmatrix}
\end{align*}



}

\thm
\textbf{Théorème de Kolmogorov} 

$\Xunan \sim P^*$ iid à valeur dans $\R$, avec $F^*(t) = P(x_i \le t) = P^*((-\infty,t])$ et $\widehat F_n(t) = \frac 1n \somme{i=1}{n}\1_{\{X_i\le t\}}$

En posant $D_n = \Sup{t \in \R}\lvert \widehat F_n(t)-F^*(t)\rvert$ on a :\petitespace

\hspace{2em} 1) La loi de $D_n$ ne dépend pas de $F^*$


\hspace{2em} 2) $\sqrt n D_n\cvl K$ où $\forall t >0, F_K(t) = 1-2\somme{j=1}{\infty}(-1)^{j-1}e^{-2j^2t^2}$, avec $F_K(t) = 0$ si $t\le 0$
\vspace{2em}

\thm 
\textbf{Théorème de Donsker}\petitespace

Soient $\Xunan$des variables aléatoires iid de fonction de répartition $F^*$. On défini le processus aléatoire empirique $G_n(t,\omega) = \sqrt n(\frac 1n\somme{i=1}{n}\1_{\{X_i \le t\}}-F^*(t))$

Alors $G_n \cvl G$ où G est un processus Gaussien dans l'espace des fonctions bornées continues sur $[0,1]$ avec $\esp{G} = 0$




\vspace{2em}

\thm 
\textbf{Delta methode} \petitespace

Soit $(\bf X_n)_{n \in \N^*}$ une suite de vecteurs aléatoires dans $\R^d, d \ge 1$ de carré intégrable, et $$\fonct{g}{\R^d}{\R^p}{x}$$

($p \ge 1$) une fonction $\cal C^1(\R^d)$. Soit $\bf a\in \R^d$ et $\Sigma\in \cal M_{d\times d}(\R)$ telle que :

$$\sqrt n(\bf X_n-\bf a) \cvl \cal N_d(0, \Sigma)$$

Alors, si de plus on a $J_g(\bf a)$ de rang plein, \ie: $\rg(J_g(\bf a)) = \min(d,p)$ (équivalent à $g'(a) \ne 0$ en dimension 1, ou a $\det(J_g(\bf a))  \ne 0$ si $d=p$), on a pour $k=\rg(J_g(\bf a))$

$$\sqrt n(g(\bf X_n)-g(\bf a)) \cvl \cal N_k(0, \nabla g(\bf a)^T\Sigma\nabla g(\bf a))$$
\petitespace

\corr{\textbf{Preuve : } On a : 

$$\sqrt n(\bf X_n-\bf a) \cvl \cal N_d(0, \Sigma)$$

Soit $g_1 \ldots g_p$ dess fonctions de $\R^d$ dans $\R$ tel que $$\forall u \in \R^d, g(u) = \begin{pmatrix} g_1(u) \\ \vdots \\ g_p(u)\end{pmatrix}$$

notons $f_i, \forall i \in \ldbrack 1, p\rdbrack$ des fonctions de $\R$ dans $\R$ tel que 

$$f_i(t) = g_i(t\bf X_n + (1-t)\bf a), \forall t \in \R$$


On a par théorème fondamentale de l'analyse, $\forall i \in \ldbrack 1, p\rdbrack$

\begin{align*}
f_i(1)-f_i(0) &= \int_0^1f_i'(t)dt\\
\iff g_i(\bf X_n)-g_i(\bf a) &= \int_0^1(\nabla g_i(t\bf X_n + (1-t)\bf a))^Tdt( \bf X_n-\bf a)
\end{align*}

D'où on obtient 

$$g(\bf X_n)-g(\bf a) = \int_0^1J_g(t\bf X_n+(1-t)\bf a)dt(\bf X_n-\bf a)$$

équivalent à écrire que 

$$ g(\bf X_n)-g(\bf a) = A_n(\bf X_n-\bf a) \quad A_n = \int_0^1J_g(t\bf X_n+(1-t)\bf a)dt $$

on a donc $$\forall n \in \N^*, \sqrt n(g( \bf X_n)-g(\bf a)) = A_n \sqrt n( \bf X_n-\bf a)$$

Or $\sqrt n (\bf X_n-\bf a) \cvl \cal N_d(0, \Sigma)$ et $\frac{1}{\sqrt n}\cvn 0$\petitespace

Donc, par Slutsky on a $\frac{1}{\sqrt n}\sqrt n(\bf X_n-\bf a) \cvl 0\times Z = 0$ pour $Z \sim \cal N_d(0, \Sigma)$ \petitespace

Équivalent à $\bf X_n-\bf a\cvl 0$ et ainsi $\bf X_n \cvP \bf a$  \petitespace

Comme $g \in \cal C^1(\R^d)$, on a donc $J_g \in \cal C^0(\R^p\times \R^d)$ ainsi :

$$J_g(\bf X_n) \cvP J_g(\bf a)$$

Montrons que $A_n \cvP J_g(\bf a)$. Soit $\epsilon >0$ et $\bf Y_n(t) := t\bf X_n + (1-t)\bf a$, on a :

\begin{align*}
	P(\norme{A_n-J_g(\bf a)}>\epsilon)&=P(\norme{\int_0^1J_g(t\bf X_n+(1-t)\bf a)dt-J_g(\bf a)}>\epsilon)\\
	&=P(\norme{\int_0^1[J_g(\bf Y_n(t))-J_g(\bf a)]dt}>\epsilon)\\
	&\le P(\int_0^1\norme{J_g(\bf Y_n(t))-J_g(\bf a)}dt>\epsilon)\\
	&\le P(\int_0^1\Max{x \in [0,1]}\norme{\{J_g(\bf Y_n(x))-J_g(\bf a)\}}dt>\epsilon)\\
	&\le P(\Max{x \in [0,1]}\norme{\{J_g(\bf Y_n(x))-J_g(\bf a)\}}>\epsilon)\\
\end{align*}

Soit $x \in [0,1]$, soit $M>0 \mid P(\norme{a}>M)=0$ et soit $\chi = B(0,M+1)$, comme $g$ est $\cal C^1(\R^d)$, $J_g$ est continue, donc par le théorème de Heine, $J_g$ est absolument continue sur un compact, en particulier, $J_g$ est absolument continue sur le compact $\chi$ \ie

$$\forall \epsilon >0, \exists \eta >0 \mid \forall \bf x_1, \bf x_2 \in \chi, \norme{\bf x_1-\bf x_2}\le \eta \implies \norme{J_g(\bf x_1)-J_g(\bf x_2)}\le \epsilon$$

Donc pour $\bf x_1 = \bf Y_n(\bf x)$ et $\bf x_2 = \bf a$, on a 

$$\norme{\bf Y_n(x)-\bf a}\le \eta \implies \norme{J_g(\bf Y_n(x))-J_g(\bf a)}\le \epsilon$$ 

D'où par contraposée

$$\norme{J_g(\bf Y_n(x))-J_g(\bf a)}>\epsilon \implies \norme{\bf Y_n(x)-\bf a}> \eta $$

Pour un $\omega$ fixé, on pose $t \in \text{arg}\Max{x \in [0,1]}\norme{\{J_g(\bf Y_n(x))-J_g(\bf a)}$

\begin{align*}
	 P(\Max{x \in [0,1]}\norme{\{J_g(\bf Y_n(x))-J_g(\bf a)}>\epsilon)&=\P(\{\Max{x \in [0,1]}\norme{J_g(\bf Y_n(x))-J_g(\bf a)}>\epsilon\}\cap[\{\norme{\bf a}\le M\} \cap\{\norme{\bf Y_n(t)-a}\le1\}])\\
	 &+P(\{\Max{x \in [0,1]}\norme{\{J_g(\bf Y_n(x))-J_g(\bf a)}>\epsilon\}\cap[\{\norme{\bf a}> M\} \cup\{\norme{\bf Y_n(t)-\bf a}>1\}])\\
	 &\le P(\{\Max{x \in [0,1]}\norme{J_g(\bf Y_n(x))-J_g(\bf a)}>\epsilon\}\cap \{\bf Y_n(t),\bf a \in \chi\})\\ 
	 &+P(\{\norme{\bf a}> M\} \cup\{\norme{\bf Y_n(t)-\bf a}>1\})\text{ on rappelle que $ P(\norme{\bf a}>M)=0$} \\
	 &\le P(\norme{\bf Y_n(t)-\bf a}> \eta) + 0+\P(t\norme{\bf X_n-\bf a}>1)\text{ ($\bf Y_n(t)-\bf a = t(\bf X_n-\bf a)$)}\\ 
	 &\le P(\Max{t \in [0,1]}t\norme{\bf X_n-\bf a}> \eta)+\P(\norme{\bf X_n-\bf a}>1)\text{ $t$ majoré par 1}\\
	 &\le P(\norme{\bf X_n-\bf a}> \eta) + \P(\norme{\bf X_n-\bf a}>1) \cvn 0 \text{ car $\bf X_n \cvP \bf a$}
\end{align*}	

Ainsi on a $\forall \epsilon >0$,

$$ P(\norme{A_n-J_g(\bf a)}>\epsilon)\le P(\Max{x \in [0,1]}\norme{\{J_g(\bf Y_n(x))-J_g(\bf a)\}}>\epsilon) \cvn 0$$

Donc

 $$A_n \cvP J_g(\bf a)$$

Ainsi, par Slutsky, on obtient

\begin{align*}
A_n \sqrt n(\bf X_n-\bf a)& =\sqrt n(g(\bf X_n)-g(\bf a))\cvl \cal N_k(0, J_g(\bf a)\Sigma J_g(\bf a)^T)\\
\iff &\sqrt n(g( \bf X_n)-g(\bf a))\cvl \cal N_k(0, \nabla g(\bf a)^T\Sigma\nabla g(\bf a))
\end{align*}
\petitespace}

\Rq

\cercler 1 Si $g$ est de $\R^d$ dans $\R^d$ il suffit donc simplement que $\nabla g(\bf a)$ ne soit pas de déterminent nul (inversible) et on a donc 

$$\sqrt n(g( \bf X_n)-g(\bf a))\cvl \cal N_d(0, \nabla g(\bf a)^T\Sigma\nabla g(\bf a)) $$

\cercler 2 En réalité, on a pas réellement besoin que $\rg(J_g(\bf a))$ soit de rang plein (soit dit en passant équivalent $\det(\nabla g(\bf a) \nabla g(\bf a)^T) \ne 0$ si $d \le p$ ou bien $\det(\nabla g(\bf a)^T \nabla g(\bf a)) \ne 0$ si $p \le d$). C'est juste que si elle n'est pas de rang plein, on obtiendra une loi normale dégénérée, (une constante) et que cela perd en intérêt, et devient assez inexploitable.

\petitespace

\cercler 3 Par exemple, on lit régulièrement dans les manuels de statistiques que $\nabla g(\bf a)$ doit être de rang plein équivalent quand $g : \R \mapsto \R$ à dire $g'(a) \ne 0$. Mais en réalité, ça fonctionne tout autant si $g'(a) \ne 0$, simplement notre loi normale sera une constante. On voit bien que la preuve ne requiert pas le rang plein. Nous pouvons donner l'exemple très simple avec $Z \sim \normale{0}{1}$, $X_n = a + \frac{Z}{\sqrt n}$ et $g(x) = (x-a)^2$, on a bien $g'(a) = 0$ et $\sqrt n(X_n-a) \cvl \normale{0}{1}$ et on a aussi $\sqrt n(g(X_n)-g(a)) = \frac{Z}{\sqrt n} \cvl 0 \overset{(d)}{=} \normale{0}{g'(a)\times 1} = \normale{0}{0}$. 


\petitespace
\Prop

$\Xunan$ iid de loi $P^*$, alors : \petitespace

\hspace{2em} (i) $n\widehat{F_n}(t) \sim \bin{n}{F^*(t)}$\petitespace

\hspace{2em} (ii) $\sqrt n(\widehat{F_n}(t)-F^*(t)) \cvl \normale{0}{F^*(t)(1-F^*(t))}$\espace

\subsection{Convergence Presque Sûre}\petitespace


\thm 
\textbf{Lemme de Borel-Cantelli probabilistique}\petitespace

Soit $\Xunan$ une suite de variables aléatoires réelles $\in  L^1(P)$, et $X$ une variable aléatoire réelle, toutes sur $\OmegaAP$. Si,


$$\forall \epsilon >0, \lim\limits_{n \to \infty}\somme{i=1}{n}P(\lvert X_i-X\rvert >\epsilon) < +\infty$$

Alors, $X_n \cvps X$\petitespace

Si de plus, les $\Xunan$ sont indépendantes, alors on a 

$$\forall \epsilon >0, \lim\limits_{n \to \infty}\somme{i=1}{n}P(\lvert X_i-X\rvert >\epsilon) < +\infty \iff X_n \cvps X$$\petitespace

\thm 
\textbf{Loi forte des grands nombres :}\\

Soit $\Xnsuite$ une suite de variables aléatoires dans $\R^d$ iid et intégrable, alors \\ $$\overline{X}_n \cvps \esp{X_1}$$

\corr{ \textbf{Preuve : }


%Supposons d'abord que $X_i \overset{p.s.}{\in} [a,b] $ , alors posons $\epsilon >0$, on a 





On va faire le cas où $(X_n)_{n \ge 1} \in L^2( P)$.

Soit $Y_n=X_n-\esp{X_1}$, alors montrer que $\overline X_n \cvps X \iff \overline Y_n \cvps 0$ car $\overline Y_n =\overline X_n-\esp{X_1}$

Par Chebychev, on a $\forall \epsilon>0$,

\begin{align*}
	\P(\lvert \overline Y_{n^2}\rvert >\epsilon) &\le \frac{\Var{\overline{Y}_{n^2}}}{\epsilon^2}\\
	&\le \frac{\Var{\frac{1}{n^2}\somme{i=1}{n^2}(X_i-\esp{X_1})}}{\epsilon^2}\\
	&\le \frac{\Var{X_1}}{(n\epsilon)^2} \cvn 0
\end{align*}

 Donc $$\overline Y_n^2 \cvP 0$$

\begin{align*}
	\overline Y_{n^2} \cvps 0 &\iff \P(\{\omega \in \Omega \mid \overline Y_{n^2}(\omega) \cvn 0\})=1\\
\end{align*}

Or, Soit $\omega \in \Omega, \overline Y_{n^2}(\omega) \cvn 0 \iff \forall l \in \bb{Q}, l >0, \exists n_0 \in \N \mid \forall n \ge n_0, \lvert \overline Y_{n^2}\rvert \le \epsilon$

Donc $ \omega \in \{\omega \in \Omega \mid \overline Y_{n^2}(\omega) \cvn 0\}\iff \omega \in \inter{l\in \Q_+^*}{\infty}\union{n_0 \in \N}{}\inter{n=n_0}{\infty}\{\lvert \overline Y_{n^2}\rvert \le \frac{1}{l}\} $


Soit $l \in \Q^*_+$, 

\begin{align*}
	P(\{\omega \in \Omega \mid \overline Y_{n^2}(\omega) \cvn 0\})&=P(\omega \in \union{n_0 \in \N}{}\inter{n=n_0}{\infty}\{\lvert \overline Y_{n^2}\rvert \le \frac{1}{l}\})\\
	\iff P(\{\omega \in \Omega \mid \overline Y_{n^2}(\omega) \text{  ne converge pas vers 0}\}) &= P(\omega \in \inter{n_0 \in \N}{}\union{n=n_0}{\infty}\{\lvert \overline Y_{n^2}\rvert > \frac{1}{l}\})\\
\end{align*}

Or, par Borel Cantelli, si pour une suite d'évènement $(A_n)_{n \in \N}, \somme{n=1}{\infty} P(A_n) <\infty$, \\alors $P(\lim\sup A_n)=0$, en notant $A_n = \{\omega \in \Omega \mid Y_{n^2}(\omega)\rvert > \frac{1}{l}\}$ (on a montré que \\$\somme{n=1}{\infty}\P(\lvert \overline Y_{n^2}\rvert >\frac{1}{l}) \le \somme{n=1}{\infty}\frac{\Var{X_1}}{(n\epsilon)^2} <\infty$ par Riemann)

On a donc par Borel Cantelli que 
\begin{align*}
	P(\lim\sup A_n)&=\P(\omega \in \inter{n_0 \in \N}{}\union{n=n_0}{\infty}\{\lvert \overline Y_{n^2}(\omega)\rvert > \frac{1}{l}\})\\
	&=0\\
	\implies P((\lim\sup A_n)^c)&=\P( \union{n_0 \in \N}{}\inter{n=n_0}{\infty}\{\lvert \overline Y_{n^2}\rvert \le \frac{1}{l}\})=1
\end{align*}
Donc $\overline Y_{n^2} \cvps 0$\petitespace

Soit $k_n \in \N$ l'unique entier tel que $\forall n \in \N, k_n^2\le n \le (k_n+1)^2$, alors, soit $n \in \N$,

\begin{align*}
	\lvert \overline Y_{n}\rvert &= \frac{1}{n}\lvert \somme{i=1}{k_n^2}Y_i+\frac{1}{n}\somme{j=k_n^2+1}{n}Y_j\rvert\\
	&=\frac{k_n^2}{n}\lvert \frac{1}{k_n^2}\somme{i=1}{k_n^2}Y_i+\frac{1}{k_n^2}\somme{j=k_n^2+1}{n}Y_j\rvert\\
	&\le\lvert \overline Y_{k_n^2}\rvert + \frac{k_n^2}{n}\lvert \frac{1}{k_n^2}\somme{j=k_n^2+1}{n}Y_j\rvert\\
	&\le  \lvert\overline Y_{k_n^2}\rvert + \frac{1}{k_n^2}\lvert \somme{j=k_n^2+1}{n}Y_j \rvert \text{    (on pose $S_n = \somme{i=1}{n}Y_i$)}\\
	&\le \lvert\overline Y_{k_n^2}\rvert + \frac{1}{k_n^2}\lvert S_n-S_{k_n^2}\rvert\\
	&\le \lvert\overline Y_{k_n^2}\rvert + \frac{1}{k_n^2}\max\limits_{l \in \{k_n^2;...;(k_n+1)^2-1\} }\rvert S_l-S_{k_n^2}\rvert
\end{align*}


Ainsi, si $\frac{1}{k_n^2}\max\limits_{l \in \{k_n^2;...;(k_n+1)^2-1\} }\rvert S_l-S_{k_n^2}\rvert\cvps 0$ on aura donc automatiquement que $\overline Y_{n}\cvps 0$ puisqu'on a déjà montré que $\overline Y_{k_n^2}\cvps 0$

Soit $\epsilon >0$, 

\begin{align*}
	&\somme{n \ge 1}{}\P(\frac{1}{k_n^2}\max\limits_{l \in \{k_n^2;...;(k_n+1)^2-1\} }\rvert S_l-S_{k_n^2}\rvert >\epsilon)\\
	&=\somme{n\ge 1}{} \P(\union{l=k_n^2}{(k_n+1)^2-1}\frac{\rvert S_l-S_{k_n^2}\rvert }{k_n^2}>\epsilon)\\
	&\le \somme{n \ge 1}{} \somme{l=k_n^2}{(k_n+1)^2-1}\P(\frac{\rvert S_l-S_{k_n^2}\rvert }{k_n^2}>\epsilon)\text{or $\esp{S_l-S_{k_n^2}}=0$ donc}\\
	&\le \somme{n\ge 1}{} \somme{l=k_n^2}{(k_n+1)^2-1}\frac{\Var{S_l-S_{k_n^2}}}{\epsilon^2nk_n^4}\\
	&\le \somme{n \ge 1}{}\somme{l=k_n^2}{(k_n+1)^2-1}\somme{i=k_n^2+1}{l}\frac{\Var{Y_i}}{\epsilon^2nk_n^4}\\
	&\le  \somme{n \ge 1}{}\somme{l=k_n^2}{(k_n+1)^2-1}\frac{(l-k_n^2)\Var{Y_1}}{\epsilon^2nk_n^4}\\
	&\le  \somme{n \ge 1}{}\frac{((k_n+1)^2-1-k_n^2)\Var{Y_1}}{\epsilon^2nk_n^4}\\
	&\le  \somme{n \ge 1}{}\frac{2}{\epsilon^2k_n^3}\Var{Y_1}\\
\end{align*}

Or, $$	 \somme{n \ge 1}{}\frac{((k_n+1)^2-1-k_n^2)\Var{Y_1}}{\epsilon^2nk_n^4} \le  \somme{n \ge 1}{}\frac{2}{\epsilon^2n^{\frac{3}{2}}}\Var{Y_1}<\infty \quad\quad \text{(Selon Riemman)}$$





On a donc bien $$\frac{1}{k_n^2}\max\limits_{l \in \{k_n^2;...;(k_n+1)^2-1\} }\rvert S_l-S_{k_n^2}\rvert\cvps 0$$

Comme $$\lvert \overline Y_n\rvert  \le \lvert\overline Y_{k_n^2}\rvert + \frac{1}{k_n^2}\max\limits_{l \in \{k_n^2;...;(k_n+1)^2-1\} }\rvert S_l-S_{k_n^2}\rvert$$

On a bien que $$\overline Y_n=\overline X_n -\esp{X_1}\cvps 0$$

équivalent à $$\overline X_n \cvps \esp{X_1}$$
}
 
\thm \textbf{Glivenko-Cantelli }\petitespace

Soient $\Xunan$ des variables aléatoires iid dans $\R^d$, alors

$$\Sup{t \in \R^d}\lvert \widehat{F_n}(t)-F^*(t)\rvert \cvps 0$$

Avec $\widehat{F_n}(t) = \frac 1n\somme{i=1}{n}\1_{X_i \le t}$ la fonction de répartition empirique et $F^*$ la fonction de répartition de $\Xunan$

\petitespace

\Rq

Par loi forte des grands nombres, on a : $\widehat{F_n}(t)  \cvps F^*(t), \quad \forall t \in \R^d$.


\petitespace




\newpage

\section{Fonction Caractéristique et Fonction Gamma}\petitespace

\subsection{Rappel sur la Loi Gamma}\petitespace

On définit la fonction gamma :

$$\fonct{\Gamma}{\C^{\setminus -\N}}{\R}{\int_0^\infty t^{x-1}e^{-t}dt}$$

Avec  $\forall x \in \bb{R}$  $\Gamma(x+1)=x\Gamma(x)$ et donc naturellement, si $n \in \bb{N}, \Gamma(n)=(n-1)!$\petitespace


\textbf{Valeurs à connaître}\petitespace

\shift $\bullet \Gamma(1) = 1 $ (à noter que $\Gamma(0)$ n'est pas définie donc on ne peux pas dans ce cas aller plus loin dans la récurrence)\petitespace

\shift$\bullet \Gamma(\frac 12)=\sqrt \pi$\petitespace

On obtient par récurrence : \petitespace

\shift$\bullet \Gamma(\frac 32)=\frac 12 \Gamma(\frac 12) = \frac 12 \sqrt \pi$




\subsection{Fonction Caractéristique}


Soit $\bf X$ un vecteur aléatoire dans $\R^d, d \ge 1$, alors pour tout $ x \in \R^d$, on a la \textbf{fonction caractéristique} de X
$$ \fonct{\varphi}{\R^d}{\bb{C}}{\esp{e^{i x^T\bf X}}}$$


\textbf{Théorème de Levy : } \\

Soit $(\bf X_n)_{n \ge 1}$ une suite de vecteurs aléatoires dans $\R^d$ et $\bf X$ un vecteur aléatoire dans $\R^d$, on a alors que 

$$ \bf X_n\cvl X \iff \varphi_{\bf X_n}(t) \cvn \varphi_{\bf X}(t)$$

avec pour $n \ge 1$, $ \varphi_{\bf X_n}$ la fonction caractéristique de $\bf X_n$ et $ \varphi_{\bf X}$ la fonction caractéristique de $\bf X$.\\

\Prop

Soit $\bf X$ un vecteur aléatoire dans $\R^d,d \ge 1$ tel que $\bf X \in L^p( P), p \ge 1$ alors pour tout $k\le p$ et pour tout $\bf t \in \R^d$, on a 

$$ \frac{\d^k\varphi}{\d \bf t^k}(\bf t)= \frac{1}{i^k}\esp{\bf X^ke^{i\bf t^T\bf X}}$$

\Prop 
\textbf{caractérisation de l'indépendance par la fonction caractéristique}\petitespace

Soient $\bf X$ et $\bf Y$ deux vecteurs aléatoires sur l'espace probabilisé $\OmegaAP$ dans $\R^d$ et $\R^m$, avec $d,m \ge 1$. Alors $\bf X\indep \bf Y$ si, et seulement si 

$$\forall(\bf s, \bf t) \in \R^d\times \R^m, \quad \varphi_{(\bf X,\bf Y)}(\bf t,\bf s) = \varphi_{\bf X}(\bf t)\varphi_{\bf Y}(\bf s) $$

\petitespace

\Prop

Soit $\bf X$ un vecteur aléatoire réel de fonction caratéristique $\varphi$. Si, pour tout $t \in \R$, $|\varphi(t)| = 1$, alors $\bf X$ est une constante, \ie: $\exists c \in \R^d \mid \bf X \overset{p.s.}{=}c$ 






\newpage
\section{Vecteurs Aléatoires}\petitespace

On définit un vecteur aléatoire $\bf X$ de dimension $d \ge 1$ sur un ensemble probabilité $\OmegaAP$ dans $(E^d, \cal E^{\otimes d})$ un vecteur composé de variables aléatoires $X_1, \ldots, X_d$ sur $\OmegaAP$ dans $\EE$. On peut ainsi écrire :

$$\forall \omega \in \Omega, \quad\quad\quad \bf X(\omega) = \begin{pmatrix} X_1(\omega) \\ \vdots \\ X_d(\omega) \end{pmatrix}  $$

\subsection{Règles de Calcul à Connaître}\petitespace



Pour un vecteur aléatoire réel $\bf X\in \R^m, m \ge 1, \bf u \in \R^m$ on a :\petitespace

\shift 1) $\Var{\bf u^T\bf X} = \bf u^T\Var{\bf X}\bf u$\\


\shift 2) Soit $A \in \cal M_{m\times m}(\R)$, $\Var{A\bf X} = A\Var{\bf X}A^T$\petitespace

 Pour $\bf X$ un vecteur aléatoire dans $\R^d, \quad d \ge 2$, et $\bf Y$ un vecteur aléatoire dans $\R^m, \quad m \ge 2$, on définit la matrice de covariance par :

$$\cov(\mathbf{X}, \mathbf{Y}) = \mathbb{E}[(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{Y} - \mathbb{E}[\mathbf{Y}])^T] = \esp{\bf X \bf Y^T} - \esp{\bf X}\esp{\bf Y}^T$$
et on a donc 

$$\cov(\bf X, \bf Y) = \cov(\bf Y, \bf X)^T$$
\espace

On définit ainsi $\Var{\bf X} = \cov(\bf X, \bf X)$

\petitespace

\Prop 
\textbf{inégalité de Cauchy Schwarz}

Soient $\bf X, \bf Y$ deux vecteurs aléatoires réels de taille $d \ge 1$, de carré intégrable tel que $\esp{\bf Y \bf Y^T}$ est inversible.  Alors 

$$\esp{ \bf X \bf Y^T}\esp{\bf Y \bf Y^T}\esp{\bf Y \bf X^T} \preceq \esp{\bf X \bf X^T}$$

\espace

\subsection{Vecteur Gaussiens}\espace

\Def\espace


Soit $\bf X=\begin{pmatrix} X_1 \\\vdots\\X_d \end{pmatrix}$ un vecteur aléatoire de taille $d \ge 1$.\petitespace

On dit que $\bf X$ est un \textbf{vecteur gaussien} si et seulement si toute combinaison linéaire des coordonnées de $\bf X$ sui une loi normale dans $\R$ \ie: $\forall t_1 \ldots t_d \in \R, \quad \somme{i=1}{d} t_iX_i \sim \normale{\mu}{\sigma^2}, \quad \sigma^2 \ge 0$ \petitespace

\Rq

\shift$\bullet$On rappelle que $\forall \mu \in \R^d, \normale{\mu}{0} \overset{(d)}{=} \delta_{\mu}$ et donc que $\begin{pmatrix} 0 \\\vdots\\ 0 \end{pmatrix}$ est un vecteur gaussien.\petitespace

\shift $\bullet$ si $\bf X=\begin{pmatrix} X_1 \\\vdots\\X_d \end{pmatrix}$ est un vecteur gaussien, alors $\Xunan$ suivent une loi normale.\espace\petitespace


\Def

On appelle $\cal N_d(\mu, \Sigma)$ la loi de n'importe quel vecteur gaussien $\bf X$ tel que $\esp{\bf X}=\mu$ et $\Var{\bf X}=\Sigma$\petitespace

\Rq

Si $\Xunan \simiid \normale{0}{1}$, alors $\begin{pmatrix} X_1 \\ \vdots \\ X_n \end{pmatrix} \sim \cal N(0, \bf I_n)$\espace

\Prop
 
 Soit $\mu \in \R^d$ et $\Sigma \in \cal S_d^+$ où $\cal S_d^+$ représente l'ensemble des matrices symétriques semie définie positives, \ie: $\Sigma \succeq 0$
 
 Soit $\bf X \sim \cal N_d(\mu, \Sigma)$ Alors $\bf X$ admet une densité par rapport à la mesure de Lebesgue si, et seulement si $\Sigma$ est inversible. Dans ce cas, sa densité est donnée par
 
$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  f :& \R^d &\mapsto& \R\\
  & x & \mapsto & \frac{1}{\sqrt{2\pi}\sqrt{det(\Sigma)}}e^{\frac{-1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{array}
$$

\corr{\textbf{Preuve :}

Si $\Sigma$ n'est pas inversible, alors il existe $u \in \R^{d\setminus 0_d}$ tel que $\Sigma u =0_d$ alors $\Var{u^T\bf X}=u^T\Sigma u =0$ donc $u^T\bf X$ est une constance presque surement égale à $u^T\mu$. 

On a alors pour $H=\{x \in \R^d \mid u^Tx=u^T\mu\}$, que $X \in H$ presque surement, or $H$ est un hyperplan affine, donc de mesure de Lebesgue nulle.

Supposons désormais $\Sigma$ inversible, on a donc que $\Sigma^{-1/2}(\bf X-\mu) \sim \cal N_d(0, \bf I_d)$

Soit $\varphi : \R^d \to \R^d$ tel que $\varphi(y)=\mu + \Sigma^{1/2}y$, on a que $\varphi \in \cal C^1$, bijective : $\varphi^{-1}(x) = \Sigma^{-1/2}(x-\mu)$ qui est bien $\cal C^1$

Donc, d'après la formule du changement de variable, on a que $\bf X$ admet une densité donnée par 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  & \R^d &\mapsto& \R\\
  & x & \mapsto & f(\varphi^{-1}(x))|det J_{\varphi^{-1}}(x)|\end{array}
$$

où 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  f :& \R^d &\mapsto& \R\\
  & y & \mapsto & \frac{e^{-\frac{\norme{y}^2}{2}}}{(2\pi)^{d/2}} \end{array}
$$\petitespace

Avec $\text{det}( J_{\varphi^{-1}}(x))=\Sigma^{-1/2}$ ($\varphi^{-1}$ est affine)

D'où l'on tire la densité de $\bf X$ :

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  g :& \R^d &\mapsto& \R\\
  & x & \mapsto & \frac{1}{\sqrt{2\pi}\sqrt{det(\Sigma)}}e^{\frac{-1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{array}
$$
\petitespace}

\Prop

Soit $\bf X,\bf Y$ deux vecteurs aléatoires de tailles respectives $p$ et $q$ tel que $\begin{pmatrix} \bf X\\\bf Y \end{pmatrix}$ est un vecteur gaussien.

Alors $\bf X\indep \bf Y \iff \cov(\bf X,\bf Y)=0$\petitespace

\corr{\textbf{Preuve :}

Si $\cov(\bf X,\bf Y)=0$, alors :

\[
\Var{\begin{pmatrix} \bf X\\\bf Y \end{pmatrix}}=\begin{pmatrix}
\Sigma_1 & 0 \\
0 & \Sigma_2
\end{pmatrix}
\]
\petitespace

Alors $\forall (s,t) \in \R^p\times\R^q$ notons $\esp{\bf X}=\mu_1, \esp{\bf Y}=\mu_2, \Sigma_1=\Var{\bf X}, \Sigma_2=\Var{\bf Y}$


\begin{align*}
	\varphi_{(X,Y)}(s,t)&=e^{i(s^T\mu_1+t^T\mu_2)-\frac{-1}{2}\begin{pmatrix} s\\t \end{pmatrix}^T\Sigma\begin{pmatrix} s\\t \end{pmatrix}}\\
	&=e^{is^T\mu_1-\frac{s^T\Sigma_1s}{2}}e^{it^T\mu_2-\frac{1}{2}t^T\Sigma_2t}\\
	&= \varphi_X(s)\varphi_Y(t)
\end{align*}

(L'autre sens est toujours vrai)
\petitespace}


\thm

On l'appelle parfois le théorème de \textbf{Cochran}.


Soit $\bf X \sim \cal N_d(0,\bf I_d)$, $d\ge 1$, un vecteur aléatoire. Alors 

$$\norme{\bf X}_2^2 \sim \chideux{d}$$

\petitespace

\corr{\textbf{Preuve :}

On a :

$$\bf X = \begin{pmatrix} X_1\\ \vdots\\ X_d \end{pmatrix} \overset{(d)}{=}  \begin{pmatrix} Y_1\\ \vdots\\ Y_d \end{pmatrix} = \bf Y, \quad\quad Y_1, \ldots Y_d \simiid \normale{0}{1} $$

Donc, prenons $t \in \R$, on a

\begin{align*}
	\varphi_{\norme{\bf X}_2^2}(t) &= \esp{e^{it\norme{\bf X}_2^2}} \\
	&= \esp{e^{it\norme{\bf Y}_2^2}}\\
	& = \varphi_{Y_1^2}(t)^d
\end{align*}

Regardons la loi de $Y_1^2$, soit $h$ une fonction mesurable positive, on a 

\begin{align*}
	\esp{h(Y_1^2)} &= \frac{1}{\sqrt{2\pi}} \int_\R h(x^2) e^{-\frac{x^2}{2} } dx\\
	&=  \frac{1}{\sqrt{2\pi}}\big(\int_{+\infty}^0 h(u) e^{-\frac u2}  \frac{1}{\sqrt{2u}}du + \int^{+\infty}_0 h(u) e^{-\frac u2}  \frac{1}{\sqrt{2u}} du \big)\\
	&=  \int^{+\infty}_0 h(u) \frac{1}{\sqrt{2\pi}} e^{-\frac u2}  \frac{1}{\sqrt{u}} du
\end{align*}

On reconnaît par la méthode de la fonction test la densité $f(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac u2}  \frac{1}{\sqrt{u}}$ de la loi $\gaminv{\frac 12}{\frac 12} = \chideux{1}$.

\petitespace

Donc $\varphi_{Y_1^2}(t) = (\frac{\frac 12}{\frac 12-it})^{\frac 12} = (\frac{1}{1-2it})^{\frac 12}$ et donc 

$$(\varphi_{Y_1^2}(t))^d =  (\frac{1}{1-2it})^{\frac d2} = \varphi_\Psi(t), \quad \quad \Psi \sim \gaminv{\frac d2}{\frac 12} =\chideux{d}$$

Et donc on a bien 

$$\norme{\bf X}_2^2\sim \chideux{d} $$

\petitespace}

\espace

\subsection{Théorèmes de Convergences pour les Vecteurs Aléatoires}\espace

\thm
\textbf{Théorème central limite multi-varié}\petitespace

Soit une suite de vecteurs aléatoires réels \( \mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n \) indépendants et identiquement distribués tel que \( \mathbb{E}[\mathbf{X}_i] = \boldsymbol{\mu} \) et  \( \Var{X_1} = \boldsymbol{\Sigma} \), le \textbf{théorème central limite multivarié} stipule que :

\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n (\mathbf{X}_i - \boldsymbol{\mu}) \cvl \mathcal{N}_m(\mathbf{0}, \boldsymbol{\Sigma})
\]
\petitespace

\newpage

\section{Espérances Conditionnelles}

\subsection{Espérances Conditionnelles pour les Variables Aléatoires de Carré Intégrable}

Soit $X$ une variable aléatoire définie sur un espace probabilisé $\OmegaAP$, supposons que $X$ admette un moment d'ordre deux. \ie : $X \in  L^2\OmegaAP$.

Soit $\cal B$ une sous tribu de $\cal A$ \ie : $\cal B \subset \cal A$, où $ L^2 (\Omega, \mathcal B, P)$ est l'ensemble des variables aléatoires $\cal B$-mesurable. \espace

\textbf{Lemme}\petitespace

$L^2\OmegaAP$ est un \textbf{espace Hilbertien} (\ie \hspace{0.1em} un espace vectoriel complet muni d'un produit scalaire) avec comme produit scalaire

$$X,Y \in L^2\OmegaAP \mapsto \esp{XY} $$

De plus, si $\cal B$ est une sous tribu de $\cal A$, alors $L^2(\Omega, \cal B,  P)$ est un \textbf{sous-espace vectoriel fermé} de $L^2\OmegaAP$.

\petitespace
\Def

Soit $X \in \cal L^2 \OmegaAP$, son espérance conditionnelle sachant $\cal B$ est la projection orthogonale de X sur $L^2(\Omega, \mathcal B,  P)$, \ie : l'unique $\widetilde X \in L^2(\Omega, \mathcal B,  P) \mid \forall Y \in L^2(\Omega, \mathcal B,  P)$

$$\esp{(X-\widetilde X)Y}=0 \iff \esp{\widetilde XY}=\esp{XY}$$\\

On note alors $\widetilde X = \espcond{X}{\cal B}$ \petitespace

\Rq\petitespace

Si $\cal B = \big\{ \emptyset, \Omega \big\}$, alors $\espcond{X}{\cal B} = \esp{X}$, en effet, soit $\omega \in \Omega$, et soit $\lambda = \espcond{X}{\cal B}(\omega)$, alors 

$$\espcond{X}{\cal B}^{-1}(\lambda) = \Omega \text{\shift (car $\omega \in \espcond{X}{\cal B}^{-1}(\lambda)$). }$$\petitespace

Donc $ \forall \omega \in \Omega, \quad\espcond{X}{\cal B}(\omega) = \lambda$. Or, pour toute variable aléatoire $\cal B$-mesurable, on a 

$$ \esp{XY} = \esp{\espcond{X}{\cal B} Y}$$

Pour $Y(\omega) = 1$ (constante pour tout $\omega \in \Omega$, donc $\cal B$-mesurable) on a donc 

$$ \esp{X} = \esp{\espcond{X}{\cal B}} = \lambda $$

d'où $ \espcond{X}{\cal B} = \esp{X}$

\petitespace
\Prop

Il est important de noter aussi que $\espcond{X}{\cal B}$ est l'unique variable aléatoire $\widetilde X \in L^2(\Omega, \mathcal B, P) \mid \forall B \in \cal B$, 

$$\esp{X\1_{B}}=\esp{\widetilde X \1_B}$$\petitespace

\corr{\textbf{Preuve :}

Par définition de $\espcond{X}{\cal B}$, comme $\1_B$ est $\cal B$-mesurable, on a bien que 

$$\forall B \in \cal B, \quad, \esp{\espcond{X}{B}\1_B} = \esp{\esp{X}\1_B} $$

Montrons désormais que $\espcond{X}{\cal B}$ est la seule variable aléatoire vérifiant cette propriété.

Soit $\tilde X \in \cal L^2(\Omega, \cal B, P)$ vérifiant  

$$\forall B \in \cal B, \quad \esp{\tilde X \1_B} = \esp{\esp{X}\1_B} $$

et montrons que $\tilde X = \espcond{X}{\cal B}$. Soit $Z \in L^2(\Omega, \cal B,  P)$, montrons que $\esp{XZ} = \esp{\tilde X Z}$. 

On sait qu'il existe une suite $(Z_n)_{n \ge 1}$ de variable aléatoires étagées $\cal B$-mesurables telles que 

$$\esp{(Z_n-Z)^2} \cvn 0 $$

Pour tout $n \ge 1$, $\esp{XZ_n} = \esp{ \tilde X Z_n}$ (car $(Z_n)_{n \ge 1}$ étagées), on a juste à montrer que 

$$\esp{XZ_n} \cvn \esp{XZ} \quad \text{et} \quad \esp{\tilde XZ_n} \cvn \esp{\tilde XZ}  $$

On a 

$$|\esp{XZ_n} - \esp{XZ} | = |\esp{X(Z_n-Z)}| \overset{C.S.}{\le} \sqrt{\esp{X^2}\esp{(Z_n-Z)^2}} \cvn 0$$

De même, 

$$|\esp{\tilde XZ_n} - \esp{\tilde XZ} | = |\esp{\tilde X(Z_n-Z)}| \overset{C.S.}{\le} \sqrt{\esp{\tilde X^2}\esp{(Z_n-Z)^2}} \cvn 0$$

Ce qui montre bien que $\esp{XZ_n} \cvn \esp{XZ}$ et $ \esp{\tilde XZ_n} \cvn \esp{\tilde XZ}$

Et donc que $\tilde X = \espcond{X}{\cal B}$


\petitespace}

\Rq

Si $\cal B$ et une sous-tribu de $\cal A$ engendrée  par une variable aléatoire $Z$ sur $\Omega$ \ie \hspace{0.1em} $\cal B= \sigma(Z)$, on note 

$$\espcond{X}{\cal B}=\espcond{X}{Z}$$ \petitespace

De plus, pour $Y$ dans $( \Omega, \cal F,  P)$ et $Z$ dans  $(\Omega, \cal G,  P)$, on a 

$$\espcond{X}{Y,Z} =  \espcond{X}{\sigma(Y,Z)}$$

Avec $\sigma(Y,Z) = \big\{ (Y,Z)^{-1}(A), A \in \cal F \otimes \cal G  \big\}$

\petitespace

\Prop \textbf{Lemme de Doob-Dynkin}\petitespace

Toute variable aléatoire $\sigma(Z)$-mesurable peut s'écrire sous la forme $h(Z)$ où $h$ est une fonction mesurable.\petitespace

\corr{\textbf{Preuve :}

Soit $X$ une variable aléatoire définie sur un espace probabilisé $\OmegaAP$, à valeurs dans un espace $\EE$ quelconque. et soit $Y$ sur $(\Omega, \cal B, P)$ dans $\FF$ avec $\cal B = \sigma(X) = \sigma(\{X^{-1}(A), A \in \cal E\})$

Montrons que $\exists h : E \mapsto F$ mesurable, tel que $Y=h(X)$. Pour cela, procédons en plusieurs étapes :\petitespace

\cercler 1 Supposons que $Y=\1_A$, pour un certain $A \in \cal A$

Alors, nécessairement, il existe $B \in \cal E$ tel que $A=X^{-1}(B)$, car $A = Y^{-1}(\{1\})\subset \cal B = \sigma(X) = \sigma(\{X^{-1}(A), A \in \cal E\})$ donc $\exists B \in \cal E \mid X^{-1}(B)=A$. Donc 

\begin{align*}
	Y^{-1}(\{1\}) = &A = X^{-1}(B)\\
	&A^c = (X^{-1}(B))^c = X^{-1}(B^c)\\
	&\Omega = X^{-1}(B) \cup X^{-1}(B^c)\\
	&\phi = X^{-1}(\phi)
\end{align*}


On prend alors $Y = \1_B(X)$, en effet, $\forall \omega \in \Omega, Y(\omega) = \1_B(X(\omega))$, en effet, 

\begin{align*}
	&\text{Si $\omega \in A$}, Y(\omega) = 1 \text{ et } \1_B(X(\omega)) = 1\text{ car } X(\omega) \in X(A) = X(X^{-1}(B))\subset B\\
	& \text{Si $\omega \in A^c$}, Y(\omega) =0, \1_{B^c}(X(\omega)) = 0 \text{ car } X(\omega) \in X(A^c) = X(X^{-1}(A)^c) = X(X^{-1}(A^c)) \subset B^c
\end{align*}

Par conséquent, $Y = \1_B(X)$ et donc $h = \1_B$\petitespace

\cercler 2 Si $Y$ est étagée, $Y = \somme{i=1}{n} \alpha_i \1_{A_i} =  \somme{i=1}{n} \alpha_ih_i(X) = h(X)$ avec $h$ mesurable comme composée de fonctions mesurables. \petitespace

\cercler 3 Supposons enfin $Y$ variable aléatoire quelconque. On peut, sans perte de généralité, supposer que $Y$ est positive car pour tout $Y$, on pourra se ramener à $Y = h^+ - h^-$ avec $h^+ = \sup(h,0), h^-=\sup(-h,0)$

Supposons donc $Y$ une variable aléatoire positive quelconque, et $(Y_n)_{n \ge 1}$ une suite croissante de variables aléatoires étagées positives. Alors $\forall n \in \N^*, Y_n = h_n(X)$, où $h_n$ est une fonction mesurable, et donc 

$$ \Lim{n \to + \infty}Y_n = Y = \sup h_n(X) = h(X)$$

avec $h$ mesurable comme sup de fonctions mesurables. 
\petitespace}

\Rq

On déduit de la définition des espérances conditionnelles que pour toute fonction $f$ mesurable, et pour toute variable aléatoire $X,Y$, on a 

$$\esp{(X-\espcond{X}{Y})f(X)} = 0 $$

En effet, on a :

\begin{align*}
	\esp{(X-\espcond{X}{Y})f(X)} &= \esp{Xf(X)} - \esp{\espcond{X}{Y}f(X)} \\
	&=  \esp{Xf(X)} - \esp{Xf(X)} =0 \text{ (par définition)}
\end{align*}


\Prop 


Soit $X,Y \in L^2 \OmegaAP$, et $\cal B$ une sous tribu de $\cal A$, alors :\petitespace

\shift 1)  $\forall \lambda,\mu\in \R$ on a :

$$ \espcond{\lambda X + \mu Y}{\cal B}= \lambda\espcond{X}{\cal B}+\mu\espcond{Y}{\cal B}$$\\

\shift 2) Si $X \ge$ 0 p.s., Alors :

$$\espcond{X}{\cal B} \ge 0\text{  p.s.}$$\\

\shift 3)$$\esp{\espcond{X}{\cal B}}=\esp{X}$$\\

\shift 4) Si $\cal E$ sous tribu de $\cal B$, alors :

$$ \espcond{\espcond{X}{\cal B}}{\cal E}= \espcond{X}{\cal E}$$\\

\shift 5) $\forall X \in L^2(\OmegaAP)$, 

$$\lvert \espcond{X}{\cal B}\rvert \le \espcond{\lvert X \rvert }{\cal B}\text{  p.s.}$$\\

\shift 6) Soit $Y$ une variable aléatoire $\cal B$-mesurable bornée, alors :

$$\espcond{XY}{\cal B}=Y\espcond{X}{\cal B}$$\petitespace

\corr{\textbf{Preuve :}\petitespace

\cercler 1 C'est direct par linéarité de l'espérance.

\petitespace

\cercler 2 Supposons que $X \overset{p.s.}{\ge} 0$. On pose $\tilde X = \espcond{X}{\cal B}$ et $B = \big\{ \omega \in \Omega \mid \tilde X(\omega) <0 \big\} = \tilde X^{-1}(\R_-) \in \cal B$, car $\tilde X$ est $\cal B$-mesurable. Ainsi, $\1_B \in L^2(\Omega, \cal B,  P)$, d'où 

$$\esp{\tilde X \1_B} = \esp{X\1_B} $$ 

Or, $\esp{ X \1_B} \ge 0$ car $X$ est positive, donc $\esp{\tilde X \1_B} \ge 0$. D'autres part, $\tilde X \1_B \le 0$, donc $\tilde X \1_B \overset{p.s.}{=} 0$. Donc 

$$ P(\underbrace{\tilde X \1_B \ne 0}_{ \supset B}) =0) $$

Donc $ P(B) = 0$. et ainsi $ P(\tilde X \ge 0) = 1$ donc on a bien $\espcond{X}{\cal B} \overset{p.s.}{\ge} 0$

\petitespace

\cercler 3 Pour $Y \overset{p.s.}{=}1$, $\cal B$-mesurable, on a 

$$\esp{X} = \esp{\espcond{X}{\cal B}} $$

\petitespace

\cercler 4 C'est direct par le fait que l'on projette $X$ sur $L^2(\Omega, \cal B,  P)$ puis sur $L^2(\Omega, \cal C,  P)$

\petitespace

\cercler 5 On a $|X|-X \ge 0$, donc $\espcond{|X|-X}{\cal B} \overset{p.s.}{\ge} 0$ \ie

$$\espcond{|X|}{\cal B} \ge \espcond{X}{\cal B} $$

De même pour $|X|+X \ge 0$, on a 

$$ \espcond{|X|}{\cal B} \ge -\espcond{X}{\cal B} $$

D'où 

$$ |\espcond{X}{\cal B}| \overset{p.s.}{\le} \espcond{|X|}{\cal B}  $$

\petitespace

\cercler 6 Soit $Z \in L^2(\Omega, \cal B,  P)$, il suffit de montrer que 

$$\esp{XYZ} = \esp{UZ} $$

où $U = Y\espcond{X}{\cal B}$. On a 

$$\esp{XYZ} = \esp{X(YZ)} = \esp{\espcond{X}{\cal B} (YZ)} = \esp{(Y\espcond{X}{\cal B}) Z}  $$

car $YZ \in L^2(\Omega, \cal B,  P)$.


\petitespace}

\subsection{Généralisation aux Variable Intégrables}\petitespace

\Prop 

Soit $X \in L^1\OmegaAP$, et $\cal B$ une sous tribu de $\cal A$. Alors $\exists!\widetilde X \in \cal L^1(\Omega, \mathcal B,  P)\mid \forall B \in \cal B$,

$$\esp{\widetilde X\1_B}=\esp{X\1_B}$$

On note donc $\widetilde X=\espcond{X}{\cal B}$. 
\petitespace

\corr{\textbf{Preuve :}

\cercler 1 Existence : Supposons que $X \ge 0$. Pour $n \ge 1$, notons $X_n = X\1_{X \le n}$. Alors $X_n \in L^2\OmegaAP$ car $X_n$ est bornée pour tout $n$. De plus, $(X_n)_{n \in \N^*}$ est croissante et $X_n \cvps X$. \petitespace

Pour tout $n \ge 1$, soit $\tilde X_n = \espcond{X_n}{\cal B}$. Alors $(\tilde X_n)_{n \in \N^*}$ est une suite positive et croissante. 

En particulier, $\tilde X_n$ converge $P-$p.p vers une variable aléatoire $\tilde X$ à valeur dans $\R_+\cup\{+\infty\}$ presque-sûrement.

 De plus, par la propriété $5|5.1|1$, on a 

$$\forall n \ge 1, \quad \forall B \in \cal B,\quad  \esp{X_n \1_B} = \esp{\tilde X_n \1_B} $$

En prenant $B = \Omega \in \cal B$, $\esp{X} = \esp{\tilde X}$ (par théorème de convergence monotone), on a $\esp{\tilde X}<\infty$, donc $\tilde X \in L^1(\Omega, \cal B, P)$. Lorsque $X$ est de signe quelconque, on peut considérer $X^+$ et $X^-$ les parties positives et négatives de $X$
\petitespace

\cercler 2 unicité : Soit $\tilde X$ et $\tilde X'$ deux variables aléatoires dans $L^1(\Omega, \cal B, P)$ telles que

$$\forall B \in \cal B,\quad  \esp{X \1_B} = \esp{\tilde X \1_B}  = \esp{\tilde X' \1_B}  $$

Soit $B = \{ \omega \in \Omega \mid \tilde X(\omega) <  \tilde X'(\omega) \} \in \cal B$. Alors $\esp{( \tilde X-  \tilde X') \1_B} = 0$, or $( \tilde X-  \tilde X')\1_B \overset{p.s.}{\le}0$. On a donc 

$$ P(( \tilde X-  \tilde X')\1_B = 0) = 1 \iff   P(\underbrace{( \tilde X-  \tilde X')\1_B}_{\supset B} \ne 0) = 0 $$

D'où 

$$ P(B) = 0	 $$

Et donc $\tilde X \overset{p.s}{\ge}  \tilde X'$. 

En faisant exactement le même raisonnement avec $B' = \{ \omega \in \Omega \mid \tilde X'(\omega) <  \tilde X(\omega) \}$ on obtient donc que 

$$ \tilde X' \overset{p.s}{=} \tilde X $$

\petitespace}


\Prop 

Dans le cas $L^1\OmegaAP$ aussi, on a les \textbf{même propriétés} que les espérances conditionnelles dans $L^2 \OmegaAP$.\petitespace

\thm
\textbf{Théorème de Transfert Conditionnelle : }\\  

Soient $X,Y$ deux variable aléatoires quelconques à valeurs dans des espaces mesurables $\EE$ et $\FF$, supposons que $X\indep Y$ et soit h mesurable :

$$
\ffonct{h}{E}{F}{\R}{h(X,Y)} \text{($x$=$X$, $y=Y$)}
$$

$h \in \cal L^1\OmegaAP$, \ie $h \in \cal L^1(\P_X \otimes \P_Y)$

Alors, 

$$\espcond{h(X,Y)}{Y}=H(Y)$$

où 

$$\fonct{H}{F}{\R}{\esp{h(X,x)}=\int_Eh(y,x)dP_X(y)}$$

\ie : $H(Y)=\esp{h(X,y)}$

\petitespace

\corr{\textbf{Preuve :}

posons $\cal B=\sigma(Y)=\sigma(\{Y{-1}(A), A \in \cal F\})$, soit $A \in \cal B$, on a :


\begin{align*}
	\esp{h(X,Y)\1_{Y \in A}}&=\int_{E\times F}h(x,y)\1_{y \in A}dP_X(x)dP_Y(y)\\
	&=\int_F(\int_E h(x,y)\1_{y \in A}dP_X(x))dP_Y(y)\\
	&= \int_FH(y)\1_AdP_Y(y)\\
	&=\esp{H(Y)\1_A}
\end{align*}
}

\textbf{Corollaire :}

Soit $X$ intégrable, $Y$ une variable aléatoire quelconque tel que $X \indep Y$. Alors

\begin{align*}
\espcond{X}{Y}&=\espcond{h(X,Y)}{Y}\quad \text{   où $\quad \ffonct{h}{E}{F}{\R}{x}$}\\\\
&=H(Y)\\
&=\int_\R h(x,Y)dP_X(x)\\
&=\int_\R XdP_X(x)\\
&=\esp{X}
\end{align*}

$\implies \espcond{X}{Y}=\esp{X}$

\petitespace

\Prop \petitespace

Soit $X$ une variable aléatoire sur $\OmegaAP$ telle que $X \in L^1( P)$ et $A$ un évènement tel que $ P(A) >0$. Alors 

$$\espcond{X}{A} = \frac{\esp{X\1_{\{ A \}} }}{\esp{\1_{\{ A\} }}} =  \frac{\esp{X\1_{\{ A \}} }}{ P(A)}$$


\corr{\textbf{Preuve :}

On a $\sigma(A) = \big\{ \phi, A, A^c, \Omega\big\}$. Soit $B \in \sigma(A)$, alors $\esp{X\1_B} = \esp{\espcond{X}{A}\1_B} $. Notons $\tilde X = \espcond{X}{\sigma(A)}$

Pour tout $\omega \in \Omega$, on a \petitespace

\shift $\bullet$ Soit $a \in A$ et $x_a = \tilde X(a)$ alors  $\tilde X^{-1}(x_a) = A$ ou $\Omega$ (comme $a \in A$, on a $a \in \tilde X^{-1}(x_a)$, alors

\shift\shift $\tilde X^{-1}(x_a) \ne A^c$, puisque de manière générale, $\tilde X^{-1}(\tilde X(A)) \supset A$ et $\tilde X(\tilde X^{-1}(A)) \subset A$)  \petitespace

\shift $\bullet$ Soit $a_c \in A^c$ et $x_{a_c} = \tilde X(a_c)$ alors  $\tilde X^{-1}(x_{a_c}) = A^c$ ou $\Omega$ avec le même raisonnement. \petitespace


(Par ailleurs, si on a $X$ dans $(\R, \cal B(\R))$ on a donc que $\exists \lambda, \mu \in \R,\forall \omega \in \Omega, \quad \tilde X(\omega) = \lambda \1_A + \mu \1_{A^c} $ ) \petitespace

Dans tous les cas, $\tilde X$ est constante sur $A$, d'où le fait que $\espcond{X}{A} = \lambda$, ainsi :

\begin{align*}
	\esp{X\1_A} &= \esp{\espcond{X}{\sigma(A)}\1_A} \\
	& = \underbrace{\espcond{X}{A}}_{=\lambda}\esp{\1_{A}}
\end{align*}

D'où

$$ \espcond{X}{A} = \frac{\esp{X\1_A}}{\esp{\1_A}} $$


\petitespace}

\Prop
\textbf{Formule des espérances totales}\petitespace

Soit $X$ une variable aléatoire sur $\OmegaAP$, $X \in L^1(P)$, et $E$ un ensemble tel que pour $(B_i)_{i \in E}$ une partition de $\Omega$ \ie \shift $\union{i \in E}{} B_i = \Omega$ et $\forall i \ne j,\quad B_i \cap B_j =\emptyset $, et tel que $\forall i \in E,\quad P(B_i) >0$, alors 

$$\esp{X} = \somme{i \in E}{}\espcond{X}{B_i} P(B_i) $$

\petitespace

\corr{\textbf{Preuve :}

Comme $(B_i)_{i \in E}$ forme une partition de $\Omega$, on a que $\somme{i\in E}{} \1_{B_i} \overset{p.s.}{=} 1$.

De plus, comme pour tout $i\in E,\quad P(B_i) >0$, on a que pour tout $i \in E, \espcond{X}{B_i} = \frac{\esp{X\1_{B_i}}}{ P(B_i)}$. Ainsi :

\begin{align*}
	\esp{X} &= \esp{X \somme{i \in E}{} \1_{B_i}}\\
	&= \somme{i\in E}{}\esp{X\1_{B_i}} \text{ (Fubini)} \\
	& = \somme{i\in E}{} \espcond{X}{B_i}  P(B_i)
\end{align*}
\petitespace}


\Def

Soit $\bf X$ un vecteur aléatoire réeel dans $\R^d, d \ge 1$, intégrable. On définit l'espérance conditionnelle du vecteur $\bf X $ sachant $\cal B$ comme 

$$\espcond{\bf X}{\cal B} = \begin{pmatrix} \espcond{X_1}{\cal B} \\ \vdots \\ \espcond{X_d}{\cal B} \end{pmatrix}  $$



\newpage


\section{Lois Conditionnelles}\petitespace

\subsection{Définitions et Premières Propriétés} \espace

Soient $\EE, \FF$ deux espaces mesurables. Un \textbf{noyau de transition} de $\EE$ vers $\FF$ est une fonction 

$$\d : E \times \cal F \mapsto \R $$

telle que \petitespace

\shift $\bullet\quad \forall B \in \cal F, \quad \d(.,B)$ est mesurable \petitespace

\shift  $\bullet\quad \forall x \in E,\quad \d(x,.) $ est une mesure de probabilité sur $\FF$\petitespace

\underline{Exemple}:\petitespace

$\forall x \in \R, \mu_x = \normale{x}{1}$

$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \d :& \R \times \cal B(\R) &\mapsto& \R\\
  & (x,B) & \mapsto &\mu_x(B) =\frac{1}{2\pi}\int_B e^{\frac{-1}{2}(t-x)^2} dt \end{array}
$$\petitespace

est un noyau de transition de $(\R, \cal B(\R))$ vers $(\R, \cal B(\R))$.\petitespace

\Prop 

Soient $\EE$ et $\FF$ deux espaces mesurables, et soient $X$ et $Y$ deux variables aléatoires sur $\OmegaAP$ à valeures dans $E$ et $F$. Alors il existe un noyau de transition $\d$ de $\EE$ vers $\FF$ tel que :

$$\forall A \in \cal E,\quad \forall B \in \cal F,\quad P(\{ X \in A\} \cap \{Y \in B\}) = \int_A \underbrace{\d(x,B)}_{P_{Y|X=x}(B)} dP_X(x) $$

Pour tout $x \in A$, $\d(x,.)$ est appelée \textbf{loi conditionnelle de $Y$ sachant $X=x$} et on la note $P_{Y|X=x}$, $\d$ est appelée "loi conditionnelle de $Y$ sachant $X$"

$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \d :& E \times F &\mapsto& \R\\
  & (x,B) & \mapsto &P_{Y|X=x}(B) \end{array}
$$
\petitespace

\underline{Exemple}:\petitespace

Soit $X$ une variable aléatoire dans $\N$, soit $\epsilon \sim \normale{0}{1}$ tel que $\epsilon \indep X$. Soit $Y  = X+\epsilon$

 Alors, $\forall A  \in \cal P(\N), \quad \forall B \in \cal B(\R)$, 
 
 \begin{align*}
 	&P(\{ X \in A\} \cap \{Y \in B\}) = \int_A \P_{Y|X=x}(B)dP_X(x)\\
	=&P(\union{x \in A}{} [X=x]\cap [Y \in B])\\
	=&\somme{x \in A}{} P(\{X=x\}\cap\{Y \in B\})\\
	=& \somme{x \in A}{} P(\{X=x\}\cap\{\epsilon +x \in B\})\\
	=&  \somme{x \in A}{} P(\{X=x\})\P(\{\epsilon +x \in B\}) \text{ (par indépendance de $X$ et $\epsilon$)}\\
	=& \int_A \mu_x(B)d P_X(x) \text{ ($\epsilon + x \sim \normale{x}{1}$)}
 \end{align*}

Ainsi, on a 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \d :& \N \times \cal B(\R) &\mapsto& \R\\
  & (x,B) & \mapsto &\mu_x(B) \end{array}
$$

et donc, $\forall x \in \N,\quad P_{Y|X=x} = \normale{x}{1}$\petitespace

\Prop

Si $P$ est une mesure de probabilité sur $\EE$ et $\d$ est un noyau de transition de $\EE$ vers $\FF$, alors il existe une unique probabilité $Q$ sur $(E\times F, \cal E \otimes \cal F)$, telle que :

$$\forall A \in \cal E,\quad \forall B \in  \cal F, \quad Q(A\times B) = \int_A \d(x,B)dP(x) $$\petitespace


\subsection{Densités Conditionnelles} \espace

\Prop

Soient $\mu_1$ et $\mu_2$ deux mesures $\sigma-$finies sur $\EE$ et $\FF$ respectivement. 

Soient $X,Y$ deux variables aléatoires dans $E$ et $F$ respectivement tel que 

$$P_{(X,Y)}\ll \mu_1 \otimes \mu_2, \quad \forall x \in E ,\quad \frac{dP_X}{d \mu_1}(x) \ne 0$$

Soit la densité sur $F$

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  f_{Y|X=x} :&  F &\mapsto& \R\\
  & y & \mapsto &{\huge \frac{ \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y)}{ \frac{dP_X}{d \mu_1}(x)}} \end{array}
$$

La fonction associée est une loi conditionnelle de $Y$ sachant $\{X=x\}$. $f_{Y|X=x}$ "\textbf{densité conditionnelle de $Y$ sachant $\{X=x\}$}". \petitespace

\Rq

À noter que, d'une manière générale, quand on écrit $\frac{dP_X}{d \mu_1}(x)$, on parlera toujours de la \textbf{densité de $P$ par rapport à $\mu$}, il s'agit donc d'un abus de notation en soit mais très intuitif. Par exemple, si on a $\cal P$ qui admet une densité $f$ par rapport à une mesure $\mu$, on a alors que 

$$f = \frac{d\cal P}{d\mu} $$

\petitespace

\corr{\textbf{Preuve : }

Soient $A \in \cal E$ et $B \in \cal F$

\begin{align*}
	P(\{X \in A\} \cap \{ Y \in B\}) &= \int_{A \times B}dP_{(X,Y)}(x,y)\\
	&= \int_{A \times B} \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y) d\mu_1 \otimes \mu_2(x,y)\\
	&=\int_A\big(\int_B \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y) d\mu_2(y)\big)d\mu_1(x) \\
\end{align*}

Soit $\widetilde A = \{x \in A \mid \frac{d\P_X}{d\mu_1}(x) \ne 0\} \in \cal E$, alors $A = \widetilde A \cup \widetilde{A}^{^{^{^c}}}$, $\forall x \in  \widetilde{A}^{^{^{^c}}}, 0=\frac{dP_X}{d\mu_1}(x) =\int_F \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y)dy$

Mais comme $\frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y) \ge 0$, on a que $\forall x \in \widetilde{A}^{^{^{^c}}}, \quad \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y)=0$ pour $\mu_2$- presque tout $y\in F$

D'où :

\begin{align*}
	P(\{X \in A\} \cap \{ Y \in B\}) &= \int_{\widetilde{A}}\big(\int_B \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y) d\mu_2(y)\big)d\mu_1(x) \\
	&=  \int_{\widetilde{A}}\big(\int_B \underbrace{\frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y)}_{=f_{X,Y}} \big/ \underbrace{\frac{dP_X}{d\mu_1}(x)d\mu_2(y)}_{= f_X}\big)dP_X(x) \\\\
\end{align*}

\vspace{-1cm} Soit 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \d :&  E\times \cal F &\mapsto& \R\\
  & (x,B) & \mapsto &\begin{cases} 
\int_B  \frac{f_{(X,Y)}(x,y)}{f_X(x)}d\mu_2(y), & \text{si $x \in \widetilde A$}\\[6pt]
P_Y(B), & \text{sinon}
\end{cases} \end{array}
$$

\petitespace

$\d$ est alors un noyau de transition de $\EE$ dans $\FF$ puisque \petitespace

\shift $\bullet \quad \forall B \in \cal F, \quad x \mapsto \d(x,B)$ est $\cal E-$mesurable  \petitespace

\shift $\bullet \quad \forall x \in E, \quad B \mapsto \d(x,B)$ est une mesure de probabilité \petitespace

En \textbf{conclusion}, $\forall A \in \cal E, B \in \cal F$, 

$$P(\{X \in A\} \cap \{ Y \in B\})  = \int_A \d(x,B) dP_X(x)$$\petitespace

Donc $\d(x,.)$ est une loi conditionnelle de $Y$ sachant $X=x$, et si $\frac{d P_X}{d \mu_1}(x) \ne 0, \d(x,.)$ admet une densité donnée par 

$$P_{Y|X=x}(B) = \int_B \frac{dP_{(X,Y)}}{d \mu_1 \otimes \mu_2}(x,y)/ \frac{dP_X}{d\mu_1}(x)\mu_2(y) $$

\Rq 

Si $\frac{dP_X}{d\mu_1}(x) \ne 0$, alors, {\Large $$\forall y \in F, f_{Y|X=x}(y) = \frac{ \frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y)}{ \frac{dP_x}{d \mu_1}(x)} = C\frac{dP_{(X,Y)}}{d \mu_1\otimes \mu_2}(x,y) $$ }
}




\underline{exemple}\petitespace

Si $X$ et $Y$ sont deux variables aléatoires sur $\OmegaAP$ de loi jointe continue et de densité 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  f :&   \R^2 &\mapsto& \R\\
  & (x,y) & \mapsto &2e^{-(x+y)}\1_{0 \le x \le y }
\end{array}
$$\petitespace

$\forall x \ge 0, \quad \frac{dP_X}{d\lambda_1}(x) =\int_\R 2e^{-(x+y)}\1_{0 \le x \le y }dy >0$ et

 
\begin{align*}
	f_{Y|X=x}(y)&= C\frac{dP_{(X,Y)}}{d\lambda_1 \otimes \lambda_2}(x,y)\\
	&=ce^{-(x+y)}\1_{x \le y}\\
	&=(ce^{-x})e^{-y}\1_{x \le y}\\
	&=c'e^{-y}\1_{x \le y}
\end{align*}

Or $\int_\R f_{Y|X=x}(y)dy =1$ si et seulement si

$$ c'\int_x^\infty e^{-y} dy =1 $$

Si, et seulement si $c'=e^x$, si et seulement si $c=e^{2x}$. Ainsi, pour tout $x \ge 0,$

$$f_{Y|X=x}(y)=e^{-(y-x)}\1_{x \le y }(y)$$
\petitespace

\subsection{Cas Discret}\espace

\Prop 

Soient $E$ et $F$ des ensembles au plus dénombrables, munis de leurs tribus discrète, et $X$ et $Y$ deux variables aléatoires sur $\OmegaAP$ dans $\EE$ et $\FF$ telle que pour tout $x \in E,\quad P(X=x) \ne 0$. 

Alors, $\forall x \in E, \quad P_{Y|x=x}$ est une \textbf{loi discrète} sur $F$ donnée par :

$$P_{Y|x=x}(\{y\}) = P_{[X=x]}(\{Y=y\})$$

\petitespace

\corr{\textbf{Preuve :}

La densité conditionnelle de $Y$ sachant $X=x$ est donnée par 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  f_{Y|X=x} :&   F &\mapsto& \R\\
  & y & \mapsto & \frac{P(\{X=x\}\cap \{Y=y\})}{P(\{X=x\})} = P_{[X=x]}(Y=y)
\end{array}
$$
\petitespace}

\subsection{Caractérisation de l'Indépendance via les Lois Gaussiennes}\petitespace

\Prop 

Soient $X,Y$ deux variables aléatoires sur $\OmegaAP$ dans des espaces mesurables $\EE$ et $\FF$. Alors 

$$X\indep Y \iff  \forall x \in E, P_Y \text{ est une loi conditionnelle de $Y$ sachant $X=x$} $$

\petitespace

\corr{\textbf{Preuve :}

"$\implies$" : Supposons que $X \indep Y$, $\forall A \in \cal E, B \in \cal F,$

$$ P(\{X \in A\} \cap \{Y \in B\}) = P(\{X \in A\})\P(\{Y \in B\}) = \int_A P(\{Y \in B\})dP_X(x)$$

Donc

$$\begin{array}{rrrl}

  \d :&   E\times F &\mapsto& \R\\
  & (x,B) & \mapsto &P( Y \in B)
\end{array}
$$


est un noyau de transition qui donne une loi conditionnelle de $Y$ sachant $X$.\petitespace

$\forall x \in E, \quad \d(x,B) = P_Y(.)$ est une loi conditionnelle de $Y$sachant $\{X=x\}$.\petitespace

"$\Longleftarrow$" : $ \forall A \in \cal E, B \in \cal F$, 

\begin{align*}
	P(\{X \in A\} \cap \{Y \in B\}) &= P((X,Y) \in A\times B)\\
	&=\int_A P_Y(B)dP_X(x)\\
	&=\big(\int_A dP_X(x)\big)P_Y(B)\\
	&=P_X(A)P_Y(B)
\end{align*}

Ainsi, $X\indep Y$
\petitespace}

\Prop

S'il existe une loi $Q$ sur $\FF$ telle que $\forall x \in E$, $Q$ est une loi conditionnelle de $Y$ sachant $X=x$, alors $Q=P_Y$, et $X \indep Y$.

\petitespace

\corr{\textbf{Preuve :}

$\forall  B \in \cal F$, 

\begin{align*}
	P_Y(B) &= P(\{X \in E\} \cap \{ Y \in B\})\\
	&=\int_E Q(B) dP_X(x)\\
	&=Q(B)
\end{align*}
\petitespace}

\subsection{Théorème de Fubini Généralisé}\espace

\thm 

Soient $\EE$, $\FF$ deux espaces mesurables, et $P_1$ une probabilité sur $\EE$. Soit $\d$ un \textbf{noyau de transition de $\EE$ vers $\FF$}.\petitespace

Alors notons $P_1.\d$ la loi alors définie sur $(E \times F, \cal E \otimes F)$ : \petitespace

\shift i) $\forall f : E \times F \mapsto \R_+$ mesurable, alors 

$$\int_{E \times F}f(x,y)d(P_1.\d)(x,y) = \int_E\big(\underbrace{ \int_F f(x,y)\d(x,dy)}_{\P_{Y|X=x}(y)}\big)dP_1(x)$$

 \petitespace

\shift ii) Soit $f : E\times F \mapsto \R$ mesurable, alors $f \in L^1(E\times F, P_1.\d)$ si, et seulement si 

$$\begin{cases}
\forall P_1,\quad \forall x \in E,\quad f(x,.)\in L^1(F, \d(x,.)) \\
E \mapsto \R \\
x \mapsto \int_F f(x,y) \d(x, dy) \in L^1(E, P_1)

\end{cases} $$

\shift \shift et i) est alors vérifié \petitespace


\subsection{Liens avec l'Espérance Conditionnelle}\petitespace

\subsubsection{Théorème de Transfert Conditionnel}\espace



\thm
\textbf{Théorème de transfert conditionnel}\petitespace

Soient $X$ et $Y$ deux variables aléatoires sur des espaces mesurables $\EE$ et $\FF$, et soit $h: E \times F \mapsto \R$ une fonction mesurable telle que : $h \ge 0 $ ou $h(X,Y) \in L^1(P)$ \ie : $h \in L^1(P(X,Y))$

Soit 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \varphi :&   E  &\mapsto& \R\\
  & x & \mapsto &\int_F h(x,y)dP_{Y|X=x}(y)
\end{array}
$$

$\espcond{h(X,Y)}{X} = \varphi(X)$ 

\petitespace

\corr{\textbf{Preuve :}

Cas où $h \ge 0$. Soit $B \in \sigma(X)$. Montrons que $\esp{h(X,Y)\1_B}= \esp{\varphi(X)\1_B}$ 

$B$ s'écrit $B=X^{-1}(A), A \in \cal E$

\begin{align*}
	\esp{h(X,Y)\1_B} &=\esp{h(X,Y)\1_{\{ X \in A\}}}\\
	&= \int_{E\times F}h(x,y)\1_A(x) dP_{(X,Y)}(x,y)  \text{ (théorème de transfert)}\\
	&=\int_E\big(\int_Fh(x,y)\1_A(x)dP_{Y|X=x}(y)\big)dP_X(x) \text{ (Fubini généralisé cas positif)}\\
	&=\int_E\1_A(x)\big(\int_F \underbrace{h(x,y)dP_{Y|X=x}(y)}_{\varphi(x)}\big)dP_X(x) \\
	&=\int_E \1_A(x)\varphi(x) dP_X(x)\\
	&=\esp{\varphi(X)\1_{\{X \in A\}}} \text{ (théorème de transfert)}\\
	&=\esp{\varphi(X)\1_{B}}
\end{align*}

$\varphi(X)$ étant bien $\sigma(X)-$ mesurable, on a bien $\espcond{h(X,Y)}{X} = \varphi(X)$
\petitespace}

\underline{Exemple} :\petitespace

1) Soit $X$ une variable aléatoire réelle, et $\epsilon \sim \normale{0}{1}$. $Y=X+\epsilon$ tel que $X \indep \epsilon $ qu'en est-il de $\espcond{Y^2}{\sigma(X)}$ ?\petitespace

D'après le théorème de transfert conditionnel, $\espcond{Y^2}{\sigma(X)} = \varphi(X)$
avec 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \varphi :&   \R  &\mapsto& \R\\
  & x & \mapsto &\int_\R (x+x)^2 dP_{\epsilon|X=x}(y)
\end{array}
$$

or, $P_{\epsilon|X=x}= P_\epsilon$ car $X \indep \epsilon$\petitespace


2) Si $X$ est une variable aléatoire dans $\N$, intégrable, $\epsilon \sim \normale{0}{1}$ tel que $X \indep \epsilon$. 

Soit $Y=X+\epsilon$, alors, $\forall n \in \N, \quad P_{Y|X=x}=\normale{x}{1}$, donc, d'après le théorème de transfert conditionnel, $\espcond{Y}{X}=\varphi(X)$ où, $\forall x \in  \N,$

\begin{align*}
	\varphi(x)&=\int_\R y dP_{Y|X=x}(y)\\
	&=\int_R y \frac{1}{\sqrt{2\pi}}e^{\frac{-1}{2}\frac{(y-x)^2}{2}}dy\\
	&=x
\end{align*}

Ainsi, $\espcond{Y}{X}=X$

Autre manière : $\espcond{Y}{X} = \espcond{X}{X} + \espcond{\epsilon}{X} = X+0$ par indépendance de $X$ et $\epsilon$

$\forall x \in \R,$

\begin{align*}
	 \varphi(x) &= \int_\R (x+y)^2\frac{1}{\sqrt{2\pi}}e^{\frac{-1}{2}y^2}dy\\
	 &=x^2+2x\esp{\epsilon}+\esp{\epsilon^2}\\
	 &=x^2+1
\end{align*}

Ainsi, $\espcond{Y^2}{\sigma(X)} = X^2+1$
\espace

\subsubsection{Outils pour le Calcul des Lois Conditionnelles}

\espace

Dans la suite, de cette partie, $\EE, \FF, (G, \cal G)$ sont des espaces mesurables, $X$ et $Y$ sont des variables aléatoires sur $\OmegaAP$ dans $\EE$ et $\FF$.

On notera, si $Q$ est une mesure de probabilité sur $\EE$, et $g : E \mapsto F$ est mesurable, $g \# Q$ la probabilité sur $\FF$ donnée par : $\forall B \in \cal F, (g\# Q)(B) = Q(g^{-1}(B))$ \textbf{la mesure image} de $Q$ par $g$

donc $g\# P_X = P_{g(X)}...$\petitespace

\Prop

Si $g : F \mapsto G$ est mesurable, alors : $\forall x \in E, P_{g(y)|X=x} =g\#P_{Y|X=x}$ \ie : si $Z \sim P_{Y|X=x}$, alors $g(Z) \sim P_{g(Y)|X=x}$

\petitespace

\corr{\textbf{Preuve :}

$\forall A \in \cal E, \forall C \in \cal G,$

$$P(\{X \in A\}\cap \{g(Y) \in C\}) = P( \{X \in A\}\cap \{ Y \in g^{-1}(C)\}) = \int_A \underbrace{P_{Y|X=x}(g^{-1}(C))}_{= g\# P_{Y|X=x}(C)}dP_X(x)$$

$\forall x \in E, g\#P_{Y|X=x}$ est la loi conditionnelle de $g(Y)$ sachant $X=x$, on vérifie en effet que 

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  &  E\times G  &\mapsto& \R\\
  & (x,c) & \mapsto & (g\# P_{Y|X=x})(C)
\end{array}
$$

est un noyau de transition 
\petitespace}

\Prop 

Soit $h : E\times F \mapsto G$ mesurable. Alors

$$ \forall x \in E, \quad P_{h(X,Y)|X=x}=P_{h(x,Y)|X=x}$$\petitespace


\underline{Exemple} : \petitespace

Si on prend $Y=X+\epsilon$, alors, pour $x \in \R$, $P_{Y|X=x} = P_{x+\epsilon|X=x}=\normale{x}{1}$
\petitespace

\petitespace


\newpage

\section{Inégalités}

On considèrera dans cette section (sauf indications contraires) que toutes les variables aléatoires introduites sont réelles et sur un espace probabilisé $\OmegaAP$.

\petitespace


 \textbf{Inégalité de Markov }\petitespace

Soit $X$ une variable aléatoire positive admettant un moment d'ordre 1, alors :

$\forall \epsilon>0$  $$P(X>\epsilon) \le \frac{\esp{X}}{\epsilon}$$

\corr{ \textbf{Preuve : }

Soit $t \in \R_+^*$, on a 
\begin{align*}
	X \ge t\1_{X \ge t} &\implies \esp{X} \ge t\esp{\1_{X\ge t}}\\
	&\iff \esp{X} \ge tP(X\ge t)\\
	&\iff P(X \ge t) \le \frac{\esp{X}}{t}
\end{align*}

Et ainsi on a bien $\bb{P}(X>\epsilon) \le \frac{\esp{X}}{\epsilon}, \forall \epsilon >0$\petitespace
}



\textbf{Inégalité de Paley-Zygmund}\petitespace

Soit $X$ une variable aléatoire telle que $X \overset{p.s.}{\ge}0$, et $X \in L^2(P)$. Alors, pour tout $\theta \in [0,1]$ on a 

$$P(X \ge \theta \esp X) > (1-\theta)^2\frac{\esp{X}^2}{\esp{X^2}} $$

\corr{ \textbf{Preuve :}
\begin{align*}
	\esp X &= \esp{\overbrace{X\1_{X \le \esp X\theta}}^{ \le \esp X\theta}} + \overbrace{\esp{X\1_{X > \esp X\theta}}}^{\overset{(C.S.)}{\le} \esp{X^2}^{1/2}P(X > \esp X\theta)^{1/2}} \\
\end{align*}

Donc 

\begin{align*}
	& \esp X \le \esp X\theta + \esp{X^2}^{1/2}P(X > \esp X\theta)^{1/2}\\
	& \iff  P(X > \theta \esp X) \ge (1-\theta)^2\frac{\esp{X}^2}{\esp{X^2}} 
\end{align*}


}
\petitespace


\textbf{Inégalité de Hölder}\petitespace

Soit $X \in L^p( P)$ et $ Y\in L^q( P)$, avec $p,q \ge 1,  p,q \in \R$ deux variables aléatoires, tel que $\frac1p + \frac1q = 1$
 Alors on a 
 
 $$\esp{\lvert XY\rvert} \le (\esp{\lvert X \rvert^p})^{\frac 1p}(\esp{\lvert Y\rvert^q})^{\frac 1q}$$
 
 \corr{\textbf{Preuve :}
 
 On a $p,q > 1$ car $\frac 1p + \frac 1q = 1$. (si on avait $p \le 1 \implies \frac 1p \ge 1, \forall q >0, \frac 1p + \frac 1q>1 \ne 1$). En rappelant que pour tout réels $a,b$ positifs, et pour tout $\lambda>0$, on a, par concavité du $\log$ :
 
 \begin{align*}
	\log(\frac 1p (\lambda a)^p +\frac 1q (\frac b\lambda)^q) &=  \log(\frac 1p (\lambda a)^p +(1-\frac 1p) (\frac b\lambda)^q)\\
	&\ge \frac 1p \log((\lambda a)^p) + \frac 1q\log((\frac b \lambda)^q) = ln(ab)
\end{align*}

Par croissance de l'exponentielle, on a donc 

$$ab \le \frac 1p (\lambda a)^p +\frac 1q (\frac b\lambda)^q $$

 Donc $|XY|$ positif, ainsi $|XY|\le \frac 1p(\lambda |X|)^p + \frac 1q (\frac{|Y|}{\lambda})^q , \forall \lambda >0$ donc \petitespace
 
 $\esp{|XY|} \le \frac{\lambda^p}{p}\esp{|X|^p} + \frac{1}{\lambda^qq}\esp{|Y|^q}$ or, $X \in  L^p( P)$ et $Y \in  L^q( P)$ donc $XY \in  L^1( P)$\petitespace
 
 Minimisons la fonction $f(\lambda) = \frac{\lambda^p\esp{|X|^p}}{p} + \frac{\esp{|Y|^q}}{\lambda^qq}$
 
$$ f'(\lambda) = 0 \iff 0 = \lambda^{p-1}\esp{|X|^p} + -\lambda^{-q-1}\esp{|Y|^q}$$
 
 $$f'(\lambda) = 0 \iff \lambda = \big(\frac{\esp{|Y|^q}}{\esp{|X|^p}}\big)^{\frac{1}{p+q}} $$
 
 Et donc
 
 \begin{align*}
 	\esp{|XY|}&\le \frac 1p\big(\frac{\esp{|Y|^q}^{\frac{1}{p+q}}}{\esp{|X|^p}^{\frac{1}{p+q}}}\big)^p\esp{|X|^p} + \frac 1q\big(\frac{\esp{|X|^p}^{\frac{1}{p+q}}}{\esp{|Y|^q}^{\frac{1}{p+q}}}\big)^q\esp{|Y|^q}\\
	&\le \frac 1p \big(\frac{\esp{|Y|^q}^{\frac{p}{p+q}}}{\esp{|X|^p}^{\frac{p}{p+q}-1}}\big) +  \frac 1q\big(\frac{\esp{|X|^p}^{\frac{q}{p+q}}}{\esp{|Y|^q}^{\frac{q}{p+q}-1}}\big)\\
	&\le \frac 1p \big(\frac{\esp{|X|^p}^{1-\frac{p}{p+q}}}{\esp{|Y|^q}^{\frac{-p}{p+q}}}\big) +  \frac 1q\big(\frac{\esp{|X|^p}^{\frac{q}{p+q}}}{\esp{|Y|^q}^{\frac{-p}{p+q}}}\big)\\
	&\le \esp{|X|^p}^{1-\frac{p}{p+q}}\esp{|Y|^q}^{\frac{p}{p+q}}\\
	&\le \esp{|X|^p}^{\frac1p}\esp{|Y|^q}^{\frac1q} \text{ car $\frac 1p+\frac 1q = 1\iff p+q=pq$}
 \end{align*}
 
 }\petitespace

\textbf{Inégalité de Minkovski}\petitespace

Pour $X,Y \in L^p(P), p \ge 1$ quelconque, on a 

$$\esp{\lvert X+Y\rvert^p}^{\frac 1p} \le \esp{\lvert X\rvert^p}^{\frac 1p}+\esp{\lvert Y\rvert^p}^{\frac 1p}$$

\corr{\petitespace \textbf{Preuve :}

Si $X, Y, \in L^p(P)$, alors $X+Y \in L^p( P)$ car $\esp{|X+Y|^p} \overset{\text{I.T.}}{\le} \esp{|X|^p} + \esp{|Y|^p}<\infty$

On a aussi que $|X||X+Y|^{p-1}$ et $|Y||X+Y|^{p-1}\in L^1(P)$ car on a par Hölder :

$$\esp{|X||X+Y|^{p-1}}\le \esp{|X|^{p}}^{\frac{1}{p}}\esp{|X+Y|^{p}}^{\frac{p-1}{p}} < \infty$$ 

et 

$$\esp{|Y||X+Y|^{p-1}}\le \esp{|Y|^{p}}^{\frac{1}{p}}\esp{|X+Y|^{p}}^{\frac{p-1}{p}} < \infty$$

(car $\frac{1}{p}+\frac{p-1}{p}=1$)


et on peut ainsi dire que 


$$\esp{|X+Y|^p} =\esp{|X+Y||X+Y|^{p-1}} \overset{\text{I.T.}}{\le} \esp{|X||X+Y|^{p-1}} + \esp{|Y||X+Y|^{p-1}}$$


Donc 

$$\esp{|X+Y|^p}\le  \esp{|X||X+Y|^{p-1}} + \esp{|Y||X+Y|^{p-1}} $$

Par Hölder, avec $\frac 1p +\frac{p-1}{p} =1 $

on a 

\begin{align*}
	\esp{|X+Y|^p} &\le \esp{|X||X+Y|^{p-1}} + \esp{|Y||X+Y|^{p-1}}\\
	& \le \esp{|X|^p}^{\frac 1p}\esp{|X+Y|^{p}}^{\frac{p-1}{p}} + \esp{|Y|^p}^{\frac 1p}\esp{|X+Y|^{p}}^{\frac{p-1}{p}}\\
	&\le \esp{|X+Y|^{p}}^{\frac{p-1}{p}}( \esp{|X|^p}^{\frac 1p}+ \esp{|Y|^p}^{\frac 1p})\\
\end{align*}

Donc 

$$\esp{|X+Y|^p} \le  \esp{|X+Y|^{p}}^{\frac{p-1}{p}}( \esp{|X|^p}^{\frac 1p}+ \esp{|Y|^p}^{\frac 1p})$$

équivalent à 

$$\frac{\esp{|X+Y|^p}}{\esp{|X+Y|^{p}}^{\frac{p-1}{p}}} \le \esp{|X|^p}^{\frac 1p}+ \esp{|Y|^p}^{\frac 1p} $$

équivalent à 

$$\esp{|X+Y|^p}^{\frac 1p} \le \esp{|X|^p}^{\frac 1p}+ \esp{|Y|^p}^{\frac 1p} $$\petitespace

}




\textbf{Inégalité de Jensen}\petitespace

Soit $X\in  L^1( P)$ une variable aléatoire, et $f$ une fonction convexe tel que $f\circ X \in  L^1( P)$, alors


$$f(\esp{X}) \le \esp{f(X)}$$


\corr{\textbf{Preuve :}

$f$ est une fonction convexe, faisons la preuve dans le cas où $f$ est différentiable. $\forall x,y$, on a 

$$ f(x) \ge f(y) +\nabla f(y)^T(x-y)$$

donc pour $y = \esp{X}$, et $=X$, on a 

$$ f(X)\ge f(\esp{X})+\nabla f(\esp{X})(X-\esp{X})$$
ainsi, 

$$\esp{f(X)}\ge \esp{f(\esp{X})}+\nabla f(\esp{X})(\esp{X}-\esp{X})= \esp{f(\esp{X})}=f(\esp{X})$$

donc on a bien : 

$$\esp{f(X)} \ge \esp{f(X)}$$

\petitespace}

\Prop 

De même, pour les espérances conditionnelles, on a que si $X\in  L^1( P)$ une variable aléatoire, et $f$ une fonction convexe tel que $f\circ X \in  L^1( P)$, alors pour une tribu $\cal B \subset \cal A$ on a

$$\espcond{f(X)}{\cal B}\le f(\espcond{X}{\cal B}) $$

\petitespace

\corr{\textbf{Preuve :}

Identiquement, on fera la preuve dans le cas où  $f$ différentiable. On a : 

$$f(X) \ge f(\espcond{X}{\cal B}) + \nabla f(\espcond{X}{\cal B})(X-\espcond{X}{\cal B})$$

$$\iff \espcond{f(X)}{\cal B} \ge \espcond{f(\espcond{X}{\cal B})}{\cal B} + 0$$

$$\iff  \espcond{f(X)}{\cal B} \ge f(\espcond{X}{\cal B})$$
\petitespace}



\textbf{Inégalité de Kolmogorov}\petitespace

Soit $(X_n)_{n\in \N}$ des variables aléatoires indépendantes, dans $ L^2( P)$. 

Alors, $\forall x>0, \forall n \ge 1$, ($n$ peut valoir $+\infty$)

$$P( \Sup{N\in\ldbrack1,n\rdbrack}\{ \lvert \somme{j=1}{N}X_j\rvert\}>x)\le \frac{\somme{i=1}{n}\esp{X_i^2}}{x^2}$$

\espace

\subsection{Inégalités de Concentration}\espace

Dans la théorie des probabilités, les inégalités de concentration fournissent des bornes sur la probabilité qu'une variable aléatoire dévie d'une certaine valeur (généralement l'espérance de cette variable aléatoire).

\subsubsection{Variables Aléatoires bornées}

\espace

\textbf{Inégalité de Tchebychev } \petitespace

Soit X une variable aléatoire admettant un moment d'ordre deux.

Alors $\forall \epsilon >0$ $$P(\lvert X-\esp{X}\rvert > \epsilon) \le \frac{\bb{V}(X)}{\epsilon^2}$$

\petitespace

\corr{

On prend $Y=( X-\esp{X})^2$ Alors selon l'inégalité de Markov, comme Y est positive, on a :\petitespace

$\forall \epsilon^2$ ($\epsilon >0$) $P(Y>\epsilon^2) \le \frac{\esp{Y}}{\epsilon^2} \iff P(( X-\esp{X})^2>\epsilon^2) \le \frac{\esp{( X-\esp{X})^2}}{\epsilon^2}$\petitespace

donc $P(\lvert X-\esp{X}\rvert > \epsilon) \le \frac{\bb{V}(X)}{\epsilon^2}$ \petitespace
}
\espace

Pour démontrer les inégalités qui suivent, on utilisera la \textbf{Méthode de Chernoff}. L'idée est d’appliquer Markov à la variable aléatoire positive $e^{\lambda X}$, avec $\lambda \in \R$. Démontrons d'abord un lemme, avant d'utiliser la méthode. \espace




\textbf{Lemme d'Hoeffding}

Soit $Y$ une variable aléatoire réelle bornée, centrée, telle qu'il existe $a,b \in \R, a<b$ tel que $Y \in [a,b]$ p.s.. Alors, pour tout $t>0$, on a 

$$\esp{e^{tY}}\le e^{(a-b)^2\frac{t^2}{8}} $$

\petitespace

\corr{\textbf{Preuve :}

On a $x \mapsto e^{tx}$ qui est une fonction convexe, donc, come $Y \in [a,b]$ p.s., on a, presque surement :

\begin{align*}
	e^{tY} &\le \frac{b-Y}{b-a}e^{ta}+ (1-\frac{b-Y}{b-a})e^{tb}\\
	&\le \frac{b-Y}{b-a}e^{ta}+\frac{a-Y}{b-a}e^{tb}
\end{align*}

Et donc

$$\esp{e^{tY}} \le \frac{b}{b-a}e^{ta} - \frac{a}{b-a}e^{tb}$$

On pose pour $u=t(b-a)$

\begin{align*}
	\psi(u)&= \log(\frac{b}{b-a}e^{ta} - \frac{a}{b-a}e^{tb})\\
	&=\log(\frac{b}{b-a}e^{\frac{ua}{b-a}} - \frac{a}{b-a}e^{\frac{ub}{b-a}})\\
	&= \log(e^{\frac{ua}{b-a}}( \frac{b}{b-a}-\frac{a}{b-a}e^{\frac{u(b-a)}{b-a}} ))\\
	&=\frac{a}{b-a}u + \log(1+\frac{a}{b-a}(1-e^u))\\
\end{align*}

on a $\psi(0) = \psi'(0) = 0$ et 

$$\psi''(u) = (\frac{a}{b-a} - \frac{ae^u}{b-ae^u})' = \frac{-bae^u}{(b-ae^u)^2} \overbrace{\le}^{\text{car de la forme $\frac{\alpha\beta}{(\alpha+\beta)^2}$}} \frac 14$$

En effet, par inégalité arythmético-géométrique,  on a, pour tout $x,y \in \R$ de même signe (si ils ne sont pas de même signes, alors on a directement $\frac{xy}{(x+y)^2} \le 0 \le  \frac 14$ ): 

$$\frac{|x|+|y|}{2} \le \sqrt{|xy|} \iff \frac{(|x|+|y|)^2}{4} \le |xy| \overbrace{\iff}^{|x+y|\le |x|+|y|}   \frac{(x+y)^2}{4} \le xy \iff \frac{xy}{(x+y)^2} \le \frac 14 $$

Et donc, par Taylor-Lagrange, on a qu'il existe un réel $s \in [0,u]$ tel que 

$$\psi(u) = \psi(0) + u\psi'(0) + \frac12 u^2 \psi''(s) \le \frac{(a-b)^2}{8}t^2 $$

Donc on a 

$$\esp{e^{tY}} \le e^{\frac{(a-b)^2}{8}t^2} $$


\espace}



\textbf{Inégalité de Hoeffding} \petitespace

Soit $\Xunan\in L^1( P)$, des variables aléatoires réelles indépendantes, et telles que $\forall i \in \ldbrack 1,n\rdbrack, a_i \le X_i \le b_i$ p.s. Alors $\forall t > 0$,

$$P(\lvert \somme{i=1}{n} X_i-\esp{X_i}\rvert \ge t) \le 2\text{exp}\big(\frac{-2t^2}{\somme{i=1}{n}(b_i-a_i)^2}\big)$$

et ainsi 

$$ P(\somme{i=1}{n}X_i-\esp{X_i}\ge t)\le\text{exp}\big(\frac{-2t^2}{\somme{i=1}{n}(b_i-a_i)^2}\big) $$\petitespace

\corr{\textbf{Preuve :}\petitespace

On pose $Y_i = X_i - \esp{X_i}$, les $Y_i$ sont donc centrées, et sont entre $a_i-\esp{X_i}$ et $b_i-\esp{X_i}$ presque surement. 

On pose  $S_n = \somme{i=1}{n}Y_i$, on a pour tout $x \ge 0, u>0$, par Markov :

\begin{align*}
	P(S_n-\esp{S_n} \ge x)&=P(e^{u(S_n-\esp{S_n})} \ge e^{ux})\\
	&\le \frac{\esp{e^{u(S_n-\esp{S_n})}}}{e^{ux}} \text{ (Markov)}\\
	&\le e^{-ux} \produit{i=1}{n}\esp{e^{uY_i}}
\end{align*}

On obtient par le lemme précédent, car $b_i-a_i = b_i-\esp{X_i}-(a_i-\esp{X_i})$

$$P(S_n-\esp{S_n} \ge x) \le e^{-ux + \frac{u^2}{8} \somme{i=1}{n}(b_i-a_i)^2} $$

Sachant que c'est pour tout $u >0$, on optimise et on trouve $u = \frac{4x}{ \somme{i=1}{n}(b_i-a_i)^2}$

Ce qui montre que 

\begin{align*}
	P(S_n-\esp{S_n} \ge x)&=P(e^{u(S_n-\esp{S_n})}) \le e^{- \frac{4x}{ \somme{i=1}{n}(b_i-a_i)^2}x + \frac{2x^2}{\somme{i=1}{n}(b_i-a_i)^2}}\\
	& = e^{-\frac{2x^2}{\somme{i=1}{n}(b_i-a_i)^2}}
\end{align*}

En remplaçant $X_i$ par $-X_i$ on obtient aussi 

$$ P(S_n-\esp{S_n} \le -x) \le e^{-\frac{2x^2}{\somme{i=1}{n}(b_i-a_i)^2}}$$

et donc $\forall x>0$, on a 

$$P(\lvert \somme{i=1}{n} X_i-\esp{X_i}\rvert \ge x) \le 2\text{exp}(-\frac{2x^2}{\somme{i=1}{n}(b_i-a_i)^2})$$

\espace}

\textbf{Inégalité de Bennett}\petitespace

Soit $\Xunan$ des variables aléatoires réelles centrées indépendantes, tel que pour tout $i \in \ldbrack 1,n\rdbrack$ il existe $c\in \R_+^*$ tel que $X_i \overset{p.s.}{\le} c$. 

En posant $S_n = \somme{i=1}{n} X_i$ et $V_n = \somme{i=1}{n} \esp{X_i^2} =  \somme{i=1}{n} \Var{X_i} $ on a alors, pour tout $\lambda \ge 0$ :

$$\log(\esp{e^{\lambda S_n}} )  \le \frac{V_n}{c^2}\phi(c\lambda), \quad \quad \phi(u) = e^{u}-u-1 $$

Et donc, pour tout $t \ge 0$ on a :

$$P(S_n \ge t ) \le e^{-\frac{V_n}{c^2}h(\frac{ct}{V_n})}$$

Avec $h(x) = (1+x)\log(1+x)-x$\petitespace

Également, si on a en également, $|X_i|  \overset{p.s.}{\le} c$, alors on en déduit alors :

$$P(| S_n | \ge t ) \le 2e^{-\frac{V_n}{c^2}h(\frac{ct}{V_n})} $$




\petitespace

\corr{\textbf{Preuve :} 

Supposons d'abord que $c = 1$. 

Soit $\lambda \ge 0$, on peut déjà remarquer que, comme $\phi : \R_+ \mapsto \R$ croissante et que $X_i \overset{p.s.}{\le}1$ 

$$e^{\lambda X_i} = 1+\lambda X_i + \phi(\lambda X_i)\le  1+\lambda X_i + X_i^2\phi(\lambda) \quad (\phi(\lambda X_i) \le \phi(\lambda) \overset{X_i \le 1}{\le} X_i^2 \phi(\lambda) ) $$

Donc 

\begin{align*}
	\log(\esp{e^{\lambda S_n}}) &= \somme{i=1}{n} \log(\esp{e^{\lambda X_i}}) \\
	& =  \somme{i=1}{n} \log(\esp{e^{\lambda X_i}}) \\
	&\le  \somme{i=1}{n} \log( \esp{1 + \lambda X_i + X_i^2\phi(\lambda) }) \\
	\text{(car $\log(1+x) \le x$)}& \le  \somme{i=1}{n} \esp{ \lambda X_i + X_i^2\phi(\lambda)}  \\
	& \le  \somme{i=1}{n} \esp{X_i^2}\phi(\lambda) = V_n \phi(\lambda)
\end{align*}

Pour $X_i \overset{p.s.}{\le} c$ en général, il suffit d'observer que $\frac{X_i}{c} \overset{p.s.}{\le} 1$, on a directement 

$$e^{\lambda X_i} = 1+\lambda X_i + \overbrace{\phi(\lambda X_i)}^{=\phi(\lambda c(\frac{ X_i}{c}))}\le  1+\lambda X_i + \big(\frac{X_i}{c}\big)^2 \phi(\lambda c)$$

et donc que 

$$\log(\esp{e^{\lambda S_n}}) \le  \somme{i=1}{n} \esp{\big(\frac{X_i}{c}\big)^2}\phi(\lambda) = \frac{V_n}{c^2}\phi( \lambda c)$$

\petitespace

Maintenant, montrons la deuxième inégalité. Soit $\lambda \ge 0, t \ge 0$ alors :

\begin{align*}
	P(S_n \ge t)&= P(e^{\lambda S_n} \ge e^{\lambda t}) \\
	\text{(Markov)}& \le \frac{\esp{e^{\lambda S_n}}}{e^{\lambda t}}\\
	\log\big(\frac{\esp{e^{\lambda S_n}}}{e^{\lambda t}}\big)& \le \frac{V_n}{c^2}\phi( \lambda c) - \lambda t
\end{align*}

(Minimiser $\frac{\esp{e^{\lambda S_n}}}{e^{\lambda t}}$ revient à minimiser $\log\big(\frac{\esp{e^{\lambda S_n}}}{e^{\lambda t}}\big)$ par croissance du $\log$).

Minimisons donc la fonction

 $$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \psi :& \R_+ &\mapsto& \R\\
  & \lambda & \mapsto &  \frac{V_n}{c^2}(e^{\lambda c} -\lambda c-1) - \lambda t \end{array}
$$

or $\psi''(\lambda) = V_ne^{\lambda c} > 0$ donc $\psi$ est une fonction convexe. De plus, 

\begin{align*}
	\psi'(\lambda) &= 0\\
	\iff -t + \frac{V_n}{c^2}(ce^{\lambda c}-c)&=0\\
	\iff \lambda = \frac 1c\log(1+\frac{tc}{V_n})
\end{align*}

Donc, on a, en évaluant en ce $\lambda$ : 

\begin{align*}
	 \frac{V_n}{c^2}\phi( \lambda c) - \lambda t &=   \frac{V_n}{c^2}(e^{\lambda c} -\lambda c-1) - \lambda t \\
	& =  \frac{V_n}{c^2}(e^{ \frac 1c\log(1+\frac{tc}{V_n})c} - \frac 1c\log(1+\frac{tc}{V_n}) c-1) -  \frac 1c\log(1+\frac{tc}{V_n}) t\\
	&=  \frac{V_n}{c^2}( 1+\frac{tc}{V_n} - \log(1+\frac{tc}{V_n}) -1 ) + \frac tc\log(1+\frac{tc}{V_n})\\
	& = \frac{1}{c^2}\big[  \log(1+\frac{tc}{V_n})(-V_n + tc) +tc  \big] \\
	& = \frac{-V_n}{c^2}\big[ \log(1+\frac{tc}{V_n})(1 - \frac{tc}{V_n}) -\frac{tc}{V_n}\big] \\
	& =  \frac{-V_n}{c^2} h(\frac{tc}{V_n}) \quad \text{avec $h(x) = \log(1+x)(1+x)-x$}
\end{align*}

On a donc bien que 

$$P(S_n \ge t) \le \frac{\esp{e^{\lambda S_n}}}{e^{\lambda t}} \le e^{\frac{-V_n}{c^2} h(\frac{tc}{V_n})}$$

On peut l'appliquer aussi à $-X_i$ si $|X_i| \overset{p.s.}{\le} c$, alors on a dans ce cas :

$$P(-S_n \ge t) \le e^{\frac{-V_n}{c^2} h(\frac{tc}{V_n})}$$

et donc 

$$ P( |S_n| \ge t) = P(S_n \ge t \cup -S_n \ge t) = P(S_n \ge t) + P(-S_n \ge t) \le 2e^{\frac{-V_n}{c^2} h(\frac{tc}{V_n})} $$




\petitespace}

\textbf{Inégalité de Bernstein } \petitespace

\cercler 1 Soit $\Xunan\in  L^\infty(P)$, des variables aléatoires réelles indépendantes, et $S_n = \somme{i=1}{n} X_i$. Si il existe $c_n, v_n \in \R$ tel que $\somme{i=1}{n}\esp{X_i^2} \le v_n$ et, en posant $(X_i)_+ = \max(0, X_i)$, on vérifie  :

$$\forall k \ge 3, \quad \somme{i=1}{n}\esp{(X_i)_+^k} \le \frac{v_n k! c_n^{k-2}}{2}$$

Alors, pour tout $\lambda \in [0, \frac{1}{c_n})$ on a 

$$ \log\big(\esp{e^{\lambda(S_n-\esp{S_n})}}\big) \le \frac{v_n \lambda^2}{2(1-c_n\lambda)} $$

Ce qui implique que pour tout $t \ge 0$ on a :

$$P(S_n-\esp{S_n} \ge t) \le e^{\frac{-t^2}{2(v_n+tc_n)}} $$

\cercler 2 Si $\Xunan$ sont des variables aléatoires réelles indépendantes centrées, avec $V_n = \somme{i=1}{n}\esp{X_i^2}$, et telles qu'il existe $M >0$ tel que $\forall i \in \ldbrack 1, n \rdbrack, |X_i| \overset{p.s.}{\le}M$ alors, pour tout $t > 0$ on a : 

$$P( \somme{i=1}{n}X_i  \ge t)\le \text{exp}\big(\frac{-t^2}{2V_n + \frac{2Mt}{3}}\big)$$\petitespace


Et donc 

$$ P(| \somme{i=1}{n}X_i|  \ge t)\le 2\text{exp}\big(\frac{-t^2}{2V_n + \frac{2Mt}{3}}\big)$$


\corr{\textbf{Preuve :}

\cercler 1 Soit

$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \phi :& \R &\mapsto& \R\\
  & x & \mapsto & e^x-x-1 \end{array}
$$

On a alors $\forall x \le 0, \phi(x) \le \frac{x^2}{2}$ car $\phi(x) = \somme{k=2}{+\infty} \frac{x^k}{k!}$ et donc $\phi(x) -\frac{x^2}{2} = \somme{k=3}{+\infty} \frac{x^k}{k!}$ de terme dominant $x^3 \le 0$, quand $x \le 0$.

On rappelle que l'on note $X_+ = \max(0, X)$ et $X_- = \max(0, -X), \forall X \in \R$ on a donc, pour tout $\lambda \ge 0$ :

\begin{align*}
	\phi(\lambda X_i) &= \phi(\lambda(X_i)_-) + \phi(\lambda(X_i)_+)\\
	& \le \frac{\lambda^2 (X_i)_-^2}{2} + \phi(\lambda(X_i)_+)  \\
	&=  \frac{\lambda^2 (X_i)_-^2}{2} + \somme{k=2}{+ \infty} \frac{(\lambda (X_i)_+)^k}{k!}\\
	&= \frac{\lambda^2 (X_i)^2}{2} + \somme{k=3}{+ \infty} \frac{(\lambda (X_i)_+)^k}{k!} \quad \quad \text{(*)}\\
\end{align*}

On a donc 

\begin{align*}
	\log\big(\esp{e^{\lambda(S_n-\esp{S_n})}}\big)&=\somme{i=1}{n} \log\big(\esp{e^{\lambda(X_i-\esp{X_i})}}\big) \\
	&= \somme{i=1}{n} \log\big(\esp{e^{\lambda X_i}}\big) -\lambda \esp{X_i} \\
	& =  \somme{i=1}{n} \log\big(1+ \esp{\lambda X_i + \phi(\lambda X_i)}\big) -\lambda \esp{X_i} \\
	(\log(1+x) \le x) & \le \somme{i=1}{n} \esp{\lambda X_i + \phi(\lambda X_i)} -\lambda \esp{X_i} \\
	\text{par (*) }&\le \somme{i=1}{n} \esp{ \frac{\lambda^2 (X_i)^2}{2} } +  \somme{k=3}{+ \infty} \frac{\esp{(\lambda (X_i)_+)^k}}{k!}\\ 
	& \le  \frac{\lambda^2}{2}v_n +  \somme{k=3}{+ \infty} \frac{\lambda^k}{k!}\somme{i=1}{n}\esp{(X_i)_+^k} \\
	& \le  \frac{\lambda^2v_n}{2} +  \somme{k\ge 3}{}\frac{\lambda^kv_nc_n^{k-2}}{2} \\
	& \le \frac{\lambda^2v_n}{2}\somme{k \ge 0}{}(\lambda c_n)^k \\
	\big(\text{pour $\lambda \in [0, \frac1c_n)$\big) }&\le \frac{v_n \lambda^2}{2(1-c_n\lambda)}
\end{align*}

Ainsi, $\forall \lambda \in [0, \frac1c_n)$

$$\log\big(\esp{e^{\lambda(S_n-\esp{S_n})}}\big) \le \frac{v_n \lambda^2}{2(1-c_n\lambda)}$$

Ainsi, par cette inégalité, on a pour tout $t \ge 0$ :


\begin{align*}
	 P( S_n-\esp{S_n} \ge t) &=  P( e^{\lambda(S_n-\esp{S_n})} \ge e^{\lambda t}) \\
	& \le \esp{e^{\lambda(S_n-\esp{S_n})} } e^{-\lambda t} \\
	\log\big(  P( S_n-\esp{S_n} \ge t)  \big)&\le  \frac{v_n \lambda^2}{2(1-c_n\lambda)} -\lambda t \\
\end{align*}

or $\lambda \mapsto \frac{v_n \lambda^2}{2(1-c_n\lambda)} -\lambda t$ est convexe sur l'ensemble (convexe) $[0, \frac 1c)$.

on a en effet 

\begin{align*}
	 (\frac{v_n \lambda^2}{2(1-c_n\lambda)} -\lambda t)'' &= (\frac{\lambda v_n(2-\lambda c_n)}{2(1-\lambda c_n)})'\\
	 &=v_n \frac{-c_n^2\lambda^2+\lambda c_n+1}{(\underbrace{1-\lambda c_n}_{\ge 0, \lambda \in [0, \frac 1c_n)})^3}\\
\end{align*}

Or, le polynôme $\lambda \mapsto -c_n^2\lambda^2+\lambda c_n+1$ est toujours positif sur $[0, \frac 1c_n)$. On résout donc 
\begin{align*}
	 (\frac{v_n \lambda^2}{2(1-c_n\lambda)} -\lambda t)' &= 0\\
\end{align*}

et on obtient le résultat $P(S_n-\esp{S_n} \ge t) \le e^{-\frac{t^2}{2(v_n+tc_n)}}$ \petitespace


\cercler 2 Pour la deuxième assertion, pour prouver que $P(| \somme{i=1}{n}X_i|  \ge t)\le 2\text{exp}\big(\frac{-t^2}{2V_n + \frac{2Mt}{3}}\big)$, il nous suffit de remarquer qu'on peut appliquer l'inégalité de Bennett à $\Xunan$, en effet, on a bien qu'elles sont centrées et que $|X_i| \overset{p.s.}{\le} M$, donc on a 

$$ P(S_n \ge t) \le e^{\frac{-V_n}{M^2} h(\frac{tM}{V_n})}$$

On a donc toujours $\frac{tM}{V_n}\ge 0$, montrons que pour tout $x \ge 0$, on a $h(x) \ge \frac{x^2}{2+\frac{2}{3}x}$. 

D'abord, on remarque que $h(0) = 0 \ge \frac{0^2}{2+\frac{2}{3}0} = 0$, et, pour $x \ge 0$ on a 

\begin{align*}
	(h(x)- \frac{x^2}{2+\frac{2}{3}x})'&= \log(1+x) - \frac{x}{(2+\frac 23x)^2}  = \frac{\log(1+x)(2+\frac 23x)^2-x}{(2+\frac 23x)^2} \ge 0 \\
\end{align*}

Donc on a bien que pour tout $x \ge 0, h(x) \ge \frac{x^2}{2+\frac{2}{3}x}$, donc par décroissance de $e^{-x}$ on a : 

$$ P(S_n \ge t) \le e^{\frac{-V_n}{M^2} h(\frac{tM}{V_n})} \le e^{ \frac{-V_n}{M^2}\frac{(\frac{tM}{V_n})^2}{2+\frac{2}{3}\frac{tM}{V_n}} } = \text{exp}\big(\frac{-t^2}{2V_n + \frac{2Mt}{3}}\big)$$

Et ainsi, avec le même raisonnement que pour la preuve précédente (Bennett) on obtient :

$$ P( |S_n| \ge t) = P(S_n \ge t \cup -S_n \ge t) = P(S_n \ge t) + P(-S_n \ge t) \le 2 \text{exp}\big(\frac{-t^2}{2V_n + \frac{2Mt}{3}}\big) $$

%


\petitespace}

\textbf{Illustration :}

Montrons une application concrète des inégalités de concentrations précédentes (\ie : Chebytchev, Hoeffding, Bennett et Bernstein) à une suite de variables aléatoires réelles $(X_n)_{n \ge 1} \simiid \cal U([0,1])$. Précisons notre méthode pour Hoeffding, Bennett et Bernstein.

\cercler 1 Pour utiliser Hoeffding on applique l'inégalité à la variable aléatoire $Y_i = X_i/n$, avec donc presque sûrement $0 \le Y_i \le \frac 1n, \quad \forall i \in \ldbrack 1,n\rdbrack$. Pour $t \ge 0$ on a 

$$ P(|\somme{i=1}{n} Y_i-\esp{Y_i} |\ge t)=P(|\Xbarre -\esp{X_1}| \ge t ) \le 2 \exp\big(\frac{-2t^2}{n \frac{1}{n^2}}\big) = 2 e^{-2nt^2}$$

\cercler 2 Pour Bennett, on a pour tout $n \ge 1$, pour tout $i \in \ldbrack 1,n\rdbrack$, avec $Y_i = \frac{X_i}{n}-\frac{1}{2n}$, $(Y_i)_{1 \le i\le n}$ est donc centrée et $| Y_i| = |\frac{X_i-1/2}{n}| \overset{p.s.}{\le} \frac{1}{2n}=c$. Donc :

$$V_n = \somme{i=1}{n} \esp{Y_i^2} = \somme{i=1}{n} \Var{Y_i^2} =   \somme{i=1}{n} \frac{1}{n^2}n\Var{X_i-1/2} = \frac{1}{n^2}n\frac{1}{12} = \frac{1}{12n}$$

Ainsi, pour $t \ge 0$ on a :

$$P(|\somme{i=1}{n} Y_i-\esp{Y_i} |\ge t)=P(|\Xbarre -\esp{X_1}| \ge t ) \le  2e^{-\frac{V_n}{c^2}h(\frac{ct}{V_n})}  = 2e^{-\frac{\frac{1}{12n}}{\frac{1}{(2n)^2}}h(t\frac{1/2n}{1/12n})} = 2e^{\frac{-n}{3}h(6t)}$$

\cercler 3 Enfin, pour Bernstein, on applique l'inégalité à $Y_i = \frac{X_i}{n}-\frac{1}{2n}$ (centrée). Or, pour tout $n \ge 1$, pour tout $i \in \ldbrack 1,n\rdbrack$, on a donc $|Y_i| = \frac 1n \underbrace{|X_i-\frac 12|}_{\le 1/2} \le \frac{1}{2n} = M$. 


Et donc, on a $V_n = \somme{i=1}{n} \esp{Y_i^2} =  \somme{i=1}{n} \Var{Y_i} = \somme{i=1}{n}\frac{1}{n^2} \Var{X_i-1/2} = \frac{1}{n^2}\somme{i=1}{n} \Var{X_1} = \frac{1}{12n}$. Ainsi, pour $t \ge 0$

$$P(|\somme{i=1}{n} Y_i|)=P(|\Xbarre -\esp{X_1}| \ge t ) \le  2\text{exp}\big(\frac{-t^2}{2V_n + \frac{2Mt}{3}}\big) =  2\text{exp}\big(\frac{-t^2}{\frac{1}{6n} + \frac{t}{3n}}\big) = 2\exp\big( \frac{-6n t^2 }{1+2t}\big)$$



\includegraphics[width=1\textwidth]{/Users/gabriel/Desktop/LateX/images/fiches_stat/inegalité.png} 

On observe que la précision de toutes les inégalités finit par surpasser celle de Tchebychev, ce qui est toujours vrai pour toutes les variables aléatoires respectant les conditions spécifiques de Hoeffding, Bennett et Bernstein. En revanche, on peut noter que l'inégalité de Tchebychev est plus précise que les autres pour des valeurs "petites" de $n$, ce qui est également généralement vrai pour d'autres types de variables aléatoires. On constate ainsi que la qualité de la borne de Tchebychev diminue (par rapport aux autres) à mesure que le nombre d'itérations augmente. En effet, grâce à la décroissance exponentielle des inégalités de Hoeffding, Bennett et Bernstein, on remarque qu'au début, la borne n'est pas très précise, mais qu'avec le temps, la décroissance de la borne s'accélère, tandis que celle de Tchebychev ralentit.

En résumé, il n'existe pas a priori de "meilleure" inégalité de concentration ; on sait simplement que l'inégalité de Tchebychev est préférable pour un faible nombre d'itérations, tandis que celles de Hoeffding, Bennett et Bernstein sont à privilégier lorsque 
$n$ est élevé. Cependant, l'inégalité de Bennett est toujours plus précise celle de Bernstein, car Bernstein est une majoration simple de Bennett, qui reste cependant plus pratique à manipuler. Comme on peut le voir, la différence entre Bennett et Bernstein est souvent infime, ce qui fait que les statisticiens ont tendances à privilégier cette dernière. L'inégalité de Bernstein est elle-même très souvent plus précise que celle de Hoeffding, en effet l'avantage des inégalités de Bennett et de Bernstein réside dans leur utilisation de l'information sur la variance de la loi, tandis que Hoeffding, plus grossière, se contente d'exploiter les bornes des variables aléatoires. La plus précise des inégalités est donc celle de Bennett à moyen terme, même si Hoeffding et Bernstein sont très souvent suffisante, avec une très faible différence de borne entre Bennett et Bernstein. Par ailleurs, il est important de savoir qu'actuellement, sous les conditions données par les trois inégalités, on ne peut pas vraiment faire mieux que l'inégalité de \textbf{Bennett} en termes de bornes supérieure. 

On peut aussi voir une autre image, pour un $t$ plus important, on remarque que la différence entre Bennett et Bernstein se fait plus ressentir. \petitespace

\includegraphics[width=1\textwidth]{/Users/gabriel/Desktop/LateX/images/fiches_stat/inégalité2.png} 

\espace

\subsubsection{Le Cas des Variables Aléatoires Non Bornées}

\espace

Nous avons vu maintenant les meilleures inégalités de concentrations dans le cas où $\Xunan$ sont bornées. Maintenant, nous allons étudier comment contrôler la concentrations des variables aléatoires réelles quand elles ne sont pas bornées. Pour cela, on va étudier la "queue" de distribution de nos variables aléatoires, c'est à dire, à quel point la densité de nos variables "s'écrasent" vers 0 sur leurs queues. En réalité, plus nos variables aléatoires ont un affaissement de queue rapide, plus on pourra contrôler fortement la concentration de nos variables autour de leur moyenne. Nous utiliserons toujours la méthode de \textbf{Chernoff} en commençant par démontrer un théorème que nous utiliserons abondamment par la suite et nous continuerons sur les variables aléatoires dîtes "sous-gaussiennes", qui ont un écrasement exponentiel (voir ci-dessous).

\includegraphics[width=1\textwidth]{/Users/gabriel/Desktop/LateX/images/fiches_stat/queues_de_proba.png} 

\petitespace 

\thm 
Soit $X$ une variable aléatoire réelle, et $b \in [0, \infty]$ tel que pour tout $\lambda \in (0,b)$ on a $\esp{e^{\lambda X}} < \infty$. Alors 

$$\forall t \ge 0, \quad P(X-\esp X>t)\le e^{-\psi^*(t)} $$

où $\psi^*(t) = \Sup{\lambda \in (0,b)}\{ \lambda t -\log\big(\esp{e^{\lambda(X-\esp X)}}\big) \}$
\petitespace


\corr{\textbf{Preuve :}

Soit $t\ge 0$ et $\lambda \in (0,b)$, on a $P(X-\esp X >t) = P(e^{\lambda(X-\esp X)}>e^{\lambda t}) $ et donc, par Markov,

$$ P(e^{\lambda(X-\esp X)}>e^{\lambda t}) \le \frac{\esp{e^{\lambda(X-\esp X)}}}{e^{\lambda t}} = \exp[-(\lambda t-\log\big(\esp{e^{\lambda(X-\esp X)}}\big))] $$

Comme c'est vrai pour tout $\lambda \in (0,b)$ on peut juste optimiser en $\lambda \in (0,b)$ et cela donne le résultat escompté. 
\espace}

L'intérêt de la méthode de Chernoﬀ est qu’elle permet d’obtenir des résultats non-asymptotiques, c’est à dire valables pour toute valeur de n, sous des conditions sur la loi des variables aléatoires qui sont moins restrictives que de préciser un modèle paramétrique pour la loi de la variable aléatoire. \petitespace

\Def
Soit $X$ une variable aléatoire réelle intégrable et $\sigma \ge 0$. On dit que $X$ est $\sigma^2$-\textbf{sous-gaussienne} si 

$$\forall \lambda \in \R, \quad \esp{e^{\lambda(X-\esp{X} )}} \le e^{\sigma^2 \lambda^2/2} $$

\petitespace

\textbf{Lemme}\petitespace

Soit $X \sim \normale{\mu}{\sigma^2}$, alors pour tout $\lambda \in \R :$  

$$\esp{e^{\lambda X}} = e^{\lambda\mu+ \frac 12\sigma^2\lambda^2}$$

\petitespace

\corr{\textbf{Preuve :}

\begin{align*}
	\esp{e^{\lambda X}} &= \int_\R e^{\lambda x}\frac{1}{\sqrt{2\pi \sigma^2}}\exp(\frac{-(x-\mu)^2}{2\sigma^2})dx \\
\end{align*}

Or $\lambda x  - \frac{(x-\mu)^2}{2\sigma^2} = \lambda \mu + \frac{\lambda^2 \sigma^2}{2} - \frac{1}{2\sigma^2}(x-\mu-\lambda\sigma^2)^2$

Ainsi, 

$$\esp{e^{\lambda X}} = e^{\lambda \mu + \frac{\lambda^2 \sigma^2}{2}} \underbrace{\int_\R \frac{1}{\sqrt{2\pi \sigma^2}} \exp( - \frac{1}{2\sigma^2}(x-\mu-\lambda\sigma^2)^2)dx}_{=1} $$
\petitespace}



\petitespace

\Rq 

\cercler 1 Si $X \sim \normale{\mu}{\sigma^2}$ alors $X-\esp{X} \sim \normale{0}{\sigma^2}$ et donc, par le lemme, toute variable aléatoire réelle gaussienne est $\sigma^2$-sous-gaussienne.\petitespace

\cercler 2 Toutes variables aléatoires Gaussienne de variance $\sigma^2$ est $\sigma^2-$sous-gaussienne. \petitespace

\cercler 3 Si $\Xunan$ sont des variables aléatoires indépendantes $\sigma_i^2$-sous-gaussiennes, alors $\somme{i=1}{n} X_i $ est $\somme{i=1}{n}\sigma_i^2-$sous-gaussienne. \petitespace

\cercler 4 Par le \textbf{lemme d'Hoeffding}, toutes les variables aléatoires bornées (entre $a$ et $b$) sont $(\frac{b-a}{2})^2$-sous-gaussiennes.  \petitespace

\Prop

Soient $\Xunan$ des variables aléatoires iid et $\sigma^2-$sous-gaussienne. Alors, pour tout $t \ge 0$, 

$$P(\somme{i=1}{n}X_i-\mu \ge t) \le  e^{\frac{-t^2}{2n\sigma^2}}$$
et donc 

$$P(|\somme{i=1}{n}X_i-\mu| \ge t) \le  2e^{\frac{-t^2}{2n\sigma^2}} $$

\petitespace

\corr{\textbf{Preuve :}

Soient $\Xunan$ des variables aléatoires $\sigma^2-$sous-gaussienne. On a pour tout $t \ge 0$, pour tout $\lambda > 0$, 

\begin{align*}
	P(\somme{i=1}{n}X_i-\mu \ge t) &= P(e^{\lambda \somme{i=1}{n}(X_i-\mu)} \ge e^{\lambda t}) \\
	\text{ (Markov) }&\le \frac{\esp{e^{\lambda \somme{i=1}{n}(X_i-\mu)}} }{e^{\lambda t}} \\
	\text{ (indépendance) } & = e^{-\lambda t} \produit{i=1}{n} \esp{e^{\lambda(X_i-\mu)}}\\
	\text{ (sous-gaussienne) } &\le e^{-\lambda t} (e^{\sigma^2\lambda^2/2})^n \\
	\text{ (en prenant le $\lambda$ qui optimise) } & = e^{\frac{-t^2}{2n\sigma^2}}
\end{align*}

Pour le second résultat, on a bien que si $X$ est $\sigma^2-$sous-gaussienne, alors $-X$ l'est aussi. 

\petitespace}

\textbf{corollaire}

Si $\Xunan$ sont iid et $\sigma^2$ sous-gaussiennes, alors pour tout $t \in \R_+$,

$$P\big(\Xbarre-\esp{X_1} \ge t\big) \le \exp(\frac{-nt^2}{2\sigma^2}) =\alpha $$ 

Donc, pour tout $\alpha \in (0,1)$, 

$$P\big(|\Xbarre-\esp{X_1}|\ge \sigma\sqrt{ \frac{2\log(  \frac 2\alpha)}{n} } \big) \le \alpha $$

\petitespace

\textbf{Lemme (Symétrisation)}

Soit $X$ une variable aléatoire réelle et $\lambda>0$ tel que $\esp{e^{\lambda X}}<\infty$. Alors, si $X'$ est une variable aléatoire indépendante de $X$ et de même loi que $X$ et $\epsilon$ suivant une loi de Rademacher indépendante de $X$ et $X'$, alors on a 

$$\esp{e^{\lambda(X-\esp X)}} \le \esp{e^{\lambda\epsilon(X-X')}}$$

\corr{\textbf{Preuve :}
 
Par indépendance (et même loi) de $X$ et $X'$ on a $\esp X = \espcond{X'}{X}$. De plus, soit $A\in \cal B(\R)$, on a 
 
 \begin{align*}
 	P(\epsilon(X-X')\in A) &=P(X-X' \in A \cap \epsilon = 1) + P(X'-X \in A \cap \epsilon =-1)\\
	&= P(X-X' \in A)P(\epsilon=1) + P(X'-X \in A)P(\epsilon=-1) \\
	&= P(X-X' \in A)(P(\epsilon=1)+P(\epsilon=-1)) = P(X-X'\in A)
 \end{align*}
 
 Donc $\epsilon(X-X')\overset{(d)}{=} X-X'$
 
 et donc 
 
 $$\esp{e^{\lambda (X-\esp X)}} = \esp{e^{\lambda(\espcond{X-X'}{X})}} \overset{(\text{Jensen})}{\le}\esp{\espcond{e^{\lambda(X-X')}}{X'}} = \esp{e^{\lambda(X-X')} } = \esp{e^{\lambda\epsilon(X-X')}}$$
 
 

 
 \petitespace}



\Prop 
Soit $X$ une variable aléatoire réelle centrée et telle que ses moments sont “sous géométriques" au sens où il existe $b> 0$ tel que

$$\forall k \ge 2, \quad |\esp{X^k}|\le b^k $$

Alors $X$ est $4b^2$-sous-gaussienne. \petitespace

 \corr{\textbf{Preuve :}
 
 Soit $\lambda>0$, par le lemme de symétrisation on a pour $X' \overset{(d)}{=}X, X \indep X'$ :
 
 $$\esp{e^{\lambda X}} \le \esp{e^{\lambda \epsilon(X-X')}} $$
 
 Par symétrie de la variable $\epsilon(X-X')$, on a pour tout $k\in \N$, que $\esp{(\epsilon(X-X'))^{2k+1}}=0$. En effet, 
 
 
 \begin{align*}
	\esp{\epsilon(X-X')^{2k+1}} &= \esp{(\1_{\epsilon = 1}(X-X')^{2k+1}}  + \esp{(\1_{\epsilon = -1}(X-X')^{2k+1}} \\
	 &= P(\epsilon = 1)\esp{(X-X')^{2k+1}} + (1-P(\epsilon=1))\esp{(X-X')^{2k+1} } \\
	 &= \esp{(X-X')^{2k+1}}\\
	 &= \esp{(-1)^{2k+1}(X'-X))^{2k+1}} = -\esp{(X-X'))^{2k+1}}\\
	 &= -\esp{(X'-X))^{2k+1}} = -\underbrace{\esp{\epsilon(X-X')^{2k+1}}}_{\text{car} X'-X \overset{(d)}{=}X-X'}
\end{align*}

Donc $\esp{\epsilon(X-X')^{2k+1}}=0$. On en déduit par développement en séries entières que 
 
 $$\esp{e^{\lambda X}}\le \esp{e^{\lambda \epsilon(X-X')}}=\somme{k=0}{\infty}\frac{\lambda^k\esp{(\epsilon(X-X'))^k}}{k!} = \somme{k=0}{\infty}\frac{\lambda^{2k}\esp{(X-X')^{2k}}}{(2k)!} $$
 
 Or, en rappelant que $\epsilon(X-X') \overset{(d)}{=} X-X'$ on a : 
 
 $$\esp{(X-X')^{2k}} =  \somme{l=0}{2k} \begin{pmatrix} 2k \\l \end{pmatrix} \esp{X^{2k-l}}\esp{X^{l}} \le b^{2k}\overbrace{\somme{l=0}{2k} \begin{pmatrix} 2k \\l \end{pmatrix}}^{=(1+1)^{2k}} =  b^{2k}2^{2k}$$
 
 En utilisant le fait que $(2k)! \ge 2^{k} k!$, en effet, c'est vraie pour $k=0$, supposons désormais que $(2k)! \ge 2^{k} k!$ on a :
 
  \begin{align*}
	(2(k+1))! &= (2k+2)(2k+1)2k! \\
	& \ge (2k+2)(2k+1)2^{k} k! \\
	& = 2^{k+1}(k+1)!(2k+1) \ge 2^{k+1}(k+1)!
\end{align*}
 
Ce qui démontre le résultat par récurrence. Ainsi, nous obtenons :
 
 $$\esp{e^{\lambda k}} \le \somme{k=0}{+\infty} \frac{\lambda^{2k}b^{2k}2^k2^k }{(2k)!} \le \somme{k=0}{+\infty} \frac{(\sqrt 2 b\lambda)^{2k}}{k!}  = e^{2\lambda^2b^2} $$
 \petitespace}


\espace

\Def
Passons désormais aux variables aléatoires dîtes "\textbf{sous-poissonniennes}". Il s'agit d'une majoration moins forte de la queue de notre variable aléatoire. C'est donc moins restrictif.  \petitespace

Une variable aléatoire $X$ est dite \textbf{sous Poissonnienne} s’il existe $v^2$ et $b \ge 0$ tels que 

$$\forall \lambda>0, \quad \log\big( \esp{e^{\lambda(X-\esp X) }}\le \frac{v^2}{b^2}(e^{b\lambda}-1-b\lambda)  \big) $$

On note alors $X \in \text{sPoi}(v^2,b)$, et si $b=0$ on doit comprendre qu'alors $X$ est $v^2-$sous-gaussienne. \petitespace
 
 \Rq 
 
 Si $X \sim \cal P(\theta), \theta >0$ alors $X$ est sous Poissonnienne. \petitespace
 
 \corr{\textbf{Preuve :}
 
 Soit $\lambda>0$, on rappelle que $\esp X = \theta$.
 
 \begin{align*}
 	\esp{e^{\lambda(X-\esp X)}} &= \esp{e^{-\lambda \esp X}} \varphi_X(\frac \lambda i)\\
	&=e^{-\lambda\theta}e^{\theta(e^\lambda-1)}\\
 \end{align*}
 
 Donc $\log\big(\esp{e^{\lambda(X-\esp X)}}  \big) = -\lambda\theta + \theta(e^\lambda-1) = \theta(e^{\lambda} -\lambda - 1)$, donc on a bien que $X \in \text{sPoi}(\theta,1)$.
 
 
 
 \petitespace
 }

\Prop 
Soit $X$ une variable aléatoire telle que, pour $v^2,b \ge 0$ on a :

$$\forall k \ge 2, \quad \esp{(X-\esp X)^2(X-\esp X)^{k-2}_+} \le v^2b^{k-2} $$

Alors $X\in \text{sPoi}(v^2,b)$.

\petitespace

 \corr{\textbf{Preuve :}



\petitespace}

\newpage

\section{Estimation Statistique}

\subsection{Principes d'Échantillonage}

En pratique, on a des données $x_1, \ldots x_n \in \R^m, m \ge 1$ et on pose l'hypothèse E : On observe une réalisation de $n$ variables aléatoires \underline{iid} $\Xunan$ de même loi $P^*$. On appelle $\Xunan$ l'échantillon.

On suppose aussi que $\Xunan$ sont sur un espace probabilisé $\OmegaAP$ avec 

$$X_i : \Omega \to \R^m$$

Les données sont donc les réalisations de ces variables aléatoires, \ie

$$(x_1, \ldots, x_n) = (X_1(\omega), \ldots, X_n(\omega))$$ 

On a $\bf P^*=\{ \cal P(X_i \in A), A \in \cal A\}=$ Loi de $X_i$. On restera (sauf cas contraire mentionné) dans le cas $\Omega =  \R^m, \cal A = \cal B(\R^m)$, donc $\bf P^*$ est une loi sur $(\R^m, \cal B(\R^m))$. 

Le problème statistique est de trouver l'inconnue $\bf P^*$, à partir de $\Xunan$

\vspace{1.5em}

$\bullet$ \textbf{D'abord, on se pose un problème d'estimation}\vspace{1.5em}

Soit $\mathscr{P}$ l'ensemble des lois sur $(\R^m, \cal B(\R^m))$ et on pose 

$$T : \mathscr{P} \to \R^p$$

On cherche la meilleure approximation de $T(\bf P^*)$


\vspace{1.5em}

$\bullet$ \textbf{Puis, on se pose un problème de test d'hypothèse}\vspace{1.5em}

On prend $\mathscr{P}_0 \subset \mathscr{P}$ et l'on souhaite savoir si $\bf P^* \in \mathscr{P}_0$

On peut avoir, par exemple :\vspace{1em}

\hspace{1.5em}$\bullet \mathscr{P}_0 = \{ \normale{\mu}{\sigma^2}, \mu \in \R, \sigma^2 \in \R_+ \}$

\hspace{1.5em}$\bullet \mathscr{P}_0=\{ \bf P^*, \esp{X_1} = 0\}$

\subsection{Principe d'un Estimateur Statistique}

Comme évoqué ci-dessus, on cherche à estimer $T(\bf P^*)$ , avec $\theta^*$ le paramètre à estimer, on a donc 

$$T(\bf P^*) = \theta^*$$

On sait également que la fonction de répartition $F^*$ suffit à déterminer la loi $\bf P^*$, ce qui signifie concrètement qu'il existe une bijection 

$$\Psi : \mathscr{F} \to \mathscr{P}$$

Avec $\mathscr{F}$ l'ensemble des fonctions de répartitions sur $\R^m$

Pour résumer, on estime donc 

$$\theta^* = T(\bf P^*) = T\circ \Psi(F^*)$$

par 

$$ \widehat{\theta}_n = T(\widehat{P}_n) = T\circ \Psi(\widehat{F}_n)  $$ 

Avec $\widehat P_n \overset{\text{def}}{=} \Psi(\widehat F_n)$ et $\widehat \theta_n$ notre estimateur. \petitespace

\Def

On appelle \textbf{statistique} toute fonction mesurable des observations $S(\Xunan)$ dans $\R^m$. Une statistique $S$ est donc une variable aléatoire ou un vecteur aléatoire qui ne dépend \textbf{que} de l’échantillon. Une statistique est aussi appelée estimateur si elle est utilisée pour estimer des paramètres (ou d’autres caractéristiques) d’une loi de probabilité.

\petitespace
\Rq

1) On peut par ailleurs prouver que 

$$\forall A \in \cal B(\R^m),\widehat P_n(A) = \frac 1n \somme{i=1}{n}\1_{A}(X_i) =  \frac 1n \somme{i=1}{n}\delta_{X_i}(A)$$

2) Si $h : \R^m \to \R^m$ est une fonction mesurable, alors

$$  \int h(x)d\widehat P_n(x) = \frac 1n \somme{i=1}{n} h(X_i)$$


On définit donc pour cela la moyenne empirique 

$$\overline X_n = \int_{\R^m}xd\widehat P_n(x) = \frac 1n \somme{i=1}{n}X_i$$
et la variance empirique 

$$\widehat \Sigma_n = \int_{\R^m}xx^Td\widehat P_n(x) = \frac 1n \somme{i=1}{n} X_iX_i^T - \overline{X_n}\hspace{0.1em}\overline{X_n}^T = \frac 1n \somme{i=1}{n}(X_i-\overline{X_n})(X_i-\overline{X_n})^T$$

\petitespace

On utilise aussi régulièrement des \textbf{méthodes d'inversions} pour générer des variables aléatoires à partir d'une fonction de répartition donnée. C'est aussi une méthode très efficace pour simuler des distributions continues lorsque le pseudo inverse $F^{-1}$ est explicitement calculable. On les utilise dans de nombreux domaines tel que par exemple les algorithmes fondés sur des méthodes de Monte-Carlo, ou bien en \textit{Machine Learning}. 

\petitespace

\Prop 

Soit $U$ une variable aléatoire uniforme sur $[0,1]$, $F$ une fonction de répartition et 

$$\begin{array}{rrrl} %{f}{E}{R}{2x+3}

  F^{-1} :& [0,1] &\mapsto& \R\\
  & u & \mapsto & \Inf{x \in \R}\{F(x)\ge u\} \end{array}$$

son inverse généralisés. Alors \petitespace

\hspace{2em} (i) La variable aléatoire $X = F^{-1}(U)$ a pour fonction de répartition $F$. 

\petitespace

\hspace{2em} (ii) Si $ X \sim F^*,$ et que $F^*$ est continue, alors $F^*(X)\sim \cal U([0,1])$. 

\petitespace

\corr{\textbf{Preuve :}

\cercler 1 Soit $X = F^{-1}(U)$, et $x \in \R$. Montrons que pour tout $u \in (0,1]$, on a

$$F^{-1}(u) \le x \iff u \le F(x) $$

Par définition de $F^{-1}(u)$, si  $ u \le F(x) $ alors  $F^{-1}(u)\le x $. On a donc $u \le F(x)\implies F^{-1}(u) \le x $.

Inversement, si $F^{-1}(u) \le x$, alors, pour tout $\epsilon >0,$ on a $F^{-1}(u) \le x+\epsilon$, donc, par définition de $F^{-1}(u)$, on a $u \le F(x+\epsilon)$.

 Et comme, par propriété des fonctions de répartitions, $F$ est continue à droite, on a $u \le F(x)$, par conséquent, on a bien que $$F^{-1}(u) \le x \iff u \le F(x) $$

Ainsi on a, pour tout $x \in [0,1]$ :

$$P(X \le x) = P(F^{-1}(U) \le x) = P(U \le F(x)) = F(x)$$



\petitespace

\cercler 2 Soit $X \sim F^*$, trouvons la loi de $F^*(X)$. Posons 

$$\begin{array}{rrrl} %{f}{E}{R}{2x+3}

  F^{*^{-1}} :& [0,1] &\mapsto& \R\\
  & u & \mapsto & \Inf{x \in \R}\{F^*(x)\ge u\} \end{array}$$

On a donc bien $\forall t \in [0,1], \quad  F^*(F^{*^{-1}}(t)) =t$ car : soit $t \in [0,1]$.

\begin{align*}
	F^*(F^{*^{-1}} (t)) \ge t \quad \quad \quad \text{ (par définition)}
\end{align*}

Mais, pour tout $\epsilon >0$, on a $F^*(F^{*^{-1}} (t-\epsilon )) \le t$ par définition. Par continuité de $F$, on a bien par passage à la limite que $F^*(F^{*^{-1}}(t)) =t$.


De plus, Soit $t \in [0,1]$, et considérons $A_t = \big\{ x \in \R \mid F(x) \le t \big\}$, on a 

\begin{align*}
	P(F^*(X) \le t) &= P(X \in A_t)\\
	&= P(X \in (-\infty, \sup A_t) )\\
\end{align*}


Montrons que $\sup A_t = \sup \{x\in \R : F^*(x) \le t \} = \inf\overbrace{\{x \in \R : F(x)\ge t\}}^{B_t}  = \inf B_t \overset{(\text{def})}{=} F^{*^{-1}}(t)$. Directement, par croissance de $F^*$ on a $\sup A_t \le \inf B_t$. De plus, pour montrer que $\sup A_t \ge \inf B_t$, supposons par l'absurde que $\sup A_t < \inf B_t$. Alors, par définition du $\sup$ et de l'inf, il n'existe aucun $x \in \R \mid \sup A_t <x< \inf B_t$ et $F^*(x) = t$. Cependant, par croissance de $F^*$, pour tout $x \in (\sup A_t,\inf B_t)$, $F^*(x) >t$ car $x \notin A_t$ mais aussi $F^*(x) <t$ car $x \notin B_t$ ce qui est une contradiction. Par conséquent, on a bien  $\sup A_t \ge \inf B_t$ et  $\sup A_t \le \inf B_t$, donc  $\sup A_t = \inf B_t$ et ainsi :

$$\sup A_t  =  \inf B_t  \iff   \sup \{x\in \R : F^*(x) \le t \} = \Inf{x \in \R}\{F(x)\ge t\} = F^{*^{-1}}(t)$$

Cette dernière propriété est donc vraie peu importe la fonction de répartition. Et ainsi :
\begin{align*}
	P(F^*(X) \le t) &= P(X \in A_t)\\
	&= P(X \in (-\infty, \sup A_t) )\\
	&= P(X \le \inf B_t)\\
	& = F^*(F^{*^{-1}}(t))\\
	& = t
\end{align*}

\petitespace

Avec aussi, $\forall t >1, \quad P(F^*(X) \le t) = 1$ et $\forall t <0, \quad P(F^*(X)\le t) =0$ par propriété de la fonction de répartition. Ainsi, on a bien que $F^*(X) \sim \cal U([0,1])$

\petitespace}






\subsection{Modèle Statistique}

On appelle \textbf{modèle statistique} le triplet

$$\{ \cal X, \cal F, \{\P_\theta, \theta \in \Theta \}\} $$

Avec $\cal X$ appelé "espace d'état", $\cal F$ la tribu sur $\cal X$ et $\P_\theta$ une mesure probabilité sur $\cal X$.


Dans notre cas toujours égale à $\{ \R^m, \cal B(\R^m), \{\P_\theta, \theta \in \Theta \}\} $ qu'on simplifiera donc par 

$$ \{\P_\theta, \theta \in \Theta\}$$


On fera dans toute la section sur les estimateurs l'hypothèse (P) étant que $\bf P^* \in \{\bf P_\theta, \theta \in \Theta\}$ avec donc pour un certain $\theta^* \in \Theta$ tel que $\bf P^* = \P_{\theta^*}$. \espace

\subsubsection{Définitions}\espace

1) On dit qu'un modèle statistique est \textbf{paramétrique} si

$$\exists K \in \N, \Theta \subset \R^K$$

2) On dit qu'un modèle statistique est \textbf{identifiable} si 

$$ \forall \theta, \theta' \in \Theta,\quad  \P_\theta = \P_{\theta'} \implies \theta=\theta'$$

\ie: la fonction $\theta \to \P_\theta$ est injective \petitespace

3) On dit qu'un modèle statistique est \textbf{dominé} si il existe une mesure $\mu$  $\sigma$-finie  tel que 

$$\forall \theta\in \Theta, \P_\theta \ll \mu \iff \exists f \text{mesurable} \mid \forall A \in \cal F,\quad \P_\theta(A) = \int_Af(x)d\mu(x)\text{ (Radon-Nykodim)}$$

\Rq

$\bullet$On dit qu'un modèle est \textbf{à densité} (continue) s'il est dominé par la mesure de Lebesgue\petitespace

$\bullet$ on dit qu'un modèle est \textbf{discret} si il existe $A$ au plus dénombrable tel que

$$\forall \theta \in \Theta, \quad \P_\theta(A)=1$$

un modèle est discret si et seulement si $\modelstat$ est dominé par la mesure de comptage sur $\Theta$ dénombrable\petitespace

4) On dit que $\modelstat$ est \textbf{régulier}, si il vérifie les 4 hypothèses suivantes :\vspace{1.5em}

\hspace{2em} \textbf{H1} : le support de $\P_\theta$ ne dépend pas de $\theta$, avec le support définit par 

$$\textbf{supp}(\P_\theta)= \{x \in \R^m, \forall \epsilon>0, \P_\theta(B(x,\epsilon) >0\} $$

\shift \shift Cette hypothèse est équivalente à dire que 

$$\forall \theta, \theta' \in \Theta,\quad f(x,\theta)>0 \implies f(x, \theta')>0$$
\petitespace

\hspace{2em} \textbf{H2} : $f(x,.)$ et $\log(f(x,.))$ sont $\cal C^2(\Theta)$ pour $\mu$ presque tout $x$ ($\mu$ la mesure dominante du modèle). \petitespace

\hspace{2em} \textbf{H3} : $\forall \theta \in \Theta, \exists U \subset \Theta$ tel que $U$ est ouvert, et tel qu'il existe $\Lambda$ une fonction mesurable tel que 

$$\forall x \in \R, \Sup{\theta \in U}(\lvert l'(x,\theta)\rvert + l'(x,\theta)^2+\lvert l''(x,\theta)\rvert) \le \Lambda(x) $$

\shift \shift avec $l(x, \theta) = \log(f(x,\theta)),\quad l'(x,\theta) = \frac{\d l(x,\theta)}{\d \theta}$ et

$$\int_\R \Sup{\theta \in \Theta}f(x,\theta)\Lambda(x)d\mu(x)<\infty $$\petitespace

\hspace{2em} \textbf{H4} : $\forall \theta \in \Theta,\quad \esptheta{l'(X_1,\theta)^2}>0$ (dans $\R^m$, on remplace par $\esptheta{\nabla(l)(x,\theta) \nabla(l)(x,\theta)^T}\succ 0$) \petitespace


\Rq


Les modèles gaussiens, exponentielles, Bernouilli, Binomiale, Géométrique, Poisson sont régulier

En revanche, le modèle Laplace ($f(x, \theta)\alpha e^{-(x-\theta)}$) et uniforme ne sont pas réguliers.
\espace

\subsection{Estimateurs}\espace

Toute statistique $\widehat \theta_n, \overline \theta_n, \widetilde \theta_n$ est appelé estimateur si $\exists g$ mesurable tel que $\widehat \theta_n, \overline \theta_n, \widetilde \theta_n = g(\Xunan)$\espace

\shift$\bullet$Un estimateur $\widehat \theta_n$ est dit \textbf{consistant} si $\widehat \theta_n\cvP \theta^*$\petitespace

\shift$\bullet$ Un estimateur $\widehat \theta_n$ est dit \textbf{fortement consistant} si $\widehat \theta_n \cvps \theta^*$

\espace

On définit le \textbf{biais} d'un estimateur $\widehat \theta_n$ la fonction $B_\theta : \Theta \to \R$ tel que 

$$B_\theta(\widehat \theta_n) = \esp{\widehat \theta_n-\theta} $$

\espace

\subsubsection{Comparer les Estimateurs}\espace

\Def

Soit $\widehat \theta_n$ un estimateur, on appelle \textbf{risque} de $\widehat \theta_n$ la fonction $R_\theta : \Theta \to \R$ tel que 

$$R_\theta(\widehat \theta_n) = \esp{\lVert \widehat \theta_n-\theta\rVert_2^2} $$

\Rq

On a aussi

$$R_\theta(\widehat \theta_n) = \esp{\lVert\widehat \theta_n-\esp{\widehat \theta_n}\rVert_2^2} + \lVert\esp{\widehat \theta_n-\theta}\rVert_2^2=  \esp{\lVert\widehat \theta_n-\esp{\widehat \theta_n}\rVert_2^2} + \lVert B_\theta(\widehat \theta_n)\rVert^2_2 $$
\petitespace

\corr{\textbf{Preuve :}\vspace{-1cm}

\begin{align*}
	R_\theta(\widehat \theta_n) &= \esp{\lVert \widehat \theta_n-\theta\rVert_2^2} \\
	&=  \esp{\lVert \widehat \theta_n-\esp{\widehat \theta_n} + \esp{\widehat \theta_n}-\theta\rVert_2^2}\\
	&= \esp{\lVert\widehat \theta_n-\esp{\widehat \theta_n}\rVert_2^2} +2\esp{(\widehat \theta_n-\esp{\widehat \theta_n})^T( \esp{\widehat \theta_n}-\theta)}+\esp{\lVert\esp{\widehat \theta_n}-\theta\rVert_2^2}\\
	&=  \esp{\lVert\widehat \theta_n-\esp{\widehat \theta_n}\rVert_2^2} +2(\esp{\widehat \theta_n}-\theta)(\esp{\widehat \theta_n}-\esp{\widehat \theta_n})^T +\lVert\esp{\widehat \theta_n-\theta}\rVert_2^2\\
	&=\esp{\lVert\widehat \theta_n-\esp{\widehat \theta_n}\rVert_2^2} + \lVert\esp{\widehat \theta_n-\theta}\rVert_2^2\\
	&=\esp{\lVert\widehat \theta_n-\esp{\widehat \theta_n}\rVert_2^2} + \lVert B_\theta(\widehat \theta_n)\rVert^2_2 
\end{align*}

}

Pour des variables réelles univariée, on a donc 

$$R_\theta(\widehat \theta_n) = \Var{\widehat \theta_n} + B_\theta(\widehat \theta_n)^2$$

\espace

On considère qu'un estimateur $\widehat \theta_n$ est \textbf{asymptotiquement meilleur} qu'un autre estimateur $\overline \theta_n$ si, en définissant $ V_\theta(\widehat \theta)$ et $ V_\theta(\overline \theta)$ la variance limite de ces estimateurs \ie : $V_\theta(\widehat \theta) = \lim\limits_{n \to \infty}\bb V_\theta(\sqrt n \widehat \theta_n)$ sous $\theta \in \Theta$ on a :

 $$\accogauche
&\forall \theta \in \Theta, V_\theta(\widehat \theta) \le  V_\theta(\overline \theta)    \\
& \exists \theta_0\in \Theta, V_{\theta_0}(\widehat \theta) <  V_{\theta_0}(\overline \theta)
\finaccogauche$$

\espace

Pour comparer des estimateurs de manière non asymptotique, plusieurs moyens différents existent : \espace

\shift $\bullet$  \textbf{1er moyen, l'approche min-max Worst case risk}\espace

On veut minimiser $R^*_\theta(\widehat \theta_n)=\Sup{\theta \in \Theta}R_\theta(\widehat \theta_n)$\petitespace

\shift On dit que $\widehat \theta_n$ est \textbf{préférable} à $\overline \theta_n$ si $R^*_\theta(\widehat \theta_n) < R^*_\theta(\overline \theta_n)$\petitespace

\shift On dit donc que $\widehat \theta_n$ est \textbf{min-max optimal} si 

$$\widehat \theta_n \in \text{arg}\Min{\overline \theta_n}R^*_\theta(\overline \theta_n)$$
\espace

\shift $\bullet$ \textbf{2e moyen, l'approche bayésienne}\espace

On introduit une fonction de poids $w : \Theta \to [0,1]$ tel que le risque intégré est 

$$ R^B_\theta(\widehat \theta_n) = \int_\Theta R_\theta(\widehat \theta_n)w(\theta)d\theta $$

On a alors que $\widehat \theta_n$ est préférable à $\overline \theta_n$ si 

$$R^B_\theta(\widehat \theta_n)<R^B_\theta(\overline \theta_n)$$

\espace


\textbf{Efficacité asymptotique}\espace

On ne considère que les estimateurs asymptotiquement normaux, \ie: tel que 

$$\forall \theta \in \Theta, \sqrt n(\overline \theta_n-\theta) \cvl \normale{0}{V(\overline \theta)}$$

\petitespace

\Def 

Soit deux estimateurs, $\widehat \theta_n, \overline \theta_n$ asymptotiquement normaux de variance limite $V_\theta(\widehat \theta), V_\theta(\overline \theta)$

On dit que $\widehat \theta_n$ est \textbf{asymptotiquement meilleur} que $\overline \theta_n$ si 

\[
\accogauche
&V_\theta(\widehat \theta)\le V_\theta(\overline \theta),\quad \forall \theta \in \Theta\\
&\exists \theta_0 \in \Theta\mid V_{\theta_0}(\widehat \theta)<V_{\theta_0}(\overline \theta)
\finaccogauche
\]
\petitespace

On dit qu'un estimateur est \textbf{asymptotiquement efficace} si il n'existe pas d'estimateur asymptotiquement meilleur que lui. 
 \petitespace
 
 \Def
 
 
 Dans un \textbf{modèle régulier}, un estimateur est \textbf{asymptotiquement efficace} si, et seulement si 
 
 $$V(\overline \theta_n)=\frac{1}{I(\theta)} $$
 
 Avec $I(\theta)$ l'information de Fischer. \espace

\subsubsection{Estimateur de la Méthode des Moments}\espace

$\Xunan \overset{iid}{\sim} \bf P_{\theta^*}$, des variables aléatoires dans $\R^m$, $\theta \in \Theta \subset\R^k, \quad k \ge 1$

Soit $\varphi_1\ldots\varphi_k : \R^m \to \R$ tel que $\esptheta{\varphi_j(X_i)}<\infty,\quad \forall \theta \in \Theta,\quad \forall i,j$\petitespace

On pose 

$$m_j(\theta) = \esptheta{\varphi_j(X_1)} \text{ et }\widehat m_j(\theta) = \frac 1n\somme{i=1}{n}\varphi_j(X_i)$$

\Def

On dit que $\widehat{ \theta}^{MM}$ est un estimateur de la \textbf{méthode des moments} si il vérifie

$$\forall j \in \ldbrack 1,k\rdbrack,\quad\quad m_j(\widehat{ \theta}^{MM}_n)=\widehat m_j$$
\petitespace

Autrement dit, pour $\Phi : \R^{k}\to \R^k$, on a 

$$M(\theta) = \esptheta{\Phi(X_1)}$$

et

$$\widehat{M}_n=\frac 1n \somme{i=1}{n}\Phi(X_i) $$

On a alors :

$$M(\widehat{ \theta}^{MM}_n)=\widehat M_n $$


\thm

Si $M : \Theta \to \R^k$ est un fonction continue et injective, et que $\widehat M_n \in \text{Im}(M)$ alors $\widehat \theta^{MM}$ existe et 

$$\widehat \theta^{MM}\cvps \theta $$

\corr{\textbf{Preuve :}

Comme $M$ est continue et injective, on a $M : \Theta \to \text{Im}(M)$ qui est bijective et continue, donc $M^{-1}$ est continue par théorème de la fonction réciproque 

$$\widehat M_n \cvps M(\theta)$$ 

Donc 

$$\widehat \theta^{MM}_n=M^{-1}(\widehat M_n) \cvps \theta$$
}

\textbf{Corollaire :}\petitespace

Si $M$ est injective et $\widehat M_n \in Im(M)$, alors $\widehat \theta^{MM}_n$ existe est est unique.

$$\widehat \theta^{MM}_n = M^{-1}(\widehat \theta^{MM}_n)$$

\petitespace

\thm

Si $\esp{\norme{\Phi(X_1)}^2}<\infty, M \in \cal C^1(\Theta)$ et est injective, et que $\widehat \theta^{MM}_n$ existe, alors $\widehat \theta^{MM}_n$ est asymptotiquement normal.

\petitespace

\corr{\textbf{Preuve :}

$M$ est $\cal C^1$ donc comme $M$ est injective, $M^{-1}$ l'est aussi, 

Par TCl, on a 

$$\sqrt n(\widehat M_n-\esp{\Phi(X_1)}) \cvl \cal N_k(0, \Var{\Phi(X_1)})$$

Donc avec $\delta$-méthode, on a :

$$\sqrt n(M^{-1}(\widehat M_n)-M^{-1}(\esp{\Phi(X_1)}))=\sqrt n(\widehat \theta_n^{MM}-\theta)\cvl \cal N_k[0, \nabla M^{-1}(\esp{\Phi(X_1)})^T\Var{\Phi(X_1)}\nabla M^{-1}(\esp{\Phi(X_1)})]$$
\petitespace}


\subsubsection{Estimateur du Maximum de Vraisemblance}\espace


$\Xunan \overset{iid}{\sim} \P_{\theta^*},\quad \theta^*\in \Theta \subset \R^k$	


On se place tout le long de ce chapitre, dans un modèle $\{ \P_\theta, \theta \in\Theta \}$ \textbf{dominé} $\P_{\theta^*}\ll \mu, \quad\forall \theta \in \Theta$. \petitespace

\Def

On appelle \textbf{Vraisemblance} du modèle $\modelstat$ la fonction 

$$\begin{array}{rrrl}
  L_n :& \R^n \times \Theta &\mapsto& \R_+\\
  & (x_1, \ldots, x_n,\Theta)& \mapsto & \produit{i=1}{n}f(x_i, \theta) \end{array}$$

\Rq

Si les $X_i$ ne sont pas indépendants, on a alors $L_n(x_1, \ldots, x_n, \theta)=f_{(\Xunan)}(x_1, \ldots, x_n, \theta)$

\petitespace


\Def

On dit que $\EMV$ est un \textbf{estimateur du maximum de vraisemblance} si 

$$L_n(\Xunan, \EMV)=\Max{\theta \in \Theta}L_n(\Xunan, \theta)$$

\petitespace

Donc si $\EMV \in \text{arg}\Max{\theta \in \Theta} L_n(\Xunan, \theta)$\espace

\Rq\vspace{0.5em}

Dans certains modèles, l'estimateur du maximum de vraisemblance n'existe pas ou n'est pas unique. \petitespace

\Def

On appelle la \textbf{log-vraisemblance} la fonction 

$$l_n(\theta) = - \frac 1n log(L_n(\Xunan, \theta)) = -\frac 1n \somme{i=1}{n}\log(f(X_i, \theta)) $$

D'où $\EMV \in \arg \Min{\theta \in \Theta}l_n(\theta)$\espace

\textbf{Conditions}\espace

$\int log(f(x, \theta))|f(x, \theta^*)d\mu(x)<\infty \quad \forall \theta, \theta^* \in \Theta$\petitespace

On pose $Z_i = -\log(f(X_i, \theta)),\quad \overline Z_n \cvps -\bb{E}_{\theta^*}(Z_1)$ et $J(\theta) = \bb{E}_{\theta^*}(Z_1)$ \petitespace

\thm 

\shift $\bullet J(\theta) \ge J(\theta^*) \quad \forall \theta \in \Theta$\petitespace

\shift $\bullet J(\theta)=J(\theta^*) \iff \P_\theta=\P_{\theta^*}$ (si $\modelstat$ identifiable alors $J(\theta)=J(\theta^*)\iff \theta=\theta^*)$\petitespace

\corr{\textbf{Preuve :}
\begin{align*}
	J(\theta) &= \bb{E}_{\theta^*}[-\log(f(X_1, \theta))]\\
	&=\bb{E}_{\theta^*}[-\log(f(X_1, \theta))]+\bb{E}_{\theta^*}[-\log(\frac{f(X_1, \theta)}{f(X_1, \theta^*)})]\\
	&\ge J(\theta^*)-\log(\bb{E}_{\theta^*}[\frac{f(X_1, \theta)}{f(X_1,\theta^*)})] \text{ (Jensen)}\\
	&\ge J(\theta^*) \text{ Car $\bb{E}_{\theta^*}[\frac{f(X_1, \theta)}{f(X_1,\theta^*)}]=1$}
\end{align*}

D'où, $J(\theta)\ge J(\theta^*), \quad  \forall \theta, \theta^* \in \Theta$. D'autres part, 

$$J(\theta)-J(\theta^*)=-\bb{E}_{\theta^*}[\log(\frac{f(X_1, \theta)}{f(X_1,\theta^*)})] = -\bb{E}_{\theta^*}[\log(\frac{f(X_1, \theta)}{f(X_1,\theta^*)}) - \frac{f(X_1, \theta)}{f(X_1, \theta^*)}+1]$$

Si $J(\theta)=J(\theta^*)$, alors 

$$\bb{E}_{\theta^*}[\log(\frac{f(X_1, \theta)}{f(X_1,\theta^*)}) - \frac{f(X_1, \theta)}{f(X_1, \theta^*)}+1]=0$$

Donc comme $log(x) \le x-1$ et $log(x)=x-1 \iff x=1$, on a 

$$\log(\frac{f(X_1, \theta)}{f(X_1,\theta^*)}) - \frac{f(X_1, \theta)}{f(X_1, \theta^*)}+1\overset{p.s.}{=}0$$

et 

$$f(X_1, \theta)=f(X_1, \theta^*)\iff \P_\theta=\P_{\theta^*}$$
}

\Rq

L'idée donc derrière l'utilisation du maximum de vraisemblance comme estimateur du paramètre $\theta$, est la suivante. Nous cherchons les paramètres qui "expliquent le mieux" les données observées. Cela revient à trouver les paramètres qui rendent l'événement observé le plus probable.

\petitespace

\thm

Soit $\Theta \in \R$ un ouvert, de plus on a :\vspace{1em}

\shift i) $f(x, \theta) \in \cal C^0(\Theta), \quad  \forall x \in \R$ \vspace{1em}

\shift ii) Le modèle $\modelstat$ est identifiable\vspace{1em}

\shift iii) $\int |log(f(x, \theta))|f(x, \theta^*)d\mu(x) <\infty \quad \forall \theta, \theta^* \in \Theta$\vspace{1em}

\shift iv) L'ensemble des minima locaux de $l_n(\theta)$ forment un intervalle fermé de $\R$\vspace{1em}

Alors

$$\EMV \cvps \theta^*$$

\petitespace

\Def\petitespace


On appelle \textbf{information de Fisher} dans un modèle $\modelstat$ 


$$I(\theta)=\esptheta{l'(x, \theta)^2} = \esptheta{\nabla[l](x, \theta)\nabla[l](x, \theta)^T} $$\petitespace

avec $l(x, \theta)=log(f(X_1, \theta))$, $l'(x, \theta)=\frac{\d l(x, \theta)}{\d\theta}$\petitespace

Si le modèle $\modelstat$ est régulier, alors on note le \textbf{score} du modèle 

$$s(x, \theta)=l'(x, \theta)$$

 et on a :

$$I(\theta)=\bb{V}_\theta(s(X_1, \theta))$$

\corr{\textbf{preuve : }

Il suffit de montrer que $\esptheta{s(X_1, \theta)}=0$. On a :


\begin{align*}
	&\int f(x, \theta)d\mu(x)=1\\
	 \iff &\frac{\d}{\d \theta}\int f(x, \theta)d\mu(x)=0\\
	\text{(H2+H3)}\iff& \int \frac{\d}{\d\theta}f(x, \theta)d\mu(x)=0\\
	\iff&\int \frac{f'(x, \theta)}{f(x, \theta)}f(x, \theta)d\mu(x)=0\\
	\iff&\int l'(x, \theta)f(x, \theta)d\mu(x)=0
\end{align*}
}
\thm

Dans un modèle régulier, on a

$$I(\theta)=-\esptheta{l''(X_1, \theta)} = -\esptheta{\nabla^2[l](X_1, \theta) }$$

\petitespace

\corr{\textbf{preuve :}

Par ce qui précède on a :


\begin{align*}
	&\int l'(x, \theta)f(x, \theta)d\mu(x)=0\\
	\text{par H3, H2}\iff&\int[ l''(x, \theta)f(x, \theta)+l'(x, \theta)f'(x, \theta)]d\mu(x)=0\\
	\iff&\esptheta{l''(X_1, \theta)}+\int(l'(x, \theta))^2f(x, \theta)d\mu(x)=0\\
	\iff& I(\theta)=-\esptheta{l''(X_1, \theta)}
\end{align*}
}
\thm

\textbf{normalité asymptotique de l'estimateur du maximum de vraisemblance}\vspace{1em}

Soit $\Xunan \simiid \P_{\theta^*}, \P_{\theta^*}\ll \mu, \theta^*\in \Theta$ ouvert, $\Theta \subset \R$


Supposons que \espace

\shift 1) $\EMV$ existe \espace

\shift 2) $\EMV$ est consistant\espace

\shift 3) Le modèle $\modelstat$ est régulier et identifiable, \espace


Alors on a 

$$\sqrt n (\EMV-\theta^*) \cvl \normale{0}{\frac 1{I(\theta^*)}}$$
\petitespace

\corr{\textbf{Preuve :}

Comme $\Theta$ est un ouvert, et $l(x, \theta)$ est  $\cal C^1$ en $\theta$, on a 

$$\EMV \in \arg \Min{\theta \in \Theta}l_n(\theta) \implies l'_n(\EMV)=0$$


On sait par TAF, comme $l_n(x, \theta) \in \cal C^2(\Theta)$ pour $\mu$ presque tout $x$, qu'il existe $\widetilde \theta_n \in [\theta^*, \EMV]$ tel que 

$$l'_n(\EMV)-l'_n(\theta^*)=l''_n(\widetilde \theta_n)(\EMV-\theta^*)$$ 

Donc 

$$\sqrt n(\EMV-\theta^*)=\sqrt n \frac{-l'(\theta^*)}{l''(\widetilde\theta_n)}$$

Or on a 

$$-\sqrt n l'_n(\theta^*)=\sqrt n (\frac 1n\somme{i=1}{n}log(f(X_i, \theta^*))')=\sqrt n (\frac 1n\somme{i=1}{n}s(X_i, \theta^*)-\esptheta{s(X_1, \theta)})\cvl \normale{0}{\bb{V}_\theta(s(X_1, \theta^*))}$$

Par TCL en en utilisant le fait que $\esptheta{s(X_1, \theta)}=0$ par régularité de $\modelstat$

Donc on a 

$$-\sqrt n l'_n(\theta^*) \cvl \normale{0}{I(\theta^*)}$$

on a aussi que $l''_n(\widetilde \theta_n)=l''_n(\theta^*)+l''_n(\widetilde \theta_n)-l''_n(\theta^*)$ \espace

or $l''_n(\theta^*)=\frac 1n \somme{i=1}{n}l''(X_i, \theta^*) \cvps \esptheta{l''(\theta^*)}=-I(\theta^*)$ par la loi forte des grands nombres\espace

On a aussi que $l''_n(\widetilde \theta_n)-l''_n(\theta^*) \cvP 0$ donc,  par slutsky, on a 

$$\sqrt n(\EMV-\theta^*)=\sqrt n \frac{l'(\theta^*)}{l''(\widetilde\theta_n)}\cvl \normale{0}{\frac{1}{I(\theta^*)}}$$
}

\Rq

\shift 1) Le théorème est valable même si $\EMV$ n'est pas unique \espace

\shift 2) Le théorème peut être étendu au cas où $\Theta \in \R^d, \quad d \ge 2$ et on a 

$$\sqrt n(\EMV-\theta^*)\cvl \cal N_d(0, I^{-1}(\theta^*))$$
\espace

\Def

\textbf{Divergence de Kulback Leibler}\petitespace

Soit $P, Q$ deux mesures de probabilités sur $(\cal X, \cal A)$

On appelle \textbf{divergence de Kulback Leibler} la quantité 

$$D_{K,L}(P||Q)=\accogauche
&\int_{\cal X} \frac{d P}{d Q}(x)log( \frac{d P}{d Q}(x))dQ(x) \text{ si $P \ll Q$}\\
&+\infty \text{ sinon}
\finaccogauche$$


\Rq

Si $\exists \mu \mid P \ll \mu, Q \ll \mu$ alors on a 

$$D_{K,L}(P||Q)= \int_{\cal X} p(x)log(\frac{p(x)}{q(x)})d\mu(x)$$ 

où $p(x)=\frac{d P}{d \mu}(x)$ et $q(x)=\frac{d Q}{d \mu}(x)$ respectivement les densités de $P$ et $Q$ par rapport à $\mu$  \petitespace

\Prop

\shift 1) $D_{K,L}(P||Q)\ge 0$\petitespace

\shift 2) $D_{K,L}(P||Q)=0 \iff P=Q$\petitespace

\shift 3) $D_{K,L}(P||Q) \ne D_{K,L}(Q||P)$ en générale \petitespace

\shift 4) $\begin{pmatrix} P \\ Q \end{pmatrix}\to D_{K,L}(P||Q)$ est convexe, \ie 

$$D_{K,L}(\alpha P_1 +(1-\alpha)P_2||\alpha Q_1+(1-\alpha)Q_2)\le \alpha D_{K,L}(P_1||Q_1)+(1-\alpha)D_{K,L}(P_2||Q_2)$$
\petitespace


\Prop

Dans un modèle régulier, l'information de Fisher est égale à la courbure de la divergence de Kulbak Leibler : 

$$I(\theta)=\frac{\d^2}{\d \theta^2}D_{K,L}(\P_{\theta^*}||\P_\theta)$$


\thm 

\textbf{Inégalité de Cramer-Rao}\petitespace

Si $\modelstat$ est régulier et dominé alors pour tout estimateur $\overline \theta_n$ de $\theta$ fondé sur $\Xunan \simiid \P_{\theta^*}$, on a : 

$$\esp{(\overline \theta_n-\theta)^2} \ge \frac{(1+(B_\theta(\overline \theta_n))')^2}{nI(\theta)} + B_\theta(\overline \theta_n)^2 $$

équivalent à 

$$\Var{\overline \theta_n} \ge \frac{(1+(B_\theta(\overline \theta_n))')^2}{nI(\theta)}$$

Avec $B_\theta(\overline \theta_n)'$ la dérivée du biais de $\overline \theta_n$ par rapport à $\theta$

\petitespace
\corr{\textbf{Preuve :}

Notons $s_n(X, \theta)=\somme{i=1}{n}l'(X_i, \theta)$. Par Cauchy Schwarz on a 

$$\cov(s_n(X, \theta))\le \Var{s_n(X, \theta)}\Var{\overline \theta_n}$$

Premièrement, on a par régularité du modèle :

$$\Var{s_n(X, \theta)} \overset{iid}{=} \somme{i=1}{n}\Var{l'(X_i, \theta)}=nI(\theta) \shift (1)$$ 

Deuxièmement, 

\begin{align*}
	\cov(s_n(X, \theta), \overline \theta_n) &= \esptheta{s_n(X, \theta)\overline \theta_n}-\esptheta{s_n(X, \theta)}\esptheta{\overline \theta_n}\\
	&=\esptheta{\somme{i=1}{n}l'(X_i, \theta)\overline \theta_n}\\
	&=\esptheta{\somme{i=1}{n}\frac{\d log(f(X_i, \theta))}{\d \theta}\overline \theta_n}\\
	&=\esptheta{\frac{\d log(L_n(\Xunan, \theta))}{\d \theta}\overline \theta_n}\\
	&=\int_{\cal X^n} \frac{\d log(L_n(\Xunan, \theta))}{\d \theta}L_n(\Xunan, \theta)\overline \theta_n d\mu(x_1,\ldots, x_n)\\
	\text{Par H2, H3}&=\frac{\d}{\d\theta}\int_{\cal X^n}L_n(\Xunan, \theta)\overline \theta_nd\mu(x_1,\ldots, x_n)\\
	&=\frac{\d}{\d\theta}\esptheta{\overline \theta_n}\\
	&=\frac{\d}{\d\theta}(B_\theta(\overline \theta_n)+\theta)\shift(2)\\
	&=(1+B_\theta(\overline \theta_n)')
\end{align*}

D'où en combinant $(1)$ et $(2)$ on a 

$$\Var{\overline \theta_n} \ge \frac{(1+(B_\theta(\overline \theta_n))')^2}{nI(\theta)} $$
}


\subsubsection{Estimateurs de Bayes}

On se place dans un modèle $\modelstat$ \textbf{dominé}, $\Theta \in \R^p, p \in \N$ avec $\Xunan \simiid \P_{\theta^*}$ on notera $\vec X = (\Xunan),\quad \vec x=x_1, \ldots, x_n$\petitespace

On suppose cette fois-ci que $\theta$ suit une loi dite \textbf{à priori} $\Pi_0$. On supposera toujours que $\Pi_0$ admet une densité par rapport à la mesure de Lebesgue et on notera, par abus de langage, $\Pi_0$ sa densité.\petitespace


\Def

On dit que $\estB$ est un \textbf{estimateur de Bayes }s'il minimise le risque intégré par rapport à une loi à priori $\Pi_0$

$$\estB \in \arg\Min{\overline \theta_n}\int_\Theta \esptheta{\norme{\overline \theta_n-\theta}_2^2}d\Pi_0(\theta)$$

$$\estB \in \arg\Min{\overline \theta_n}\int_\Theta\int_{\cal X^n}\norme{\overline \theta_n(\vec x)-\theta}_2^2L_n(\vec x, \theta)d\mu_n(\vec x)d\Pi_0(\theta) $$

\petitespace

Avec $d\mu_n(\vec x)=d\mu_1(x_1) \times \ldots \times d\mu_n(x_n)$\petitespace

\Rq 

C'est la définition de l'estimateur Bayésien quand le risque quadratique est utilisé (ce que l'on considèrera tout le long ce cette section) mais de manière plus générale,  on a 

$$\estB(\vec X) \in \arg\Min{\overline \theta_n}\int_\Theta\esptheta{\varphi(\overline \theta_n, \theta)}\Pi_0(\theta)d\theta$$

pour $\varphi$ une fonction mesurable qu'on choisit.\petitespace

\thm

Soit $\Pi_n(\theta)$ la densité donnée par 

$$\Pi_n(\theta)\alpha L_n(\vec X, \theta)\Pi_0(\theta)\shift \forall \theta \in \Theta$$

Alors, si on a $\int_\Theta\norme{\theta}_2^2\Pi_0(\theta)d\theta <+\infty$ Alors, l'estimateur Bayésien existe est est donné par 

$$\estB=\int_\Theta \theta \Pi_n(\theta)d\theta$$


\textbf{Interprétation}\petitespace

Si on considère que $L_n(\vec X, \theta)$ est la densité de $\vec X|\theta$ où $\theta \sim \Pi_0$, alors $\Pi_n(\theta)$ est la densité de $\theta|\vec X$

Par Bayes, on obtient facilement donc que $\Pi_n(\theta)\alpha L_n(\vec X, \theta)\Pi_0(\theta)$, comme $\Pi_n$ est une densité de probabilité, on a en fait une autre formule de $\Pi_n$ donnée par 

$$\Pi_n(\theta)=\frac{\Pi_0(\theta)L_n(\vec X, \theta)}{\int_\Theta \Pi_0(t)L_n(\vec X, t)dt}$$\petitespace

\textbf{Pour le choix de la loi à priori}\petitespace

Il n'y a pas de choix universel de $\Pi_0$, ce dernier est toujours subjectif, il vaut donc mieux le choisir tel que\petitespace

\shift 1) $\Pi_0$ a pour support $\Theta$, avec $\Theta$ un ouvert \petitespace
\shift 2) Choisir $\Pi_0$ tel que le calcul de $\estB$ soit faisable


\Def

Soit $\cal P_0$ une famille de loi sur $\Theta$. On dit que $\cal P_0$ est une \textbf{famille conjuguée} pour le modèle $\modelstat$ si $\forall \Pi_0 \in \cal P_0$, la loi a priori $\Pi_n \in \cal P_0$. (Par exemple $\Xunan \simiid \pois{\theta}$ et $\cal P_0$ la famille des loi Gamma)\petitespace


\thm

Dans un modèle régulier, si $\Pi_0$ a pour suport $\Theta$ et si $\estB$ est consistant, alors

$$\sqrt n(\estB-\theta^*)\cvl \normale{0}{\frac{1}{I(\theta^*)}}$$\espace

\newpage

\section{Test Statistique}\petitespace

\subsection{Notion de Test}\petitespace

\Def

Soit $\modelstat$ un modèle statistique sur $\cal X^n$, soient $\Theta_0, \Theta_1\subset \Theta \mid \Theta_0\cap \Theta_1 = \emptyset$\petitespace

Durant cette partie, on notera $\cal Z = \cal X^n$ et $Z = (X_1(\omega), \ldots, X_n(\omega)) = (x_1, \ldots, x_n)$ (pour un certain $\omega \in \Omega$)

On dit que $H_0 : \theta \in \Theta_0$ est \textbf{l'hypothèse nulle} et que $H_1 : \theta \in \Theta_1$ est \textbf{l'hypothèse alternative}.\petitespace

Un \textbf{test} de $H_0$ contre $H_1$ est une fonction mesurable 

$$ \phi : \cal Z \mapsto \{0,1\}$$

tel que si $\phi(Z) = 1$ on dit que l'on \textbf{rejette} l'hypothèse nulle et si $\phi(Z) = 0$, on l'accepte.\petitespace

Deux types d'erreurs : \petitespace

\begin{center}
\begin{tabular}{|c|c|c|}
  \hline
  \diagbox{$\phi$}{$\theta$} & $H_0$ &$H_1$\\
  \hline
  $\phi(Z)=0$ & Cool  &Erreur de type II\\
  \hline
  $\phi(Z)=1$ &Erreur de type I&Cool\\
  \hline
\end{tabular}
\end{center}
\petitespace

On cherche généralement à minimiser l'erreur de type I. \petitespace

\Def

Soit $\phi : \cal Z \mapsto \{0,1\}$ un test. La fonction de \textbf{puissance} de $\phi$ est la fonction

$$\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \beta_\phi  :& \Theta &\mapsto& [0,1]\\
  & \theta & \mapsto & \P_\theta(\phi(Z)=1)
  
\end{array}$$\petitespace

Idéalement, on aimerait que $\beta_\phi(\theta)=0$ si $\theta \in \Theta_0$ et $\beta_\phi(\theta)=1$ si $\theta \in \Theta_1$. Mais c'est impossible si les supports de lois de $\Theta_0$ et de $\Theta_1$ se recouvrent.

Autre objectif : pour un $\epsilon$ petit, 

$$\accogauche
&\beta_\phi(\theta) \le \epsilon, \forall \theta \in \Theta_0\\
&\beta_\phi(\theta)\ge 1- \epsilon, \forall \theta \in \Theta_1
\finaccogauche$$

C'est possible, si $\Theta_0, \Theta_1$ sont "séparés" (il y a une distance non nulle entre les deux).\petitespace


\Def

Le \textbf{niveau} du test $\phi: \cal Z \mapsto \{0,1\}$ est défini par :

$$\alpha(\phi) = \Sup{\theta \in \Theta_0}\beta_\phi(\theta) = \Sup{\theta \in \Theta_0}\P_\theta(\phi(Z)=1) $$\petitespace

On dit que $\phi$ est de niveau (au plus) $\alpha$, si $\alpha(\phi)\le \alpha$ (probabilité de type I $\le \alpha$). \espace

\textbf{Point de vue de Neymann-Pierson :}\petitespace

Chercher un test de niveau $\alpha$ (souvent $\alpha = 10\%/5\%/1\%$) aussi puissant que possible sous $H_1$.\petitespace

\Def 

Soit $(\phi_\alpha)_{\alpha \in [0,1]}$ une famille de tests $\phi_\alpha : \cal Z \mapsto \{0,1\}$ de $H_0$ contre $H_1$ tel que \petitespace

\shift \cercler 1 $\phi_\alpha$ est de niveau $\alpha, \quad \forall \alpha \in [0,1]$.\petitespace

\shift \cercler 2 Pour tout $ \alpha, \alpha' \in [0,1]$ tel que $\alpha \le \alpha'$, on a $\phi_\alpha \le \phi_{\alpha'}$\espace

\Def

On définit alors la \textbf{p-valeur} par

$$\widehat \alpha = \widehat \alpha(Z) = \inf\big\{\alpha \in [0,1] : \phi_\alpha(Z) =1\big\}$$

Donc on a $\forall \alpha \in [0,1] :$\petitespace

\shift $\bullet$   $\widehat \alpha <\alpha \implies \phi_\alpha(Z) = 1$\petitespace

\shift $\bullet$   $\widehat \alpha > \alpha \implies \phi_\alpha(Z) = 0$\espace


Attention : on calcul la p-valeur \textbf{uniquement sous $H_0$} !

\Prop

\shift \cercler{1} Sous $H_0$, la p-valeur $\widehat \alpha(Z)$ est "sur-uniforme" \ie : $\forall \theta_0 \in \Theta_0$ on a

$$\P_{\theta_0}(\widehat \alpha(Z)\le t)\le t, \forall t \in [0,1] $$ \petitespace

\shift \cercler{2} Si de plus, $\Theta_0 = \{\theta_0\}$ et si $\alpha(\phi_\alpha)=\alpha, \forall \alpha \in [0,1]$, alors sous $\P_{\theta_0}$ on a 

$$\widehat \alpha \sim \cal U([0,1])$$

\corr{\textbf{Preuve :}

\cercler{1}  Soit $\alpha >t$, si $\widehat \alpha \le t$ alors $\widehat \alpha \le \alpha$, donc $\phi_\alpha(Z) =1$ donc 

$$\P_{\theta_0}(\widehat \alpha \le t) \le \P_{\theta_0}(\phi_\alpha(Z)=1)\overset{\text{par def}}{\le} \alpha$$

Car $\phi_\alpha$ est de niveau $\alpha$. Ceci est vrai $\forall \alpha>t$ donc $\P_{\theta_0}(\widehat \alpha \le t) \le t $

(si $\widehat \alpha >t$, on a directement $\P_{\theta_0}(\widehat \alpha \le t) = 0 \le t$)

\petitespace

\cercler{2} Si $\widehat \alpha >t$, alors $\phi_t(Z)=0$ donc 

$$\P_{\theta_0}(\widehat \alpha >t) \le \P_{\theta_0}(\phi_t(Z)=0) = 1-t $$

Par hypothèse. Donc $\P_{\theta_0}(\widehat \alpha \le t) \ge t$ d'où on a $\widehat \alpha \sim \cal U([0,1])$
}
\petitespace


\Def

Un \textbf{test randomisé} de $H_0$ contre $H_1$ est une fonction 

$$\phi : \cal Z \mapsto [0,1] $$

Le résultat  du test est obtenu de la façon suivante : 

Soit $U \sim \cal U([0,1])$ indépendant de $Z$. On prend $\widehat \Psi = \1_{\{U \le \phi(Z) \}}$. Alors 

$$P( \widehat \Psi =1|\phi(Z)) = P(U \le \phi(Z)|\phi(Z)) = \phi(Z)$$

On définit 

$$\beta_\phi(\theta) = \P_\theta(\widehat \Psi =1)=\esptheta{P(\widehat \Psi =1|\phi(Z))} = \esptheta{\phi(Z)} $$ \espace

\subsubsection{Régions de Confiance}\espace

\Def 

Soit $\alpha \in (0,1)$. Une \textbf{région de confiance} de niveau $1-\alpha$ est une fonction

$$\begin{array}{rrrl}

  C_\alpha  :& \cal Z &\mapsto& \cal P(\Theta) \text{ (ensemble des partie de $\Theta$)}\\
  & Z & \mapsto & C_\alpha(Z)
  
\end{array}$$

telle que en notant $ \widehat C_\alpha = C_\alpha(Z)\subset \Theta $ on a pour tout $\theta \in \Theta$

$$\P_\theta(\theta \in  C_\alpha(Z))\ge 1-\alpha  $$

\Rq

$\theta \in \Theta$ est déterministe, la région $\widehat C_\alpha$ est aléatoire.\petitespace

Il y a une correspondance exacte entre : \petitespace

\shift $\bullet$ régions de confiance de niveau $1-\alpha$\petitespace

\shift $\bullet$ tests d'adéquations : pour tout $\theta_0 \in \Theta$, le test $\phi^{(\theta_0)}_\alpha : \cal Z \mapsto \{0,1\}$ de niveau $\alpha$ de $H_0^{(\theta_0)} : \theta =\theta_0$\petitespace

 \shift contre $H_1^{(\theta_0)} : \theta \ne \theta_0 $\petitespace

(Les tests d'adéquations sont une classe spécifique de test)\espace

\Prop

\textbf{Lien régions de confiance/test d'adéquation}\petitespace

\cercler{1} Soit $\widehat C_\alpha$ une région de confiance de niveau $1-\alpha$. Alors pour tout $\theta_0 \in \Theta$, le test $\phi_\alpha^{(\theta_0)}(Z) = \1_{\{ \theta_0 \notin \widehat C_\alpha\}}$ 

\shift est un test d'adéquation de niveau $\alpha$\espace

\cercler 2 Soit $(\phi_\alpha^{(\theta_0)})_{\theta_0\in \Theta}$ une famille de test d'adéquation de niveau $\alpha$. Alors 

$$ \widehat C_\alpha = \{\theta_0 \in \Theta \mid \phi_\alpha^{(\theta_0)}(Z)=0\} $$ 

est une région de confiance de niveau $1-\alpha$. De plus, ces deux transformations sont inverses l'une de l'autre. \petitespace


\corr{\textbf{Preuve :}

\cercler 1 Soit $\theta_0 \in \Theta$. Le niveau de $\phi_\alpha^{(\theta_0)}$ est donné par 

$$\P_{\theta_0}(\phi_\alpha^{(\theta_0)}(Z)=1) = \P_{\theta_0}(\theta_0\notin \widehat C_\alpha) =1-\P_{\theta_0}(\theta_0 \in \widehat C_\alpha)\le \alpha $$\petitespace

\cercler 2 Pour tout $\theta_0 \in \Theta$. Le niveau de $\phi_\alpha^{(\theta_0)}$ est donné par 

$$\P_{\theta_0}(\theta_0\notin \widehat C_\alpha) = \P_{\theta_0}(\phi_\alpha^{(\theta_0)}(Z)=1)\le \alpha$$
}

\textbf{Méthode du pivot}\petitespace

\Def

Un \textbf{pivot} est une fonction $G : \cal Z \times \Theta \mapsto E$ telle que la loi de $G(Z, \theta)$ lorsque $Z \sim P_\theta$ ne dépend pas de $\theta$\petitespace

\Rq


Avec les notations ci-dessus, notons $Q$ la loi de $G(Z, \theta)$. Soit $B_\alpha \subset E$ tel que $Q(B)\ge 1-\alpha$. Alors $\widehat C_\alpha = \{\theta \in \Theta \mid G(Z, \theta) \in B_\alpha\}$ est une région de confiance de niveau $1-\alpha$. En effet, pour tout $\theta \in \Theta$, 

$$\P_\theta(\theta \in \widehat C_\alpha) = \P_\theta(G(Z, \theta)\in B) = Q(B)  \ge 1-\alpha$$

\petitespace


\subsection{Tests Uniforméments Plus Puissants}

\petitespace

\Def
Soient $\alpha \in (0,1)$ et $\phi_\alpha : \cal Z \mapsto [0,1]$. On dit que $\phi_\alpha$ est \textbf{uniformément plus puissant de niveau $\alpha$} (UPP-$\alpha$) si \petitespace

\shift i) $\phi_\alpha$ est de niveau $\alpha$\petitespace

\shift ii) pour tout autre test $\phi$ de niveau $\alpha$, $\phi_\alpha$ est "meilleur que $\phi$", au sens ou :

$$\forall \theta \in \Theta_1,\quad \beta_{\phi_\alpha}(\theta) \ge \beta_\phi(\theta)$$

\Rq

Il n'existe pas nécessairement de test UPP-$\alpha$ de $H_0$ contre $H_1$, mais nous allons voir des conditions dans lesquelles il en existe. \petitespace

\subsubsection{Dans le Cas de Deux Hypothèses Simples}\petitespace

On considère dans cette partie le cas $Z \sim P$, sur $\cal Z$, et on a $\Theta_0 = \{\theta_0 \} = \{0\}$ et $\Theta_1 = \{\theta_1 \} = \{1\}$

Avec les hypothèses : $H_0 : P = P_0, \quad H_1 : P = P_1$.\petitespace

Si $\phi:\cal Z \mapsto [0,1]$ est un test randomisé, on a :\petitespace


\shift $\bullet$ $\alpha(\phi) = \bb E_0[\phi(Z)] = \beta_\phi(0)$\petitespace


\shift $\bullet$ $\beta(\phi) = \beta_\phi(1) = \bb E_1[\phi(Z)]$ \petitespace

On suppose que $P_0$ et $P_1$ admettent des densités $p_0, p_1 : \cal Z \mapsto \R_+$ par rapport à une même mesure $\mu$ sur $\cal Z$ (on peut prendre par exemple $\mu = P_1 + P_0$).\petitespace

On considère aussi la condition :

\begin{equation}
p_0, p_1 > 0, \mu.p.p. \tag{C}
\end{equation}


On note que $\phi$ est UPP-$\alpha$ si :\petitespace


\shift $\bullet$ $\alpha(\phi)\le \alpha$\petitespace


\shift $\bullet$ Pour tout $\phi'$, tel que $\alpha(\phi') \le \alpha$, on a $\beta(\phi) \ge \beta(\phi')$ \petitespace

si $\phi$ est une solution du problème d'optimisation sous contrainte :

$$\arg \Max{\underset{\int_{\cal Z} \phi p_0 d\mu \le \alpha}{\phi : \cal Z \mapsto [0,1]} } \int_{\cal Z} \phi p_1 d\mu$$

Par théorème de changement de variable on a :

$$\bb{E}_i[\phi(Z)] = \int_{\cal Z}\phi(z)dP_i(z) = \int_{\cal Z} \phi(z)p_i(z)d\mu(z) $$\petitespace

Il est pour cela bon de randomiser car $\phi : \mapsto [0,1]$ espace convexe, ce qui permet d'avoir un problème d'optimisation convexe et linéaire.

Le problème est équivalent à 


\begin{equation}
\arg \Min{\phi : \int \phi p_0 d\mu \le \alpha} \big(-\int \phi p_1 d\mu\big) \tag{*}
\end{equation}

Afin de résoudre (*), on considère la version régularisée/Lagrangienne étant donnée $\lambda \in \R_+,$ on définit

$$E_\lambda(\phi) = -\int \phi p_1 d\mu + \lambda\int \phi p_0 d\mu $$

\textbf{Lemme}\petitespace

Pour tout $\lambda \in \R_+$, et $q\in [0,1],$ on définit le test \petitespace

$$\phi _{\lambda, q}(z) = \1_{p_1(z)>\lambda p_0(z)} + q\1_{p_1(z) = \lambda p_0(z)} =\begin{cases}
1 & \text{si } p_1(z) > \lambda p_0(z) \\
q & \text{si } p_1(z) = \lambda p_0(z) \\
0 & \text{sinon}
\end{cases}$$\petitespace

Alors, pour tout $q\in [0,1], \phi_{\lambda, q}\in \arg \min E_{\lambda}$

\petitespace

Réciproquement, si $\phi \in \arg \Min{\phi}E_\lambda(\phi)$, et si $p_0, p_1>0$, alors 

$$\phi(z) = \begin{cases} 
1 & \text{si } \frac{p_1(z)}{p_0(z)} > \lambda \\
0 & \text{si } \frac{p_1(z)}{p_0(z)}<\lambda
\end{cases}$$

\petitespace

\corr{\textbf{Preuve :}


Pour tout $\phi : \cal Z \mapsto [0,1]$, on a 

$$E_\lambda(\phi) = \int_{\cal Z} \phi(z)(\lambda p_0(z) -p_1(z) ) d\mu(z)$$

Or, pour tout $z \in \cal Z$ :

$$ \arg \Min{\phi(z)\in [0,1]}  \phi(z)(\lambda p_0(z) -p_1(z) ) = \begin{cases} 
0 & \text{si } \lambda p_0(z)-p_1(z)>0 \\
1 & \text{si } \lambda p_0(z)-p_1(z)<0 \\
[0,1] & \text{si } \lambda p_0(z)-p_1(z)=0
\end{cases}$$

En particulier, 

$$\phi(z)(\lambda p_0(z) -p_1(z) ) \ge \phi_{\lambda, q}(z)(\lambda p_0(z) -p_1(z) ) $$

On notera $F(z) = \phi(z)(\lambda p_0(z) -p_1(z))$ et $G(z) = \phi_{\lambda, q}(z)(\lambda p_0(z) -p_1(z) )$

Donc $E_\lambda(\phi) \ge E_\lambda(\phi_{\lambda, q})$ et donc 

$$\phi_{\lambda, q}\in \arg \min E_\lambda$$

Pour la réciproque, on utilise que si $F(z)\ge G(z), \forall z \in \cal Z$, et si $\int F d\mu = \int G d\mu$, alors $F \overset{\mu.p.p.}{=}G$
\petitespace}

\thm
\textbf{Neymann-Pearson}\petitespace

\cercler 1 Pour tout $\alpha \in [0,1]$, il existe $\lambda \in \R^*$ et $q \in [0,1] \mid$

$$   \alpha(\phi_{\lambda, q}) = \alpha  $$\petitespace


\cercler 2 Dans ce cas, $\phi_{\lambda, q}$ est UPP-$\alpha$. De plus, tout test $\phi$ UPP-$\alpha$ est un minimiseur de $E_\lambda$, en particulier, 

$$\mu.p.s., \phi(z) =  \begin{cases} 
1 & \text{si } p_1(z) > \lambda p_0(z) \\
0 & \text{si } p_1(z) < \lambda p_0(z)
\end{cases}$$

\petitespace

\corr{\textbf{Preuve :}\petitespace

\cercler 1 On pose $F : \R_+ \mapsto [0,1] \mid F(\lambda ) =P_0( p_1(Z) \le p_0(Z)\lambda ) = P_0(  \frac{p_1(Z)}{p_0(Z)} \le \lambda)$ 
\petitespace 


Alors $F$ est une fonction de répartition, donc $F$ est croisante, cadlàg, et telle que $\Lim{\lambda \to + \infty}F(\lambda) = 1$, $F(\lambda)=0$, si $\lambda <0$\petitespace

\Rq

\begin{align*}
	\alpha(\phi_{\lambda, q}) &= P_0(p_1(z) > p_0(z)\lambda)  + qP_0(p_1(z) = p_0(z)\lambda)\\
	&= 1-F(\lambda) + q(F(\lambda) - F(\lambda_-) )\\
	&\in [1-F(\lambda), 1-F(\lambda_-)]
\end{align*}


On pose $\lambda^* = \inf\{\lambda \in \R_+ : F(\lambda) \ge 1-\alpha \} < \infty$\petitespace

$F(\lambda) \underset{\lambda \to \infty}{\longrightarrow} 1 >1-\alpha$ Alors, par définition : $F(\lambda)<1-\alpha,\quad \forall \lambda<\lambda^*$\petitespace

Donc $F(\lambda_-^*) \le 1-\alpha $ \ie : $1-F(\lambda_-^*)\ge \alpha$. Par croissance de $F$, on a $F(\lambda)\ge 1-\alpha$ pour tout $\lambda > \lambda^*$\petitespace


Donc $F(\lambda^*) = \Lim{\underset{\lambda> \lambda^*}{\lambda \to \lambda^*}}F(\lambda)\ge 1- \alpha$ \ie :  $1-F(\lambda^*) \le \alpha$. On cherche $q \in [0,1]$ tel que 

$$\alpha(\phi_{\lambda^*,q}) = \alpha = 1-F(\lambda^*) + q(F(\lambda^*)-F(\lambda^*_-)) \iff q = \frac{\alpha-(1-F(\lambda))}{F(\lambda^*)-F(\lambda^*_-)}$$\petitespace


\cercler 2 Soient $\lambda \in \R^*$ et $q \in [0,1] \mid \alpha(\phi_{\lambda,q}) = \alpha$ montrons que $\phi_{\lambda,q}$ est UPP-$\alpha$. Soit $\phi$ un autre test tel que $\alpha(\phi)\le \alpha$, alors :


\begin{align*}
	\beta(\phi) &= \lambda \alpha(\phi)-E_\lambda(\phi) \\
	&\le \lambda \alpha-E_\lambda(\phi_{\lambda,q}) \text{ (Lemme)}\\
	&\le \lambda \alpha(\phi_{\lambda,q}) -E_\lambda(\phi_{\lambda,q}) \\
	&\le \beta(\phi_{\lambda,q})
\end{align*}

Donc $\phi_{\lambda,q}$ est UPP-$\alpha$. Réciproquement, si $\phi$ est UPP-$\alpha$, alors 

\begin{align*}
	E_\lambda(\phi)  &= \lambda \alpha(\phi)- \beta(\phi) \\
	& \le \lambda\alpha-\beta(\phi_{\lambda, q}) \text{ ($\phi$ est UPP-$\alpha$)}\\
	&\le E_\lambda(\phi_{\lambda, q}) = \inf E_\lambda
\end{align*}
 }

\Rq

Si la loi de $\frac{p_1(z)}{p_0(z)}$ sous $Z \sim P_0$ est $\cal C^0$, alors il existe un test de rapport de vraisemblance \textbf{pur} UPP-$\alpha$ 

$$\phi_\lambda(z) = \1_{p_1(z)>\lambda p_0(z)} $$

Sinon, il peut être nécessaire d'avoir recours à un test randomisé. \espace


 \subsubsection{Test UPP Cas Général}\espace

\textit{Rappel} : $\phi : \cal Z \mapsto [0,1]$ est UPP-$\alpha$ si $\phi$ est de niveau $\alpha$ et si $\forall \phi'$ de niveau $\alpha$, 

$$\forall \theta_1 \in \Theta_1, \quad \beta_\phi(\theta_1) = \bb{E}_{\theta_1}[\phi(Z)] \ge \beta_{\phi'}(\theta_1) $$


\textbf{Observation :}\petitespace



On sait que si $\Theta_0 =\{ \theta_0\}$ et si $\phi$ est UPP-$\alpha$ de $H_0 : \theta=\theta_0$ contre $H_1 : \theta \in \Theta_1$ alors $\phi$ est UPP-$\alpha$ de $H_0 : \theta=\theta_0$ contre $H_1 : \theta = \theta_1$  pour tout $\theta_1 \in \Theta_1$

En particulier, par le cours précédent, il existe $\lambda \in \R_+$ tel que 

$$\phi(z) =  \begin{cases} 
1 & \text{si } \frac{p_{\theta_1}(z)}{p_{\theta_0}(z)} > \lambda  \\
0 & \text{si } \frac{p_{\theta_1}(z)}{p_{\theta_0}(z)} < \lambda
\end{cases} \mu. p.p.$$

\textbf{Questions :}\petitespace

Si $\phi$ est UPP-$\alpha$ de $H_0 : \theta=\Theta_0$ contre $H_1 : \theta = \Theta_1$, est-il vrai que $\phi$ est aussi UPP-$\alpha$ de $H_0 : \theta=\theta_0$ contre $H_1 : \theta = \theta_1, \forall \theta_0 \in \Theta_0, \theta_1 \in \Theta_1$ ?\petitespace

La réponse est non. En effet, prenons $\Xunan \simiid \normale{\theta}{1}$, $H_0 : \theta \le 0$ contre $H_1 : \theta>0$

Alors $\phi(Z_n) = \1_{\overline X_n > \frac{q_{1-\alpha}}{\sqrt n}}$ est UPP-$\alpha$, en revanche, si $\theta_0 <0$ et $\theta_1>0$, $\phi$ n'est pas UPP-$\alpha$ de $H_0 : \theta = \theta_0$ contre $H_1 : \theta = \theta_1$ car le test UPP-$\alpha$ est donné par 


$$\phi'(Z_n) = \1_{\overline X_n > \theta_0+\frac{q_{1-\alpha}}{\sqrt n}} $$

\espace

\Def

Soient $\modelstat$ un modèle statistique identifiable et dominé par $\mu$ sur $\cal Z$. On dit que le modèle admet des \textbf{rapports de vraisemblance monotones} si  $\Theta \subset \R$ et s''il existe une statistique $T : \cal Z \mapsto \R$ telle que pour tout $\theta_0, \theta_1 \in \Theta$ avec $\theta_0 <\theta_1$, il existe une fonction croissante $\Psi_{\theta_0, \theta_1} : \R \mapsto \R_+$ telle que 

$$\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)}  = \Psi_{\theta_0, \theta_1}(T(Z)),\quad \forall Z \in \cal Z $$

\petitespace

En particulier, si $\Psi_{\theta_0, \theta_1}$ est strictement croissante, on a 

$$\1_{\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)}>\lambda} = \1_{T(Z) > \Psi^{-1}_{\theta_0, \theta_1}(\lambda) } $$
	
\espace

\textbf{Lemme}

Soit $Y$ une variable aléatoire, et $f,g :  \R \mapsto \R$ croissantes et positives (ou intégrables). Alors 

$$\esp{f(Y)g(Y)} \ge \esp{f(Y)}\esp{g(Y)} $$

\petitespace

\corr{\textbf{Preuve :}

Soit $Y'$ une variable aléatoire indépendante de $Y$ et de même loi. 

En distinguant les cas $Y \leq Y'$ et $Y > Y'$, grâce à la croissance de $f$ et $g$ on prouve facilement l'inégalité :
        \[(f(Y) - f(Y'))(g(Y) - g(Y')) \geq 0\]

D'où


\begin{align*}
	0& \le \esp{(f(Y)- f(Y'))(g(Y) - g(Y'))}\\
	& = \esp{f(Y)g(Y)} + \esp{f(Y')g(Y')} -\esp{f(Y)g(Y')} - \esp{f(Y')g(Y)}\\
	&= 2(\esp{f(Y)g(Y)}-\esp{f(Y)}\esp{g(Y)})
\end{align*}

Ce qui démontre le résultat. (équivalent à dire que $\cov(f(Y),g(Y)) \ge 0$)
\petitespace}


\textbf{Lemme}

Sous les conditions du théorème (qui suit), pour toute fonction $g : \R \mapsto \R$ croissante, la fonction $\theta \mapsto \esptheta{g(T(Z))}$ est croissante. En particulier, $\beta_{\phi_{\lambda, q}}$ est croissante.

\petitespace

\corr{\textbf{Preuve :}

Soient $ \theta_0 < \theta_1$, on a 

\begin{align*}
	\bb{E}_{\theta_1}[g(T(Z))] &= \int_{\cal Z}g(T(z))p_{\theta_1}(z)d\mu(z)\\
	&= \int_{\cal Z}g(T(z))\frac{p_{\theta_1}(z)}{p_{\theta_0}(z)}p_{\theta_0}(z)d\mu(z)\\
	&=\bb{E}_{\theta_0}[g(T(Z))\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)}]\\
	&= \bb{E}_{\theta_0}[g(T(Z))\Psi_{\theta_0, \theta_1}(T(Z))]\\
	\text{(Lemme)}&\ge   \bb{E}_{\theta_0}[g(T(Z))]\bb{E}_{\theta_0}[ \Psi_{\theta_0, \theta_1}(T(Z))]\\
\end{align*}

Or 

$$ \bb{E}_{\theta_0}[ \Psi_{\theta_0, \theta_1}(T(Z))] = \int_{\cal Z}\frac{p_{\theta_1}(z)}{p_{\theta_0}(z)}p_{\theta_0}(z)d\mu(z) = 1$$

Donc $\bb{E}_{\theta_1}[g(T(Z))] \ge \bb{E}_{\theta_0}[g(T(Z))]$, d'où la croissance de $\theta \mapsto \esptheta{g(T(Z))}$.

Pour la deuxième assertion, on note $\phi_{\lambda,q}(Z)= g_{\lambda,q}(T(Z))$, où $ g_{\lambda,q}(t) = \1_{\{t>\lambda \}} + q\1_{\{ t=\lambda \}}$ croissante
\petitespace}


\thm
\textbf{Neymann-Pierson}\petitespace

Soit $\modelstat$ un modèle admettant des rapports de vraisemblance monotones pour la statistique $T$. On cherche à tester $H_0: \theta < \theta_0$ contre $H_1$ : $\theta > \theta_0$. Alors\petitespace


           \shift 1) Il existe $\lambda \in \R$ et $q \in [0, 1]$ tels qu'en notant $\phi_{\lambda, q}(z) = \1_{\{T(z) > \lambda \}} + q \1_{\{ T(z) = \lambda\}}$, on a :
           
        
$$\beta_{\phi_{\lambda, q}}(\theta_0) = P_{\theta_0}(T(Z) > \lambda) + q P_{\theta_0}(T(Z) = \lambda) = \alpha $$\petitespace

	\shift 2)  Dans ce cas, $\phi_{\lambda, q}$ est un test UPP-$\alpha$.
	
\petitespace

\corr{\textbf{Preuve :}\petitespace

\cercler 1 Existance de $\lambda \in \R, q\in [0,1] \mid \quad \beta_{\phi_{\lambda,q}}(\theta_0) = \alpha$ est admise, car la preuve est identique à celle faire précédemment.\petitespace

\cercler 2 Montrons que $\phi_{\lambda,q} = \1_{\{T(Z)>\lambda \}} + q \1_{\{T(Z)=\lambda \}}$ est UPP-$\alpha$\petitespace

\shift \cercler a Montrons d'abord que $\alpha(\phi_{\lambda,q})\le \alpha$. 

\begin{adjustwidth}{5.2 em}{0cm}
	Pour tout $\theta \le \theta_0$, par le lemme, on a 
	$$\beta_{\phi_{\lambda,q}}(\theta) \le \beta_{\phi_{\lambda,q}}(\theta_0) = \alpha $$
\end{adjustwidth}

\shift \cercler b Montrons que $\phi_{\lambda, q}$ est UPP-$\alpha$

\begin{adjustwidth}{5.2 em}{0cm}
	Soit $\theta_1 > \theta_0$, soit $\phi$ un autre test de niveau $\alpha$ pour $\Theta_0$. On va montrer que $\phi_{\lambda,q}$ est un test de rapport de vraisemblance. On pose $\lambda(\theta_1) = \Psi_{\theta_0, \theta_1}(\lambda)$.\petitespace
	
	$\bullet$ Si $\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)}> \lambda(\theta_1)$, alors $T(Z)>\lambda$ donc $\phi_{\lambda,q}(Z) =1$.\petitespace
	
	$\bullet$  Si $\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)}< \lambda(\theta_1)$, alors $T(Z)<\lambda$ donc $\phi_{\lambda,q}(Z) =0$.\petitespace
	
	De plus, $\alpha(\phi_{\lambda, q}) = \alpha$, donc, par le cours précédent, $\phi_{\lambda, q}$ est UPP-$\alpha$ de $\{\theta_0\}$ contre $\{\theta_1\}$. Donc $\beta_{\phi,q}(\theta_1) \ge\beta_{\phi,q}(\theta_0)$
\end{adjustwidth}
\petitespace}

\Rq

Si $\Theta \subset \R, \quad (\P_\theta)_{\theta \in \Theta}$ modèle avec des rapports de vraisemblances monotones en $T(Z)$, alors le modèle $(\P_\theta)_{\theta \in \Theta}$ admet des rapports des rapports de vraisemblances monotones en $-T(Z)$, et c'est un test UPP$-\alpha$ de $H_0 : \theta \ge \theta_0$ contre $H_1 : \theta<\theta_0$ sous la forme 

$$\phi(Z) = \1_{\{T(Z)<c\}} + q \1_{\{T(Z)=c\}} $$


\Rq

$\Xunan \simiid \normale{0}{1} \quad H_0 : \theta=0$ contre $H_1 : \theta \ne 0$

Il n'y a pas de test UPP-$\alpha$. En effet, si $\phi$ est UPP-$\alpha$, alors \petitespace

\shift $\bullet \phi$ est UPP-$\alpha$ de $H_0 : \theta =0$ contre $H_1' : \theta >0$. Par l'unicité dans Neymann-Pierson, 

\begin{equation*}
\phi(Z) = \1_{\{\Xbarre > \frac{q_{1-\alpha}}{\sqrt n}\}} \quad \text{ p.s. sous $P_0$}  \tag{1}
\end{equation*}

\petitespace

\shift  $\bullet \phi$ est UPP-$\alpha$ de $H_0 : \theta =0$ contre $H_1'' : \theta <0$. Par le même raisonnement, on a, presque sûrement :

\begin{equation*}
\phi(Z) = \1_{\{\Xbarre < \frac{q_{1-\alpha}}{\sqrt n}\}} \tag{2}
\end{equation*}
\petitespace

$(1)$ et $(2)$ sont contradictoires (valeurs différentes) si $Z>\frac{q_{1-\alpha}}{\sqrt n}$, de probabilité $>0$.
\petitespace

\subsubsection{Test de Rapport de Vraisemblance}
\petitespace

Méthode générale pour construire un test de $H_0 : \theta \in \Theta_0$ contre $H_1 : \theta \in \Theta_1$

$\P_\theta$ admet une densité $p_\theta$ par rapport à $\mu$ sur $\cal Z$
\petitespace

\Def

Un test de \textbf{rapport de vraisemblance} de $\Theta_0$ contre $\Theta_1$ est un test de la forme $\phi(Z) =  \1_{\{T(Z) > c\}}$

$$T(Z) = \frac{\Sup{\theta \in \Theta_1}p_\theta(Z)}{\Sup{\theta \in \Theta_0}p_\theta(Z)} $$


\Rq 

On peut aussi considérer la statistique 

$$\widetilde T(Z) = \frac{\Sup{\theta \in \Theta}p_\theta(Z)}{\Sup{\theta \in \Theta_0}p_\theta(Z)} = \frac{\max(\Sup{\theta \in \Theta_0}p_\theta(Z), \Sup{\theta \in \Theta_1}p_\theta(Z))}{\Sup{\theta \in \Theta_0}p_\theta(Z)} $$ 

\begin{align*}
	\widetilde T(Z) &= \max(1, \frac{\Sup{\theta_1 \in \Theta}p_\theta(Z)}{\Sup{\theta \in \Theta_0}p_\theta(Z)})\\
	&=\max(1, T(Z))
\end{align*}

Si $c \ge 1$, $\widetilde T(Z)>c$ si, et seulement si $T(Z)>c$.\petitespace

En théorie, on détermine $c_\alpha>0$ tel que le test soit de niveau $\alpha$. (Si $c_\alpha>1$, on peut remplacer $T(Z)$ par $\widetilde T(Z)$)

\petitespace

\subsubsection{Cas des Modèles à Rapports de Vraisemblances Monotones}
\petitespace

On suppose que $\Theta \subset \R$ et que $\forall \theta_0, \theta_1 \in \Theta_1,\quad \theta_0 \le \theta_1,$

$$\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)} = \Psi_{\theta_0, \theta_1}(S(Z))$$

$H_0 : \theta \le \overline \theta$ contre $H_1 : \theta >\overline \theta$.

\begin{align*}
	T(Z) &=  \frac{\Sup{\theta_1 > \overline\theta}p_{\theta_1}(Z)}{\Sup{\theta_0 \le  \overline\theta}p_{\theta_0}(Z)} \quad \text{ ($\theta_0 \le \overline \theta < \theta_1)$}\\
	&=\Sup{\theta_1>\overline \theta} \Inf{\theta_0 \le \overline \theta}\frac{p_{\theta_1}(Z)}{p_{\theta_0}(Z)}\\
	\text{(RVM)}&= \Sup{\theta_1>\overline \theta} \Inf{\theta_0 \le \overline \theta} \Psi_{\theta_0, \theta_1}(S(Z))\\
	&=\Psi(S(Z))
\end{align*}


où $\Psi(s) = \Sup{\theta_1>\overline \theta} \Inf{\theta_0 \le \overline \theta} \Psi_{\theta_0, \theta_1}(s)$ croissante, car $ \Psi_{\theta_0, \theta_1}$ l'est. \petitespace

Tests de rapports de vraisemblances: $\1_{\{\Psi(S(Z))>t \}}$, $t=\Psi(c) \iff \1_{\{ S(Z)>c \}} $ On retrouve le test UPP-$\alpha$.
\petitespace

\textbf{Test d'adéquation}\petitespace

$H_0 : \theta = \theta_0$ contre $H_1 : \theta \ne \theta_0$, 

$$\widetilde T(Z) = \frac{\Sup{\theta \in \Theta}p_\theta(Z)}{\Sup{\theta \in \Theta_0}p_\theta(Z)} = \frac{p_{\widehat \theta}(Z)}{p_{\theta_0}(Z)}$$

où $\widehat \theta = \arg \Max{\theta \in \Theta}p_\theta(Z)$ (estimateur du maximum de vraisemblance)\petitespace

\subsection{Test Asymptotiques}

\petitespace

Dans l'analyse statistique, la connaissance de la loi de probabilité exacte d'une statistique de test est souvent un luxe que la réalité des modèles complexes ne nous offre pas. C'est dans ce contexte que les tests asymptotiques deviennent des outils indispensables. Ils nous permettent de contourner l'absence d'une distribution explicite en s'appuyant sur le comportement de la statistique lorsque la taille de l'échantillon tend vers l'infini, offrant ainsi une méthode robuste pour l'inférence statistique même face à l'incertitude théorique.

En effet, illustrons le problème avec un exemple concret. Soit 

$$\phi(Z) = \1_{\big\{  \frac{p_{\EMV}(Z)}{p_{\theta_0}(Z)}>c_\alpha(\theta_0) \big\}}$$

Test de rapport de vraisemblance où $c_\alpha(\theta_0)$ quantile d'ordre $(1-\alpha)$ de $\frac{p_{\EMV(Z)}(Z)}{p_{\theta_0}(Z)}>c_\alpha(\theta_0)$ sous $Z \sim \P_{\theta_0}$\petitespace

\underline{Problème :} \petitespace

Il nous faut connaître $c_\alpha(\theta_0)$... On a, comme région de confiance : \petitespace

$$\widehat R_\alpha = \big\{ \theta_0 \in \Theta : \frac{p_{\widehat \theta(Z)}(Z)}{p_{\theta_0}(Z)} \le c_\alpha(\theta_0) \big\} $$


Ainsi, afin de contourner ces difficultés, il peut être utile d'avoir recours à des approximations.\espace

Déterminons maintenant le cadre asymptotique dans lequel nous nous placerons de cette sous-partie. On pose $\cal Z = \cal Z_n = \cal X^n, \quad Z_n = (\Xunan)$, où $\Xunan$ est iid de loi $\P_\theta$ sur $\cal X$.

$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  p_\theta :& \cal X &\mapsto& \R_+ \\
   p_\theta(x_1, \ldots, x_n)& & \mapsto & \produit{i=1}{n} p_\theta(x_i)\end{array}
$$


Avec $p_\theta = \frac{d\P_\theta}{d\mu}$. On a un modèle $\modelstat$, avec $\Theta_0, \Theta_1$ et  $\theta_0 \in \Theta_0$, fixés puis $n \to \infty$.

\petitespace

\Def
On dit qu'une famille $(\phi_n)_{n \ge 1}$ de tests où $\phi_n : \cal X^n \mapsto \{0,1\}$ test de $H_0 : \theta \in \Theta_0$ contre $H_1 : \theta \in \Theta_1$ est \textbf{asymptotiquement de niveau} $\alpha$ si pour tout $\theta_0 \in \Theta_0$ on a :
\begin{align*}
	&\Lim{n \to \infty}\sup \beta_{\phi_n}(\theta_0)\\
	= &\Lim{n \to \infty}\sup \P_{\theta_0}(\phi_n(\Xunan)=1))\le \alpha
\end{align*}
\petitespace

On suppose que $\{\P_\theta : \theta \in \Theta\}$ est un modèle régulier donc $l(x, \theta) = \log(p_\theta(x))$ de classe $\cal C^2$ en $\theta$.
\petitespace

Rappellons aussi que $I(\theta) = \esptheta{\nabla l(X_1,\theta) \nabla l(X_1,\theta)^T} = -\esptheta{\nabla^2 l(X, \theta)}$. On notera ici $l_n(\theta) = \log(p_\theta(\Xunan)) = \somme{i=1}{n}l(X_i, \theta)$, et on a donc $ \EMV \in \arg \Max{\theta \in \Theta}l_n(\theta)$. \petitespace


On pose \textbf{l'hypothèse (H)} :\petitespace

 Il existe une fonction $M : \cal X \mapsto \R_+$ tel que \petitespace

\shift $\bullet$ pour tout $\theta, \theta' \in \Theta, x \in \cal X$, on a :
$$ \norme{\nabla^2l(x, \theta) - \nabla^2l(x, \theta')}_F \le M(x)\norme{\theta-\theta'}_2 \quad \text{( $\norme{.}_F$ est la norme de Frobenius)} $$ 

\shift $\bullet$Pour tout $\theta \in \Theta,\quad \esptheta{M(X)} <+\infty$

\petitespace

\thm
\textbf{Théorème de Wilks}\petitespace

Soit $\modelstat$ un modèle régulier avec $\Theta \subset \R^d$ ouvert et convexe, on suppose \textbf{l'hypothèse H}.

Également, on suppose que  $\EMV \cvP \theta_0$ et $\sqrt n(\EMV-\theta_0) \cvl \normale{0}{I^{-1}(\theta_0)}$. Alors, pour tous $\theta_0 \in \Theta$ sous $\P_{\theta_0}$, avec $\EMV \in \arg \Max{\theta} l_n$, on a

\begin{align*}
	 &2\log(\frac{p_{\EMV}(\Xunan)}{p_{\theta_0}(\Xunan)}) \\
	 &= 2(l_n(\EMV)-l_n(\theta_0) )\cvl \chideux{d}
\end{align*}
\petitespace

En particulier, pour un test asymptotique de niveau $\alpha$ d'adéquation à $\theta_0$ : 

$$\phi_{n,\alpha}(\Xunan) = \1_{\big\{ 2(l_n(\widehat \theta_n)-l_n(\theta_0) )> c_{1-\alpha}(\chideux{d} )\big\}} $$

asymptotique de niveau $\alpha$ pour $H_0 : \theta = \theta_0$.

\petitespace

\corr{\textbf{Preuve :}

%modèle $\modelstat$ régulier, $\Theta \subset \R^d$ ouvert et convexe sur $\cal X, \quad \frac{\d \P_\theta}{d\mu}  = p_\theta, \quad Z_n = (\Xunan)\sim \P_\theta^{\otimes n} $ \petitespace
%
%Par développement de Taylor avec reste intégral en $\EMV$, avec 
%
%$$\phi(t) = l_n(\underbrace{(1-t)\EMV+t\theta_0}_{\widetilde \theta_t}), \widetilde \theta_n, \quad \phi : [0,1] \mapsto \R$$ 
%
%on a
% 
%
%\begin{align*}
%	l_n(\theta_0)-l_n(\EMV) &= <\overbrace{\nabla l_n(\EMV)}^{=0}, \theta_0-\EMV> + \int_0^1 (1-t)<\nabla^2l_n(\widetilde \theta_t)( \theta_0-\EMV),  \theta_0-\EMV> dt\\
%	&=-\frac n2 <I(\EMV-\theta_0), \EMV-\theta_0> + r_n \text{ \shift (1)}
%\end{align*}
%
%
%On va montrer que $r_n \cvl 0$, idée : On approche  $\nabla^2l_n(\widetilde \theta_t)$ par $\nabla^2 l_n(\theta_0) \cvps nI(\theta_0)$
%
%\petitespace
%
%\Rq
%
%Si $A$ matrice symétrique $d\times d$ et $u \in \R^d$, on a 
%
%\begin{align*}
%	|u^TAu| &= |<Au,u>| \\
%	& = |\somme{j,k=1}{d}a_{j,k}u_ju_k|\\
%	\text{(C.S.)}&\le (\somme{j,k=1}{d} a_{j,k}^2)^{\frac 12}(\somme{j,k=1}{d} u_ju_k)^{\frac 12}\\
%	&= (\somme{j,k=1}{d} a_{j,k}^2)^{\frac 12}(\somme{k=1}{d}u_k)^{\frac 12}(\somme{k=1}{d}u_k)^{\frac 12}\\
%	&= \norme{A}_F\norme{u}_2^2 \text{ (*)}
%\end{align*}
%
%
%
%\begin{align*}
%	|r_n|&\le \int_0^1(1-t)\big|\big<[\nabla^2l_n(\widetilde \theta_t) + nI(\theta_0)](\EMV-\theta_0), \EMV-\theta_0 \big> \big|dt \\
%	\text{par (*)}& \le \int_0^1(1-t)\norme{\frac{\nabla^2l_n(\widetilde \theta_t)}{n} + I(\theta_0)}_F n\norme{\EMV-\theta_0}_2^2  dt \text{ \shift (2)}
%\end{align*}
%
%Par inégalité triangulaire, on a 
%
%\begin{align*}
%	\norme{<\frac{\nabla^2l_n(\widetilde \theta_t)}{n} + I(\theta_0)}_F \le \overbrace{\frac 1n \norme{\nabla^2l_n(\widetilde \theta_t)-\nabla^2 l_n(\theta_0)}_F}^{\eta_n(t)} + \underbrace{\norme{\frac{\nabla^2l_n(\widetilde \theta_t)}{n} + I(\theta_0)}_F}_{\epsilon_n}
%\end{align*}
%
%
%
%\begin{align*}
%	\epsilon_n &= \norme{\frac 1n \somme{i=1}{n} \nabla^2 l(\theta_0, X_i) - \bb{E}_{\theta_0}[ \nabla^2 l(\theta_0,X_1)]  }_F \cvps 0 \text{\shift (3)}
%\end{align*}
%
%et 
%
%
%\begin{align*}
%	\eta_n(t) &= \frac 1n \norme{\somme{i=1}{n} \nabla^2 l(\widetilde \theta_t,X_i) - \nabla^2 l(\theta_0,X_i) }_F\\
%	& \le \frac 1n \somme{i=1}{n} \norme{ \nabla^2 l(\widetilde \theta_t,X_i) - \nabla^2 l(\theta_0,X_i) }_F\\
%	&\overbrace{\le}^{(H)} \frac 1n\somme{i=1}{n} M(X_i)\norme{\widetilde \theta_t-\theta_0}_2 
%\end{align*}
%
%Or $\frac 1n \somme{i=1}{n}M(X_i)\cvps \esptheta{M(X)}<+ \infty$ et 
%
%\begin{align*}
%	\norme{\widetilde \theta_t-\theta_0}_2 &=(1-t) \norme{\EMV-\theta_0}_2\\
%	&\le \norme{\EMV-\theta_0}_2 \cvP 0
%\end{align*}
%(Par consistance de l'EMV).\petitespace
%
%Donc, par (2) et (3) on a : 
%
%\begin{align*}
%	|r_n| &\le \int_0^1(1-t)dt (\epsilon_n+  \frac 1n\somme{i=1}{n} M(X_i)\norme{\widetilde \theta_t-\theta_0}_2 ) n\norme{\EMV-\theta_0}^2\\
%	& = \frac 12 \underbrace{(\epsilon_n+  \frac 1n\somme{i=1}{n} M(X_i)\norme{\widetilde \theta_t-\theta_0}_2 )}_{\cvP 0} \norme{\underbrace{\sqrt n (\EMV-\theta_0)}_{\cvl V \sim  \normale{0}{I(\theta_0)^{-1} } } }^2_2
%\end{align*}
%
%Ce qui montre que $r_n \cvl 0$ et donc $r_n \cvP 0$
%
%Par (1) on a donc 
%
%\begin{align*}
%	2(l_n(\EMV)-l_n(\theta_0)) &= n <I(\EMV-\theta_0), \EMV-\theta_0> -2r_n\\
%	&= (\sqrt n (\EMV-\theta_0))^TI(\sqrt n(\EMV-\theta_0) ) - 2r_n\\
%\end{align*}
%
%
%Notons $I_0 = I(\theta_0)$. Par normalité asymptotique de $\EMV$, on a 
%
%$$\sqrt n (\EMV-\theta_0) \cvl V \sim \normale{0}{I_0^{-1}}$$
%
%$V = I_0^{-\frac 12}W, \quad W \sim \normale{}{I_d}$
%
%
%\begin{align*}
%	2(l_n(\EMV)-l_n(\theta_0) )\cvl &(I_0^{-\frac 12}W)^TI(I_0^{-\frac 12}W)\\
%	&= W^T I_0^{-\frac 12} I_0 I_0^{-\frac 12}W\\
%	& = W^TW\\
%	& = \norme{W}^2_2 \sim \chideux{d}
%\end{align*}

%AUTRE PREUVE 

Posons

$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \phi :& [0,1] &\mapsto& \R\\
  & t & \mapsto &  l_n((1-t)\EMV+t\theta_0) \end{array}
$$

Comme le modèle est régulier, $l_n$ est $\cal C^2(\Theta)$ donc $\phi$ l'est aussi. Ainsi, par théorème de Taylor Lagrange, il existe $\widetilde \theta_n \in [\theta_0, \EMV]$ (il existe donc $t\in [0,1],\quad  \widetilde \theta_n = t\EMV+(1-t)\theta_0$)  tel que 

\begin{align*}
	&\phi(1) = \phi(0) + \phi'(0) + \frac 12 \phi''(\widetilde \theta_n) \\
	& \iff l_n(\theta_0)-l_n(\EMV) = <\overbrace{\nabla l_n(\EMV)}^{=0 }, \theta_0-\EMV> + \frac 12 (\theta-\EMV)^T\nabla^2l_n(\widetilde \theta_n)  (\theta-\EMV)\\
	&\iff l_n(\EMV) -  l_n(\theta_0) =  \frac 12 (\EMV-\theta_0)^T\big(-\nabla^2l_n(\widetilde \theta_n)\big)(\EMV-\theta_0) \\
\end{align*}

Mais on sait que l'on a $\sqrt n (\EMV-\theta_0) \cvl \cal N_d(0, I^{-1}(\theta_0))$, avec $I(\theta_0)$ l'information de Fisher, équivalent à 

$$\sqrt n I^{\frac 12}(\theta_0)(\EMV-\theta_0) \cvl \cal N_d(0, I_d)$$

donc, par Cochran on a 

$$\norme{\sqrt n I^{\frac 12}(\theta_0)(\EMV-\theta_0)}_2^2 = n(\EMV-\theta_0)^TI(\theta_0) (\EMV-\theta_0) \cvl \chideux{d} $$

Mais, on peut aussi écrire 

$$ (\EMV-\theta_0)^T[-\nabla^2l_n(\widetilde \theta_n)] (\EMV-\theta_0) = n(\EMV-\theta_0)^T\frac 1n [-\nabla^2l_n(\widetilde \theta_n)] (\EMV-\theta_0)$$

Or, en rappelant que l'estimateur du maximum de vraisemblance est consistant, on a

\begin{align*}
	\frac 1n \norme{-\nabla^2l_n(\widetilde \theta_n) - (-\nabla^2l_n(\theta_0))}_F &=  \frac 1n \norme{\somme{i=1}{n}\nabla^2l(X_i,\widetilde \theta_n) - \nabla^2l(X_i, \theta_0)}_F  \\
	&\le \frac 1n\somme{i=1}{n} \norme{\nabla^2l(X_i, \widetilde \theta_n) - \nabla^2l(X_i, \theta_0)}_F \\
	& \overbrace{\le}^{(H)} \frac 1n \somme{i=1}{n} M(X_i)\norme{\widetilde \theta_n-\theta_0}_2 \\
	&\le |t| \underbrace{\norme{\EMV-\theta_0}_2}_{\cvP 0} \underbrace{\frac 1n \somme{i=1}{n} M(X_i)}_{\cvP \esp{M(X_1)} < \infty} \cvP 0\\
\end{align*}

Comme $- \frac{1}{n} \nabla^2l_n(\theta_0) \cvP \esp{- \nabla^2l(\theta_0,X_1)} = I(\theta_0)$ on a donc bien que $-\frac{1}{n}\nabla^2l_n(\widetilde \theta_n) \cvP I(\theta_0)$ aussi.  

\begin{align*}
	2(l_n(\EMV) -  l_n(\theta_0))&= (\EMV-\theta_0)^T\big(-\nabla^2l_n(\widetilde \theta_n)\big)(\EMV-\theta_0)\\
	&= n(\EMV-\theta_0)^T\big(\underbrace{-\frac 1n \nabla^2l_n(\widetilde \theta_n)}_{\cvP I(\theta_0)}\big)(\EMV-\theta_0) \underbrace{\cvl}_{\text{(Slutsky)}} \chideux{d}
\end{align*}
\petitespace}


\Rq

 Test asymptotiques d'adéquation à $\theta_0 : \quad \phi_\alpha^{(\theta_0)}(\Xunan) = \1_{\big\{ 2(l_n(\EMV)-l_n(\theta_0) ) > q_{1-\alpha}(\chideux{d}) \big\}}$
 
 
 Région de confiance : $1-\alpha \quad \widehat C_\alpha = \big\{ \theta_0 \in \Theta : l_n(\EMV)-l_n(\theta_0) \le \frac{q_{1-\alpha}}{2}  \big\}$
  
  \espace

\subsubsection{Tests de Wald}\espace

\Def

On définit le \textbf{test de Wald I} 

$$\phi_\alpha(\Xunan) = \1_{\{ T_1>q_{1-\alpha}(\chideux{d})  \}}$$

Avec $$T_1 = n\big<I(\theta_0)(\EMV-\theta_0), \EMV-\theta_0\big> \cvl \chideux{d} \text{ (1)}$$  


Et le \textbf{test Wald II} par 

$$\phi'_\alpha(\Xunan) = \1_{\{ T_2>q_{1-\alpha}(\chideux{d})  \}}$$

avec 

$$T_2 = n<I(\EMV)(\EMV-\theta_0), (\EMV-\theta_0)> \cvl \chideux{d} \text{ (2)} $$

\petitespace

\Prop 
Ces deux tests sont asymptotiquement de niveau $\alpha$, on a \petitespace

\shift $\bullet$ Pour (1), vu en preuve du théorème de Wilks. \petitespace

\shift $\bullet$ Pour (2) On utilise (1) et le fait que $\norme{I(\EMV)-I(\theta_0)}_F \cvP 0$ ce qui démontre le résultat attendu. 

\espace


Le test de Wald II est utile pour déterminer des régions de confiance 

$$T_2 = \norme{I(\EMV)^{\frac 12} \sqrt n (\EMV-\theta_0)} \le q_{1-\alpha}(\chideux{d}) =q_{1-\alpha}$$\petitespace

si, et seulement si $I(\EMV)^{\frac 12} \sqrt n (\EMV-\theta_0) \in B(0, q_{1-\alpha}) = q_{1-\alpha}B_2$ avec $B_2$ la boule unité de norme 2\espace

si, et seulement si $\theta_0 \in \underbrace{\EMV + \frac{I(\EMV)^{- \frac 12}}{\sqrt n}q_{1-\alpha}B_2}_{\widehat C_{1-\alpha}  \text{ ellipsoide centre en $\EMV$}} $.

\Rq

Sous $\P_{\theta_0}$, en posant $T_0 = 2(l_n(\EMV)-l_n(\theta_0))$ on a \espace

\begin{equation*}
|T_0-T_1| \cvP 0  \quad\quad\quad  |T_1-T_2| \cvP 0 \text{ \shift sous $H_0 : \theta=\theta_0$}
\end{equation*}


Ces trois tests (wilks, Wald I, Wald II), sont asymptotiquement équivalents sous $H_0 : \theta = \theta_0$, mais pas forcément sous $H_1 : \theta \ne \theta_0$. C'est pareil pour leurs régions de confiance associées d'ailleurs. \petitespace

Par exemple, si $\theta \ne \theta_0$, sous $\P_\theta$, 

$$T_2 \approx n <I(\underbrace{\theta}_{\text{pas } \theta_0})(\EMV-\theta_0), \EMV-\theta_0> \approx n< I(\theta)(\theta-\theta_0), \theta-\theta_0 > $$

\espace


\subsubsection{Lois Discrètes : Test du $\chi^2$}\espace


Soient $d \ge 1$ et $\cal X = \{0,1, \ldots, d \}$. On pose $\Theta = \{ (\theta_1, \ldots, \theta_d)\in \R^*_+ \mid \somme{i=1}{d} \theta_i <1  \} $. Pour tout $\theta \in \Theta$, on note $P_\theta$ la loi de probabilité sur $\cal X$ définie par :

$$\forall j=1, \ldots,d, \quad P_\theta(\{ j\} )= \theta_j \quad \text{et} \quad P_\theta(\{ 0\} )=1-\somme{j=1}{d} \theta_j$$

\petitespace


\textbf{Notations :} Pour tout $\theta, \theta' \in \Theta,$ on pose $\chi^2(\theta, \theta') := \somme{j=0}{d}\frac{\theta_j-\theta'_j}{\theta_j}$\petitespace

\Prop

 Pour tout $\theta, \theta' \in \Theta =\{ (\theta_1, \ldots, \theta_d)\in \R^*_+ \mid \somme{i=1}{d} \theta_i <1  \} $ si on note $P_\theta$ la mesure de probabilité $\somme{i=0}{d} \theta_i \delta_i$ sur $\cal X = \ldbrack 1,d \rdbrack$ alors on a :
 
 $$\norme{I(\theta)^{\frac 12} (\theta'-\theta)}_2^2 = \chi^2(\theta, \theta')$$
 
 \petitespace
 
 \corr{\textbf{Preuve :}
 
 On a donc la densité de $P_\theta$ qui est $p_\theta(x) = \somme{i=0}{d} \theta_i \1_{x=i}, \forall x \in \ldbrack 1,d \rdbrack$. 
 
on sait que $l(x, \theta) = \log\big(\somme{i=0}{d}  \theta_i \1_{x=i}\big) = \log\big((1-\somme{j=1}{d} \theta_j)\1_{x=0} + \somme{i=1}{d}  \theta_i \1_{x=i} \big)$
 
 Donc on a, pour $k \in  \ldbrack 1,d \rdbrack, \quad \frac{\d l(x,\theta)}{\d\theta_k} = \frac{-\1_{x=0} + \1_{x=k}}{\somme{i=0}{d}  \theta_i \1_{x=i}}$
 
 
 Donc, pour $j,k \in  \ldbrack 1,d \rdbrack$ et $X \sim P_\theta$, on a :
 
 \begin{align*}
 	I(\theta)_{j,k} &= \esptheta{(\frac{\d l(X,\theta)}{\d\theta})_j (\frac{\d l(X,\theta)}{\d\theta})^T_k} \\
	&=\esptheta{\frac{1}{(\somme{i=0}{d}  \theta_i \1_{X=i})^2} (\1_{X=j} -\1_{X=0})(\1_{X=k}-\1_{X=0}  ) }\\
	&=\frac{ P_\theta(X=j)\1_{j=k}}{\theta_j^2} - \frac{\theta_0}{\theta_0^2} \1_{j=0} - \frac{\theta_0}{\theta_0^2} \1_{k=0} + \frac{\theta_0}{\theta_0^2} \\
	&= \frac{\1_{j=k}}{\theta_j}  - \frac{\1_{j=0}}{\theta_0}  - \frac{\1_{k=0} }{\theta_0} + \frac 1\theta_0 \\
	&=  \frac{\1_{j=k}}{\theta_j} + \frac 1\theta_0\quad  \text{ si $k \ne 0, j \ne 0$}
 \end{align*}
 
 Donc, en remarquant que $I(\theta)_{0,0} = 0$, on a :
 
  \begin{align*}
 	\norme{I(\theta)^{\frac 12} (\theta'-\theta)}_2^2& =  (\theta'-\theta)^TI(\theta)  (\theta'-\theta)\\
	&= \somme{k,j=0}{d} I(\theta)_{k,j}(\theta'_j-\theta_j)(\theta_k'-\theta_k) \\
	& = \frac 1\theta_0 \somme{j \ne k, j,k >0 }{d}(\theta'_j-\theta_j)(\theta_k'-\theta_k) + \somme{k=1}{d}(\frac 1\theta_k + \frac 1\theta_0)(\theta_k'-\theta_k)^2\\
	&= \frac 1\theta_0 \somme{ j,k =1}{d} (\theta'_j-\theta_j)(\theta_k'-\theta_k) + \somme{k=1}{d} \frac{(\theta_k'-\theta_k)^2}{\theta_k} \\
	&=  \frac 1\theta_0((1-\theta_0' - (1-\theta_0)((1-\theta_0' - (1-\theta_0))) + \somme{k=1}{d} \frac{(\theta_k'-\theta_k)^2}{\theta_k} \\
	& = \somme{k=0}{d} \frac{(\theta_k'-\theta_k)^2}{\theta_k} \\
  \end{align*} 
  }
\textbf{Conséquences :} Si $\Xunan \simiid P_\theta$, le test de Wald I de $H_0 : \theta = \theta^*$ contre $H_1 : \theta \ne \theta^*$ s'écrit en posant $\theta^*= (\theta_1, \ldots, \theta_d)$ et en supposant que les conditions de Wilks soient respectées :

$$\phi_{n,\alpha}(\Xunan) = \1_{\big\{ n\somme{j=0}{d}\frac{\widehat \theta^{MV}_{j,n} -\theta^*_j}{\theta^*_j} > q_{1-\alpha}(\chideux{d})\big\} }\quad\quad\quad\quad\quad  \text{(1)}$$  

où $\EMV = (\widehat \theta^{MV}_{1,n}, \ldots, \widehat \theta^{MV}_{d,n})$. On vérifie que 

$$\widehat \theta^{MV}_{j,n} = \frac 1n \somme{j=0}{d}\1_{\{X_i=j\}} $$

\ie \shift $\widehat \theta^{MV}_{j,n}$ est la fréquence de la classe $j$ dans l'échantillon $(\Xunan)$.\petitespace

Le test (1) est \textbf{le test du $\chi^2$}. Par la propriété précédente, il est asymptotiquement de niveau $\alpha$.

\espace

%\subsection{Garanties non-asymptotiques et non-paramétriques}\espace
%
%Dans le chapitre précédent, les résultats étaient valides sous deux conditions :\petitespace
%
%\shift 1) \textbf{Modèles paramétriques} : la loi commune des variables $\Xunan$ appartient à un modèle $\modelstat$ connu et de dimension finie $\Theta \subset \R^d, \quad d \ge 1$. \petitespace
%
%\shift 2) \textbf{Garanties asymptotiques} :  la taille $n$ de l’échantillon tend vers l’infini.\petitespace
%
%Ces deux conditions sont restrictives, et on peut chercher à les relacher. Cela conduit à chercher des garanties non-paramétriques (ne supposant pas de connaissance forte a priori sur la forme de la loi des données) et non-asymptotiques (valides pour une taille d’échantillon $n$ finie).
%
%\espace
%
%\subsubsection{Inégalités de concentration pour les sommes de variables aléatoires. i.i.d. }\espace
%
%\textbf{Modélisation}. On considère $\Xunan$ des variables aléatoires réelles, i.i.d. de loi $P$ sur $\R$. On cherche à obtenir des intervalles de confiance sur $\mu := \esp{X_1}$. On estime $\mu$ par $\Xbarre = \frac 1n \somme{i=1}{n}X_i$. Afin de construire un intervalle de confiance, on doit quantifier l’erreur $|\Xbarre-\mu|$.
%
%
%Par Tchebychev, on a pour toute variable aléatoire de carré intégrable ($\esp{X_1^2} = \sigma^2 < \infty$), 
%
%$$P(|\Xbarre-\mu| \ge t) \le \frac{\sigma^2}{nt^2} $$
%
%Si l’on souhaite un intervalle de confiance pour $\mu$ au niveau $\alpha\in (0,1)$, il suffit de prendre $t = \frac{\sigma}{\sqrt{n\alpha }}$.
%
%
%En pratique, la dépendance en $\sigma$ et $n$ est optimale au vu du TCL. Mais la dépendance en $\alpha$ est moins bonne. En effet, le TCL donne, pour tout $u>0$
%
%$$P(\sqrt n \frac{|\Xbarre-\mu|}{\sigma}\ge u) \cvn P(|Z| \ge u), \quad\quad Z \sim \normale{0}{1}$$
%
%Or :
%
%$$P(|Z| \ge u) \underset{u \to +\infty}{\sim} \frac{2}{\sqrt{2\pi }}\frac{e^{ \frac{-u^2}{2} } }{u} \underbrace{\uparrow}_{\text{Informellement}} \approx e^{-u^2/2}$$
%
%Et $e^{-u^2/2} = \alpha $ en choisissant $u = \sqrt{2\log(1/\alpha)}$. Et donc plus $n$ grandit, plus l'intervalle de confiance est resserré (est meilleur). Ceci suggère que des améliorations de Chebychev sont possibles.
%\espace


\subsection{Méthodes non Asymptotiques et Non Paramétriques}
\espace

%\textbf{Lemme (Hoeffding)}
%
%Si $a \le X \le b$ p.s.. Alors, $X$ est $\frac{(b-a)^2}{4}$ sous-gaussiens.
%
%\petitespace
%
%\corr{\textbf{Preuve :}
%
%On suppose $0 \overset{p.s.}{\le} X \overset{p.s.}{\le} 1$ et on pose pour tout $\lambda \in \R$, $\phi(\lambda) =\log(\esp{e^{\lambda X}})$. On a $\phi'(\lambda) = \frac{\esp{X e^{\lambda X}}}{\esp{e^{\lambda X}}}$ et
%
%\begin{align*}
%	\phi''(\lambda) &= \frac{\esp{X^2 e^{\lambda X}}}{\esp{e^{\lambda X}}} - \esp{X e^{\lambda X}}\frac{\esp{Xe^{\lambda X}}}{\esp{e^{\lambda X}}^2}\\
%	&=  \frac{\esp{X^2 e^{\lambda X}}}{\esp{e^{\lambda X}}} - \big(\frac{\esp{Xe^{\lambda X}}}{\esp{e^{\lambda X}}}\big)^2 \\
%	&\le \frac{\esp{X e^{\lambda X}}}{\esp{e^{\lambda X}}}  - \big(\frac{\esp{Xe^{\lambda X}}}{\esp{e^{\lambda X}}}\big)^2 \quad \text{(car $0 \overset{p.s.}{\le} X \overset{p.s.}{\le} 1\implies X^2 \overset{p.s.}{\le} X$)}\\
%	&=u-u^2 = u1-u) \le \frac 14
%\end{align*}
%
% Donc, $\phi''(\lambda) \le \frac 14$. De plus, $\phi(0) = 0$ et  $\phi'(0) = \esp X$ donc on a par Taylor, que pour tout $\lambda \in \R$, 
% 
% $$\phi(\lambda) \le \phi(0) + \phi'(0)\lambda + \frac 14 \frac{\lambda^2}{2} = \lambda \esp{X} + \frac{\lambda^2}{8} $$ 
% 
% Ainsi, $\esp{\exp(\lambda(X-\esp X) )} \le \exp(\frac{\lambda^2}{4} )$
%\petitespace}
%
%\underline{Application de l'inégalité d'Hoeffding}\petitespace
%
%Version quantitative de la loi forte des grands nombres pour les fréquences empiriques. Soit $Z_1, \ldots ,Z_n, Z$ des variables aléatoires iid de loi $P$ sur un espace mesurable $(\cal Z, \cal B)$ et $A \in \cal B$. On prend $X = \1_{\{Z \in A\}}, X_i = \1_{\{Z_i \in A\}} $. $\esp X = P(Z \in A)$

On se place, dans cette cette partie, dans le cadre $\Xunan, X$ iid de loi $P$ sur $\R$ et $F(x) = P( (-\infty,x] ) = P(X \le x)$ 


\subsubsection{Bandes de Confiance Uniformes sur la Fonction de Répartition}\petitespace

Par Hoeffding, on a pour tout $x \in \R$ et $t \in \R_+$

$$P(|\widehat F_n(x)-F(x)| \ge t)\le 2e^{-2nt^2} $$

On a en fait un résultat plus fort (uniforme en $x$) 
\petitespace



\thm
\textbf{Inégalité de Dvoretzky-Kiffer-Wolfowitz (DKW)}\petitespace

On a pour tout $t \in \R_+$, 

$$P(\Sup{x \in \R}|\widehat F_n(x)-F(x)| \ge t) \le 2e^{-2nt^2}  $$

$\Sup{x \in \R}|\widehat F_n(x)-F(x)| = || \widehat F_n-F||_\infty$ On a une "bande de confiance" sur $F$. Avec probabilité $1-\alpha$, on a pour tout $x \in \R$, $F(x) \in [\widehat F_n(x)- \sqrt{\frac{\log(2/\alpha)}{2n}}, \widehat F_n(x) + \sqrt{\frac{\log(2/\alpha)}{2n}}]$. La preuve est cependant hors de portée de ce cours.
\petitespace


\Prop

Supposons que $P$ n'admette pas d'atome (\ie : $F$ est continue). Soient $U_1, \ldots, U_n \simiid \cal U([0,1])$. Alors, 

$$\Sup{x \in \R}|\widehat F_n(x) -F(x) | \overset{(d)}{=} \Sup{t \in [0,1]}|\frac 1n \somme{i=1}{n} \1_{U_i \le t }-t| $$

%\cercler 2Si $P$ n'a pas d'atomes, alors $F(X) \sim \cal U([0,1])$.

\petitespace

\corr{\textbf{Preuve :}

On pose $F : \R \mapsto (0,1)$ strictement croissante et bijective

$$\Sup{x \in \R}|\widehat F_n(x) -F(x)| = \Sup{t = F^{-1}(x), t \in (0,1)}|\widehat F_n(F^{-1}(t)) - \underbrace{F(F^{-1}(t))}_{t} | $$

$\widehat F_n(F^{-1}(t)) = \frac 1n \somme{i=1}{n} \1_{X_i \le F^{-1}(t)} = \frac 1n \somme{i=1}{n} \1_{F(X_i) \le t}$. Donc 


$$\Sup{x \in \R}|\widehat F_n(x) -F(x)| = \Sup{t \in (0,1)}|\frac 1n\somme{i=1}{n} \1_{F(X_i)\le t }-t| $$

Or, on a déjà démontré que si $F$ est continue (ce qui est le cas car $P$ n'a pas d'atome), alors  $F(X_1), \ldots, F(X_n) \simiid \cal U([0,1]) $. 

%\petitespace
%
%
%\cercler 2 On veut montrer que pour tout $t \in (0,1)$, $P(F(X) \le t) = t$. Comme $F$ est continue, il existe $x \in \R$ tel que $F(x)=t$. Comme $F$ est croissante, si $X \le x$ alors $F(X) \le F(x) =t$ donc 
%
%$$P(F(X) \ge P(X \le x- = F(x) = t $$
%
%Soit $u >t$.  Par le même argument, il existe $y \in \R$ tel que $F(y) = u$. Si $X>y$, alors $F(X) \ge F(y) =u>t$
%
%Donc si $F(X) \le t$ alors $X \le y$, donc 
%
%$$P(F(X) \le t)\le P(X \le y)=F(y)=u $$
%
%Comme $u >t$ est arbitraire, on a $P(F(X) \le t) \le t$
\petitespace}

\Rq 

DKW pour tout $n \ge 1$ et $t \in \R_+$, nous donne :

$$P(\sqrt n \Sup{x \in \R}|\widehat F_n(x)-F(x)| \ge t)\le 2e^{-2t^2}$$
	
Quand $n \to \infty$ on a aussi :

$$\sqrt n \Sup{x \in \R}|\widehat F_n(x)-F(x)| \cvl \Sup{u \in [0,1]}|B_n| $$

où $(B_u)_{u \in [0,1] }$ est un pont gaussien : \petitespace

Un \textbf{processus gaussien} est un processus tel que : \petitespace

\shift$\bullet$ $\forall N, \forall u_1 , \ldots, u_N, (B_{u_1}, \ldots, B_{u_N})$ vecteur gaussien

\petitespace
	
\shift$\bullet$  $\esp{B_n} = 0\quad  \forall u$ et $\esp{B_uB_v} = \min(u,v)-uv$\espace

\subsubsection{Test de Permutation}
\espace



On considère le vecteur $(\Xunan)\sim P_n$ sur $\cal X^n$, et on pose l'hypothèse nulle : les observations $\Xunan$ sont iid. C'est-à-dire qu'il existe une loi $P$ sur $\cal X$ tel que $P_n = P^{\otimes n}$

\petitespace

\underline{Exemples} :\petitespace

\textbf{Détection d'anomalie} : $\Xunan$ iid et on veut tester si $X_n$ est indépendante de $X_1, \ldots, X_{n-1}$ et de même loi. Cela revient à dire que $\Xunan$ sont iid. Dans le cas contraire, on dit que $X_n$ est une anomalie.

\petitespace

\petitespace
	
\textbf{Test d'égalité de loi de deux populations :} $\Xunan\simiid P$ sur $\cal X$ et $Y_1, \ldots, Y_n \simiid Q$ sur $\cal X$ et


$\Xunan \indep  Y_1, \ldots, Y_n$. On veut tester si $P=Q$ (l'hypothèse nulle). Cela revient à dire que  $(\Xunan, Y_1, \ldots, Y_n)$ est iid de loi $P^{\otimes n}\otimes Q^{\otimes n}$

\petitespace

 \petitespace

\textbf{Test de signal} : On pose $x_1, \ldots, x_n \in \cal X$ fixés (covariables) et $Y_1, \ldots, Y_n$ des variables réelles réponses. On suppose aussi qu'il existe $\sigma>0$ et une fonction $f : \cal X \mapsto \R$ telle que $Y_i =f(x_i)+\epsilon_i$ où $\epsilon_1, \ldots, \epsilon_n \simiid \normale{0}{\sigma^2}$. On veut tester s'il y a du signal. Hypothèse nulle $H_0$ : "pas d'information/signal" \ie: La fonction de régression $f: \cal X \mapsto \R$ est \textbf{constante}. Dans ce cas, les observations $Y_1, \ldots Y_n$ sont iid. \petitespace



\textbf{Difficulté :} la loi commune  (éventuelle) $P$ de $\Xunan$ peut être arbitraire. Il peut être difficile d'estimer $P$.

\espace

On observe que si $\Xunan$ sont iid, alors, la loi de $(\Xunan)$ est \textbf{invariante par permutation} au sens où pour toute permutation (bijection) $\boldsymbol{\sigma}$ de $\{1, \ldots, n\}$, 

$$(X_{\sigma(1)}, \ldots, X_{\sigma(n)}) \overset{(d)}{=} (\Xunan) \sim P^{\otimes n} $$

\espace

\Def
\textbf{tests de permutations}\petitespace



Soit $F : \cal X^n \mapsto \R$, $Z=(\Xunan)$ une fonction mesurable. Pour toute $\boldsymbol{\sigma} \in \cal \mathfrak{S}_n$, $Z^{\boldsymbol{\sigma}}  = (X_{\boldsymbol{\sigma}(1)}, \ldots, X_{\boldsymbol{\sigma}(n)})$ avec $ \mathfrak{S}_n = \big\{\text{permutations de} \ldbrack 1, n\rdbrack \big\}$, et donc $\#\mathfrak{S}_n = n!$. On note, pour toute $\boldsymbol{\sigma}  \in \mathfrak{S}_n$, $R(\boldsymbol{\sigma} ) = \tilde R(\boldsymbol{\sigma},Z)=\somme{ \tau \in \sigma_n}{}\1_{\big\{F(Z^\tau) \le F(Z^{\boldsymbol{\sigma} })\big\}}$ \petitespace

Soit id la fonction permutation identité tel que $Z^{id}=Z$ et prenons $\alpha\in (0,1)$. Le \textbf{test par permutation} de niveau $\alpha$ correspondant à la statistique de test $F$ est le test de $H_0$ : $\Xunan$ iid est définit par :

$$\phi_\alpha(Z) = \1_{\big\{R(id) \le \alpha n!  \big\}} $$



\petitespace

\thm 
Sous $H_0 : \Xunan$ est iid, si on suppose que les valeurs $(F(Z^{\boldsymbol{\sigma}}))_{\boldsymbol{\sigma} \in \mathfrak{S}_n}$ sont des réels 2 à 2 distincts, alors le \textbf{test par permutation} est de niveau $\alpha$ \ie

$$P(\phi_\alpha(Z)=1)\le \alpha$$



\petitespace

\corr{\textbf{Preuve :}

\cercler 1 On commence par montrer que pour toute permutation $\boldsymbol{\sigma}$, $R(\boldsymbol{\sigma}) \overset{(d)}{=}R(id)$.

Si $\boldsymbol{\sigma}, \tau \in \sigma_n$, $[(Z^{\boldsymbol{\sigma}})^\tau]_i = (Z^{\boldsymbol{\sigma}})_{\tau(i)} = Z_{\boldsymbol{\sigma}(\tau(i))} = Z_{\boldsymbol{\sigma} \circ \tau (i)}$ (*).

Donc $(Z^{\boldsymbol{\sigma}})^\tau = Z^{\boldsymbol{\sigma} \circ \tau}$, $\tilde R(\boldsymbol{\sigma}, Z) = \somme{\tau \in \sigma_n}{}\1_{F(Z^\tau) \le F(Z^{\boldsymbol{\sigma}})}$ or, sous $H_0$, on a $Z\overset{(d)}{=}Z^\tau$, donc $\tilde R(\boldsymbol{\sigma},Z) \overset{(d)}{=} \tilde R(\boldsymbol{\sigma}, Z^\tau)$. Ainsi, pour $\boldsymbol{\sigma}'$ une autre permutation, on a :
 

\begin{align*}
	&R(\boldsymbol{\sigma}) = \tilde R(\boldsymbol{\sigma}, Z) \overset{(d)}{=}\tilde R(\boldsymbol{\sigma}, Z^\tau) =  \somme{\boldsymbol{\sigma'}  \in \mathfrak{S}_n}{}\1_{F((Z^\tau)^{\boldsymbol{\sigma}'}) \le F((Z^\tau)^{\boldsymbol{\sigma}})} 
\\ 
	&\underbrace{=}_{\text{par *} } \somme{\boldsymbol{\sigma} ' \in \mathfrak{S}_n}{} \1_{F(Z^{ \tau\circ \boldsymbol{\sigma} '}) \le F(Z^{ \tau\circ \boldsymbol{\sigma})}} = \somme{\sigma'' \in \mathfrak{S}_n}{} \1_{F(Z^{\sigma''}) \le F(Z^{\tau \circ \boldsymbol{\sigma}} )} \\
	& = \tilde R(\tau \circ \boldsymbol{\sigma}, Z) =\tilde R(\tau \circ \boldsymbol{\sigma})
\end{align*}

Ainsi, $R(\boldsymbol{\sigma})\overset{(d)}{=} \tilde R(\tau \circ \boldsymbol{\sigma})$ pour tout $\tau$. On prend $\tau = \boldsymbol{\sigma}^{-1}$, on conclut que $R(\boldsymbol{\sigma}) \overset{(d)}{=} R(id)$.

\petitespace


\cercler{2} Ensuite, on pose $\boldsymbol{\sigma} \sim \cal U(\mathfrak{S}_n)$ \textbf{indépendante de $Z$}, et on montre que $R(\boldsymbol{\sigma}) \overset{(d)}{=}R(id)$. \petitespace

Pour tout $A \subset \ldbrack 1,n \rdbrack,$ on a 

\begin{align*}
	P(R(\boldsymbol{\sigma}) \in A) &= \somme{\tau \in \mathfrak{S}_n}{}P(R(\boldsymbol{\sigma}) \in A, \boldsymbol{\sigma} = \tau)\\
	&\underbrace{=}_{\boldsymbol{\sigma} \indep Z}\somme{\tau \in \mathfrak{S}_n}{}\underbrace{P(\tilde R(\tau, Z) \in A)}_{= P(R(id, Z) \in A)}\underbrace{P(\boldsymbol{\sigma}=\tau)}_{1/n!}\\
	&= P(R(id) \in A)
\end{align*}

Donc $R(\boldsymbol{\sigma}) \overset{(d)}{=}R(id)$.
\petitespace



\cercler 3 On rappelle que les valeurs $(F(Z^{\boldsymbol{\sigma}}))_{\boldsymbol{\sigma} \in \mathfrak{S}_n}$ sont des réels 2 à 2 distincts, alors, si $\boldsymbol{\sigma} \sim \cal U(\mathfrak{S}_n)$ indépendante de $Z$, on a $R(\boldsymbol{\sigma}) \sim \cal U(\ldbrack 1, n! \rdbrack)$   

\petitespace


L'application  $$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  \Phi_Z :& \mathfrak{S}_n &\mapsto& \ldbrack 1,n! \rdbrack\\
  & \boldsymbol{\tau} & \mapsto & \tilde R(\boldsymbol{\tau}, Z) \end{array}
$$

 est une bijection. En effet, les éléments $F(Z^{\boldsymbol{\tau}})$ sont 2 à 2 distincts, donc c'est aussi le cas pour les $R(\boldsymbol{\tau})$, on a donc l'injection. Mais également, comme $\# \mathfrak{S}_n = \#\ldbrack 1, n! \rdbrack$, on a que $\phi_Z$ est injective d'un espace de cardinal $\#\mathfrak{S}_n = \# \ldbrack 1,n!\rdbrack =n!$ dans $ \ldbrack 1,n!\rdbrack $ (de cardinal $n!$)  et donc est surjective. Par conséquent, on a la bijection. Ainsi :

\begin{align*}
	P(R(\boldsymbol{\sigma}) = j) &=P(\tilde R(\boldsymbol{\sigma}, Z)=j) \\
	&= P(\boldsymbol{\sigma} = \Phi_Z^{-1}(j))\\
	&= \somme{\boldsymbol{\tau} \in \mathfrak{S}_n}{}\underbrace{P(\boldsymbol{\sigma} = \boldsymbol{\tau})}_{=\frac{1}{n!}}P( \Phi_Z^{-1}(j) =  \boldsymbol{\tau} )\\
	&\underbrace{=}_{Z \indep \boldsymbol{\sigma} } \frac{1}{n!}
\end{align*}


Conclusion : $R(id, Z) \sim \cal U(\ldbrack 1, n! \rdbrack )$ donc $P(R(id, Z) \le \alpha n!) = \frac{\lfloor \alpha n! \rfloor}{n!}$

Ainsi, $R(\boldsymbol{\sigma}) \overset{(d)}{=}R(\tau \circ \boldsymbol{\sigma})$ pour tout $\tau\in \mathfrak{S}_n$. On prend $\tau = \boldsymbol{\sigma}^{-1}$, on conclut que 

$$R(\boldsymbol{\sigma})\overset{(d)}{=}R(id) $$
Ainsi, sous $H_0$ on a bien 

$$P(\phi_\alpha(Z) = 1) = P(R(id) \le \alpha n!) = \frac{\lfloor \alpha n! \rfloor}{n!} = \alpha \frac{\lfloor \alpha n! \rfloor}{\alpha n!} \le \alpha$$
}


\Rq

Il n'existe à priori pas d'application $F$ fonctionnant à tout les coups (c'est-à-dire respectant $(F(Z^{\boldsymbol{\sigma}}))_{\boldsymbol{\sigma} \in \sigma_n}$ sont des réels 2 à 2 distincts) pour tout échantillon $\Xunan$. 
En revanche, si les $\Xunan$ admettent une densité par rapport à la mesure de Lebesgue, on peut prendre 

$$\begin{array}{rccl} F : &\cal Z &\mapsto& \R \\ & \bf X & \mapsto & \somme{i=1}{n}ix_i \end{array}$$

Avec donc $(F(Z^{\boldsymbol{\sigma}}))_{\boldsymbol{\sigma} \in \mathfrak{S}_n}$ étant presque sûrement des valeurs distinctes. En effet, puisque $\ker(F) = \{ \bf x \in \R^n \mid \somme{i=1}{n}i x_i = 0\}$ est un hyperplan de mesure de Lebesgue nulle. pour tout $\boldsymbol{\sigma}, \boldsymbol{\tau} \in \mathfrak{S}_n$ différents, on a $\somme{i=1}{n}iX_{\boldsymbol{\sigma}(i)} \overset{\text{p.s.}}{\ne} \somme{i=1}{n}iX_{\boldsymbol{\tau}(i)} $.

\subsection{Test d'Hypothèses Multiples}

\espace


Dans certaines applications, on peut être intéressé à effectuer simultanément plusieurs tests sur la loi des données. \petitespace

\underline{Exemple} :\petitespace

\textbf{régression univariée :} $i = 1, \ldots, n, \quad Y_i = \beta_1 X_{i,1} + \ldots + \beta_dX_{i,d} + \epsilon_i, \quad \epsilon_i \simiid \normale{0}{\sigma^2}$

Pour chaque $j = 1, \ldots, d$ on test $H_{0,j} : \beta_j = 0$ dontre $H_{1,j} : \beta_j \ne 0$. Formellement, étant donné le modèle $\modelstat$ sur $Z$ dans $\cal Z$. Pour $i = 1, \ldots, m$, on considère un test $H_{0,i} : \theta\in \Theta_{0,i}$ contre $H_{1,i}:\theta \in \Theta_{1,i}$\petitespace

\Def
Un \textbf{test multiple} est une fonction $R:\cal Z \mapsto \cal P(\{1, \ldots, m \} )$, où $R(Z)$ est l'ensemble des indices $i = 1, \ldots , m$ pour lesquelles on \textbf{rejette} $H_{0,i}$. 

Pour $\theta \in \Theta,$ on note $T_\theta = \big\{ 1 \le i \le m:\theta \in \Theta_{0,i}\big\}$ l'ensemble des indices des tests pour lesquels l'hypothèse nulle est \textbf{vraie}.

\espace\espace
 
On suppose que, pour chaque $i=1, \ldots, m$, on dispose d''une famille $(\phi_{\alpha, i})_{\alpha \in (0,1)}$ de tests simples de niveau $\alpha$. On peut définir, pour chaque $i=1, \ldots, m$, la p-valeur associée $\widehat \alpha_i = \widehat \alpha_i(Z)$. La question est de savoir comment combiner ces p-valeurs individuelles en un test multiple. 

\petitespace

\subsubsection{FWER et Correction de Bonferroni}
\petitespace

\Def
Le \textbf{family-wise error rate} (FWER) de $R$ est défini par :

$$\text{FWER}(R) = \Sup{\theta \in \Theta}\P_\theta(R(Z) \cap T_\theta \ne \emptyset) =  \Sup{\theta \in \Theta}\P_\theta(\exists i \in \ldbrack 1, m \rdbrack,\quad  \theta \in \Theta_{0,i}\quad \text{mais}\quad i \in R(Z) ) $$


Cela correspond à la probabilité de faire au moins une erreur de type I. 

\petitespace

On peut observer la méthode naïve en prenant $R(Z) = \big\{ 1 \le i \le m: \widehat \alpha_i \le \alpha\big\}$ (pas de correction pour le test multiple). Cette méthode ne fonctionne pas : considérons $20$ tests avec des hypothèses nulles vraies au niveau $\alpha = 5\%$. En moyenne, au moins 1 test va être rejeté (à tord).
 
\petitespace

\Def
\textbf{Correction de Bonferroni}

$$R_B(Z) = \big\{ i \in \ldbrack 1, m \rdbrack: \widehat \alpha_i \le \frac{\alpha}{m} \big\}   $$

\Prop

$\text{FWER}(R_B) \le \alpha$ 

\petitespace

\corr{\textbf{Preuve :}
Borne d'union : 

\begin{align*}
	\P_\theta(R(Z) \cap T_\theta \ne \emptyset ) &= \P_\theta( \exists i \in T_\theta : \widehat \alpha_i \le \frac{\alpha}{m}) \\
	& \le \somme{i \in T_\theta}{} \P_\theta( \widehat \alpha_i \le \frac{\alpha}{m}) \\
	&\le  \#(T_\theta)\frac{\alpha}{m} \quad \text{(sur-uniformité de la p-valeur sous $H_{0,i}$)} \\
	& \le \alpha
\end{align*}
}
\Rq

Le FWER est un critère conservateur, en effet, on ne souhaite effectuer aucune erreur de type I. Dans les applications où l'on effectue un grand nombre de tests simultanément ($m$ grand, $\frac \alpha m \ll1$), on risque de ne rien rejeter. 

\petitespace

\subsubsection{La FDR et la Correction de Benjamini-Hochberg}

\petitespace

L'objectif est d'adopter un critère plus souple sur les erreurs de type I afin de faire davantage de découvertes. On peut par exemple utiliser le critère des \textbf{faux positifs} (erreur de type I) :

$$\text{FP}(\theta, Z) = \#(R(Z) \cap T_\theta ) = \# \big\{ i:\theta \in \Theta_{0,i}, \quad i \in R(Z) \big\}$$

ou bien l'ensemble des \textbf{faux négatifs} (erreur de type II) :

$$\text{FN}(\theta, Z) = \#(R(Z)^c \cap T_\theta^c) $$

\petitespace 

\Def

On définit la \textbf{False discovery proportion} (FDP) ainsi : 

$$\text{FDP}(\theta, R(Z)) = \frac{ \#(R(Z) \cap T_\theta )}{\max(\#R(Z),1)} = \frac{\text{FP}}{\max(\#R(Z),1)}=\begin{cases} &\frac{\text{FP}}{\#R(Z)}\quad \text{Si }  R(Z) \ne \emptyset \\ & 0\quad\quad\quad \text{Si } R(Z) = \emptyset\end{cases} $$

Et le \textbf{False Discovery Rate} (RDR) comme cela :

$$\text{FDR}(\theta, R(Z)) = \esptheta{\text{FDP}(\theta, R(Z))} = \esptheta{ \frac{ \#(R(Z) \cap T_\theta )}{\#R(Z)}\1_{\# R(Z) \ge 1}}$$

On peut interpréter que FDR$(\theta,Z)\le \alpha$ signifie qu'en moyenne, parmi les "découvertes" (hypothèses nulles rejetées), la proportion de fausses découvertes est d'au plus $\alpha$. \petitespace

%\underline{Heuristique} (non rigoureux/fausse)\petitespace
%
%On ordonne les p-valeurs $\widehat \alpha_{(1)} \le \ldots \le \widehat \alpha_{(m)}$. On veut fixer un seuil $t = \widehat \alpha_{( \xi)}$, $\xi \in \ldbrack 0,m \rdbrack$. $R(Z) = \big\{ i \in \ldbrack 1,m \rdbrack : \widehat \alpha_i \le \widehat \alpha_{( \xi)} \big\}$, ainsi on a $\#(R(Z)) = \xi$. 
%
%Le nombre moyen de faux positifs en faisant comme si $t$ était déterministe (ce qui est faux) est : 
%
%\begin{align*}
%	\esptheta{ \#(T_\theta \cap R(Z))} &= \esptheta{\somme{i \in T_\theta}{} \1_{i \in R(Z)}}\\
%	&= \somme{i \in T_\theta}{} \P_\theta(\widehat \alpha_i \le t) \\
%	& \lessapprox \#T_\theta \times t \le m \widehat \alpha_{(\xi)}
%\end{align*}
%
%FDP$(\theta, R(Z)) =  \frac{ \#(R(Z) \cap T_\theta )}{\#R(Z)} \lessapprox \frac{m \widehat \alpha_{(\xi)}}{\xi}$
%
%On cherche par ailleurs à faire autant de découvertes que possible, ce qui suggère de choisir 
%
%$$\widehat \xi = \max(1 \le \xi \le m :   \widehat \alpha_{( \xi)} \le \frac{\xi\alpha}{m}) $$
%
%\petitespace

\textbf{Procédure de Benjamini-Hochberg}\petitespace

Soit $C_m = \somme{j=1}{m} \frac 1j$. On note 

$$R_{BH} = \big\{ 1 \le i \le m : \widehat \alpha_{i} \le  \widehat \alpha_{(\hat \xi)} \big\}  $$


où $\widehat \xi = \max(1 \le \xi \le m :   \widehat \alpha_{( \xi)}  \le \frac{\xi\alpha}{mC_m})$

\petitespace

\thm

FDR$(\theta, R_{BH}) \le \alpha$ 

\petitespace

\corr{\textbf{Preuve :}



\begin{align*}
	\text{FDR}(\theta, R_{BH}) &= \esptheta{\frac{ \#(R_{BH}(Z) \cap T_\theta )}{\#R_{BH}(Z)}\1_{\#R_{BH}(Z) \ge 1} }\\
	&= \esptheta{\frac{\somme{i \in T_\theta}{} \1_{\widehat \alpha_i \le  \widehat \alpha_{(\hat \xi)} }}{\hat \xi} \1_{\hat \xi \ge 1}} \quad \text{(*)}
\end{align*}

On remarque que : 

$$\frac{1}{\hat \xi} = \somme{j \ge \hat \xi}{}(\frac 1j - \frac{1}{j+1}) = \somme{j \ge 1}{} \frac{\1_{j \ge \hat \xi} }{j(j+1)}$$

Donc 

$$ \text{FDR} \underset{(*)}{=} \esptheta{\somme{i \in T_\theta}{} \somme{j \ge 1}{}\frac{\1_{\widehat \alpha_i \le \widehat \alpha_{(\hat \xi)} } \1_{1 \le \hat \xi \le j} }{j(j+1)} }$$

Or, si $\hat \xi \le j$, on a $\widehat \alpha_{(\hat \xi)} \underset{(def)}{\le }  \frac{\hat\xi\alpha}{mC_m} \le \min(\alpha', \frac{j\alpha'}{m}) $ car $ \hat\xi \le \min(j,m)$, avec $\alpha' = \frac{\alpha}{C_m}$. Donc


\begin{align*}
	\text{FDR} &\le \esptheta{\somme{i \in T_\theta}{} \somme{j \ge 1}{}\frac{\1_{\widehat \alpha_i \le \min(\frac{j\alpha'}{m}, \alpha')} }{j(j+1)} } \\
	&= \somme{i \in T_\theta}{} \somme{j \ge 1}{}\frac{ \P_\theta(\widehat \alpha_i \le  \min(\frac{j\alpha'}{m}, \alpha'))}{j(j+1)}\\
	&\le \somme{i \in T_\theta}{} \somme{j \ge 1}{} \frac{\min(\frac{j\alpha'}{m}, \alpha')}{j(j+1)} \quad \text{(sur uniformité sous $H_{0,i}$ de $\widehat \alpha_i$)}\\
	&\le \# T_\theta  \somme{j \ge 1}{} \frac{\min(\frac{j\alpha'}{m}, \alpha')}{j(j+1)}\\
	&\le \# T_\theta  \alpha' [ \somme{j =1}{m-1}\frac{ \frac jm}{j(j+1)} + \somme{j=m}{\infty} \frac{1}{j(j+1)} ]\\
	&\le \# T_\theta \frac{\alpha'}{m}(\somme{j=2}{m} \frac 1j +1)\quad  \text{(car $\somme{j=m}{\infty} \frac{1}{j(j+1)}= \somme{j=m}{\infty} \frac 1j-\frac{1}{j+1} = \frac 1m$)}  \\
	&\le  \# T_\theta \frac{\alpha}{C_mm}\times C_m \le \alpha
\end{align*}



\petitespace}



































\end{document}

% Ajouter variance conditionnelle
%théorème du portemanteau + levy
% Prop+Preuve sur équivalence convergence p.s., en P, L^p d'un vecteur aléatoire et de chacune de ses composantes
% preuve fonction muette 
% preuve indépendances mutuelle avec toute partie finie
%Rajouter exemple plus restructurer la partie "convergence"
% Rajouter partie dénombrement (au début)
%finir inégalités de concentrations (les queues poisonniennes, exponentielles etc)
%Preuve inégalité de DKR
%Preuve que L^2 est un Hilbert
%revoir densités conditionnelle
%Preuve X=Y p.s. ==> W = Y en (d)
%remplacer les vecteurs aléatoires X par \bf X au début au niveau de l'espérance et un peut partout avant la partie stat
% vecteurs aléatoires définir plus les règles de calcul concernant les matrices symétriques ou non
% rajouter que norme de fonction caractéristique = 1
% Ne mettre qu'UN Théorème de transfert conditionnelle
%Inégalité de Cantelli + preuve de celle de Kolmogorov
%revoir caractérisation des moments par fonction caractéristique, pour les vecteurs aléatoires
% combinatoire : https://fr.wikipedia.org/wiki/Dénombrement
% mettre donsker tout en bas et rajouter exemple arrangement et présentation permutations
%rajouter théorème de transfert avant prop 6.6.3.1

$$
\begin{array}{rrrl} %fonction à une variables, {f}{E}{R}{2x+3}

  f :& \R^d &\mapsto& \R\\
  & x & \mapsto & \frac{1}{\sqrt{2\pi}\sqrt{det(\Sigma)}}e^{\frac{-1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{array}
$$




